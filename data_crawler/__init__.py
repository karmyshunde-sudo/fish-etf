import os
import time
import pandas as pd
import logging
from datetime import datetime, date, timedelta
from retrying import retry
import akshare as ak
from pandas.tseries.offsets import CustomBusinessDay
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday
from config import Config
from .etf_list_manager import update_all_etf_list, get_filtered_etf_codes, load_all_etf_list
from .akshare_crawler import crawl_etf_daily_akshare
from .sina_crawler import crawl_etf_daily_sina
from utils.date_utils import get_beijing_time
from utils.file_utils import init_dirs

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# å®šä¹‰ä¸­å›½è‚¡å¸‚èŠ‚å‡æ—¥æ—¥å†ï¼ˆ2025å¹´ï¼‰
class ChinaStockHolidayCalendar(AbstractHolidayCalendar):
    rules = [
        Holiday("å…ƒæ—¦", month=1, day=1),
        Holiday("æ˜¥èŠ‚", month=1, day=29, observance=lambda d: d + pd.DateOffset(days=+5)),
        Holiday("æ¸…æ˜èŠ‚", month=4, day=4),
        Holiday("åŠ³åŠ¨èŠ‚", month=5, day=1, observance=lambda d: d + pd.DateOffset(days=+2)),
        Holiday("ç«¯åˆèŠ‚", month=6, day=2),
        Holiday("ä¸­ç§‹èŠ‚", month=9, day=8),
        Holiday("å›½åº†èŠ‚", month=10, day=1, observance=lambda d: d + pd.DateOffset(days=+6)),
    ]

# é‡è¯•è£…é¥°å™¨é…ç½®
def retry_if_exception(exception):
    return isinstance(exception, (ConnectionError, TimeoutError, Exception))

@retry(
    stop_max_attempt_number=3,
    wait_exponential_multiplier=1000,
    wait_exponential_max=10000,
    retry_on_exception=retry_if_exception
)
def akshare_retry(func, *args, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨å°è£…"""
    return func(*args, **kwargs)

def is_trading_day(check_date: date) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¥"""
    if check_date.weekday() >= 5:  # å‘¨å…­æˆ–å‘¨æ—¥
        return False
    
    china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
    try:
        return pd.Timestamp(check_date) == (pd.Timestamp(check_date) + 0 * china_bd)
    except Exception as e:
        logger.error(f"äº¤æ˜“æ—¥åˆ¤æ–­å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_etf_name(etf_code):
    """æ ¹æ®ETFä»£ç è·å–åç§°"""
    etf_list = load_all_etf_list()
    if etf_list.empty:
        return "æœªçŸ¥åç§°"
    
    target_code = str(etf_code).strip().zfill(6)
    name_row = etf_list[
        etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == target_code
    ]
    
    if not name_row.empty:
        return name_row.iloc[0]["ETFåç§°"]
    else:
        return f"æœªçŸ¥åç§°({etf_code})"

def get_last_crawl_date(etf_code, etf_daily_dir):
    """è·å–æœ€åä¸€æ¬¡çˆ¬å–çš„æ—¥æœŸ"""
    file_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
    if not os.path.exists(file_path):
        # æ²¡æœ‰å­˜é‡æ•°æ®ï¼Œè¿”å›æœ€è¿‘ä¸€å¹´çš„æ—¥æœŸ
        current_date = get_beijing_time().date()
        start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
        return start_date
    
    try:
        df = pd.read_csv(file_path, encoding="utf-8")
        if df.empty or "date" not in df.columns:
            # æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰dateåˆ—ï¼Œè¿”å›æœ€è¿‘ä¸€å¹´çš„æ—¥æœŸ
            current_date = get_beijing_time().date()
            start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
            return start_date
        
        last_date = df["date"].max()
        # è®¡ç®—ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºå¼€å§‹æ—¥æœŸ
        china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
        next_date = (pd.to_datetime(last_date) + china_bd).strftime("%Y-%m-%d")
        return next_date
    except Exception as e:
        logger.warning(f"è·å–{etf_code}æœ€åçˆ¬å–æ—¥æœŸå¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨æœ€è¿‘ä¸€å¹´æ—¥æœŸ")
        current_date = get_beijing_time().date()
        start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
        return start_date

def record_failed_etf(etf_daily_dir, etf_code, etf_name, error_message=None):
    """è®°å½•å¤±è´¥çš„ETFä¿¡æ¯"""
    failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
    timestamp = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(failed_file, "a", encoding="utf-8") as f:
        if error_message:
            f.write(f"{etf_code}|{etf_name}|{timestamp}|{error_message}\n")
        else:
            f.write(f"{etf_code}|{etf_name}|{timestamp}\n")

def crawl_etf_daily_incremental():
    """å¢é‡çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼ˆå•åªä¿å­˜+æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼‰"""
    logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_etf_daily =====")
    current_time = get_beijing_time()
    current_date = current_time.date()
    logger.info(f"å½“å‰æ—¶é—´ï¼š{current_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰")
    
    # éäº¤æ˜“æ—¥ä¸”æœªåˆ°è¡¥çˆ¬æ—¶é—´ï¼ˆ18ç‚¹åå…è®¸è¡¥çˆ¬ï¼‰
    if not is_trading_day(current_date) and current_time.hour < 18:
        logger.info(f"ä»Šæ—¥{current_date}éäº¤æ˜“æ—¥ä¸”æœªåˆ°è¡¥çˆ¬æ—¶é—´ï¼Œæ— éœ€çˆ¬å–æ—¥çº¿æ•°æ®")
        return
    
    # åˆå§‹åŒ–ç›®å½•
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    etf_daily_dir = os.path.join(root_dir, "data", "etf_daily")
    os.makedirs(etf_daily_dir, exist_ok=True)
    logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
    
    # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
    completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
    
    # åŠ è½½å·²å®Œæˆåˆ—è¡¨
    completed_codes = set()
    if os.path.exists(completed_file):
        with open(completed_file, "r", encoding="utf-8") as f:
            completed_codes = set(line.strip() for line in f if line.strip())
        logger.info(f"å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
    
    # è·å–å¾…çˆ¬å–ETFåˆ—è¡¨
    all_codes = get_filtered_etf_codes()
    to_crawl_codes = [code for code in all_codes if code not in completed_codes]
    total = len(to_crawl_codes)
    
    if total == 0:
        logger.info("æ‰€æœ‰ETFæ—¥çº¿æ•°æ®å‡å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­")
        return
    
    logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total}åª")
    
    # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹50åªï¼‰
    batch_size = 50
    batches = [to_crawl_codes[i:i+batch_size] for i in range(0, total, batch_size)]
    logger.info(f"å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} åªETF")
    
    # é€æ‰¹ã€é€åªçˆ¬å–
    for batch_idx, batch in enumerate(batches, 1):
        batch_num = len(batch)
        logger.info(f"==============================")
        logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(batches)}")
        logger.info(f"ETFèŒƒå›´ï¼š{batch_idx*batch_size - batch_size + 1}-{min(batch_idx*batch_size, total)}åªï¼ˆå…±{batch_num}åªï¼‰")
        logger.info(f"==============================")
        
        for idx, etf_code in enumerate(batch, 1):
            try:
                # æ‰“å°å½“å‰è¿›åº¦
                logger.info(f"--- æ‰¹æ¬¡{batch_idx} - ç¬¬{idx}åª / å…±{batch_num}åª ---")
                etf_name = get_etf_name(etf_code)
                logger.info(f"ETFä»£ç ï¼š{etf_code} | åç§°ï¼š{etf_name}")
                
                # ç¡®å®šçˆ¬å–æ—¶é—´èŒƒå›´ï¼ˆå¢é‡çˆ¬å–ï¼‰
                start_date = get_last_crawl_date(etf_code, etf_daily_dir)
                end_date = current_date.strftime("%Y-%m-%d")
                
                if start_date > end_date:
                    logger.info(f"ğŸ“… æ— æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆä¸Šæ¬¡çˆ¬å–è‡³{start_date}ï¼‰")
                    # æ ‡è®°ä¸ºå·²å®Œæˆ
                    with open(completed_file, "a", encoding="utf-8") as f:
                        f.write(f"{etf_code}\n")
                    continue
                
                logger.info(f"ğŸ“… çˆ¬å–æ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
                
                # å…ˆå°è¯•AkShareçˆ¬å–
                df = crawl_etf_daily_akshare(etf_code, start_date, end_date)
                
                # AkShareå¤±è´¥åˆ™å°è¯•æ–°æµªçˆ¬å–
                if df.empty:
                    logger.warning("âš ï¸ AkShareæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ–°æµªæ¥å£")
                    df = crawl_etf_daily_sina(etf_code, start_date, end_date)
                
                # æ•°æ®æ ¡éªŒ
                if df.empty:
                    logger.warning(f"âš ï¸ æ‰€æœ‰æ¥å£å‡æœªè·å–åˆ°æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                    # è®°å½•å¤±è´¥æ—¥å¿—ï¼Œä½†ä¸æ ‡è®°ä¸ºå·²å®Œæˆï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡è¯•
                    record_failed_etf(etf_daily_dir, etf_code, etf_name)
                    continue
                
                # ç»Ÿä¸€åˆ—åï¼ˆè½¬ä¸ºè‹±æ–‡åˆ—åï¼Œä½¿ç”¨config.pyä¸­çš„æ ‡å‡†å®šä¹‰ï¼‰
                col_map = Config.STANDARD_COLUMNS
                df = df.rename(columns=col_map)
                
                # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
                df["etf_code"] = etf_code
                df["etf_name"] = etf_name
                df["crawl_time"] = current_time.strftime("%Y-%m-%d %H:%M:%S")
                
                # ç¡®ä¿æ‰€æœ‰æ ‡å‡†åˆ—éƒ½å­˜åœ¨
                for col in Config.STANDARD_COLUMNS.values():
                    if col not in df.columns:
                        # å¡«å……ç¼ºå¤±çš„åˆ—ï¼ˆé™¤äº†etf_code, etf_name, crawl_timeå·²ç»åœ¨ä¸Šé¢æ·»åŠ ï¼‰
                        if col == "amplitude" and "æŒ¯å¹…" in df.columns:
                            df[col] = df["æŒ¯å¹…"]
                        elif col == "price_change" and "æ¶¨è·Œé¢" in df.columns:
                            df[col] = df["æ¶¨è·Œé¢"]
                        elif col == "turnover" and "æ¢æ‰‹ç‡" in df.columns:
                            df[col] = df["æ¢æ‰‹ç‡"]
                        else:
                            df[col] = None  # å¡«å……ç©ºå€¼
                
                # åªä¿ç•™æ ‡å‡†åˆ—
                df = df[list(Config.STANDARD_COLUMNS.values())]
                
                # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
                save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                if os.path.exists(save_path):
                    existing_df = pd.read_csv(save_path, encoding="utf-8")
                    # å»é‡ååˆå¹¶
                    combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=["date"], keep="last")
                    # æŒ‰æ—¥æœŸæ’åº
                    combined_df["date"] = pd.to_datetime(combined_df["date"])
                    combined_df = combined_df.sort_values("date").reset_index(drop=True)
                    combined_df["date"] = combined_df["date"].dt.strftime("%Y-%m-%d")
                    df = combined_df
                
                # ä¿å­˜æ•°æ®
                df.to_csv(save_path, index=False, encoding="utf-8")
                logger.info(f"âœ… ä¿å­˜æˆåŠŸï¼š{save_path}ï¼ˆå…±{len(df)}æ¡æ•°æ®ï¼‰")
                
                # è®°å½•å·²å®Œæˆ
                with open(completed_file, "a", encoding="utf-8") as f:
                    f.write(f"{etf_code}\n")
                
                # å•åªçˆ¬å–åçŸ­ä¼‘çœ 
                time.sleep(1)
                
            except Exception as e:
                # å•åªå¤±è´¥ä¸ä¸­æ–­ï¼Œè®°å½•æ—¥å¿—åç»§ç»­
                logger.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}", exc_info=True)
                # è®°å½•å¤±è´¥æ—¥å¿—
                record_failed_etf(etf_daily_dir, etf_code, etf_name, str(e))
                time.sleep(3)  # å¤±è´¥åå»¶é•¿ä¼‘çœ 
                continue
        
        # æ‰¹æ¬¡é—´é•¿ä¼‘çœ ï¼ˆå‡è½»æœåŠ¡å™¨å‹åŠ›ï¼‰
        if batch_idx < len(batches):
            logger.info(f"æ‰¹æ¬¡{batch_idx}å¤„ç†å®Œæˆï¼Œä¼‘çœ 10ç§’åç»§ç»­...")
            time.sleep(10)
    
    logger.info("===== æ‰€æœ‰å¾…çˆ¬å–ETFå¤„ç†å®Œæ¯• =====")
