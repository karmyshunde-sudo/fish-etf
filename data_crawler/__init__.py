#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ•°æ®çˆ¬å–æ¨¡å—
æä¾›ETFæ—¥çº¿æ•°æ®çˆ¬å–ã€ETFåˆ—è¡¨ç®¡ç†ç­‰åŠŸèƒ½
ç‰¹åˆ«ä¼˜åŒ–äº†å¢é‡ä¿å­˜å’Œæ–­ç‚¹ç»­çˆ¬æœºåˆ¶
"""

import os
import time
import pandas as pd
import logging
import tempfile  # ä¿®å¤ï¼šæ·»åŠ tempfileå¯¼å…¥
import shutil    # ä¿®å¤ï¼šæ·»åŠ shutilå¯¼å…¥
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Tuple, Optional
import akshare as ak
# ä¿®å¤ï¼šæ·»åŠ  retrying å¯¼å…¥ - è¿™æ˜¯å…³é”®ä¿®å¤
from retrying import retry

# æ·»åŠ å¿…è¦çš„å¯¼å…¥
from config import Config
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    is_file_outdated,
    is_trading_day,
    get_last_trading_day
)
from utils.file_utils import (
    ensure_dir_exists,
    get_last_crawl_date,
    record_failed_etf,
    ensure_chinese_columns,
    standardize_column_names
)
# ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
from utils.data_processor import (
    ensure_required_columns,
    clean_and_format_data,
    limit_to_one_year_data
)
from data_crawler.akshare_crawler import crawl_etf_daily_akshare
from data_crawler.sina_crawler import crawl_etf_daily_sina
from data_crawler.etf_list_manager import (
    get_filtered_etf_codes,
    get_etf_name,
    update_all_etf_list
)

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•è£…é¥°å™¨é…ç½®
def retry_if_exception(exception: Exception) -> bool:
    """é‡è¯•æ¡ä»¶ï¼šç½‘ç»œæˆ–æ•°æ®ç›¸å…³é”™è¯¯"""
    return isinstance(exception, (ConnectionError, TimeoutError, ValueError, pd.errors.EmptyDataError))

@retry(
    stop_max_attempt_number=3,
    wait_exponential_multiplier=1000,
    wait_exponential_max=10000,
    retry_on_exception=retry_if_exception
)
def akshare_retry(func, *args, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨å°è£…"""
    return func(*args, **kwargs)

def crawl_etf_daily_incremental() -> None:
    """å¢é‡çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼ˆå•åªä¿å­˜+æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼‰
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸å†åŒ…å«æ˜¯å¦æ‰§è¡Œçš„åˆ¤æ–­é€»è¾‘ï¼Œç”±è°ƒç”¨æ–¹å†³å®šæ˜¯å¦æ‰§è¡Œ"""
    try:
        logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_etf_daily =====")
        beijing_time = get_beijing_time()
        logger.info(f"å½“å‰åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        Config.init_dirs()
        etf_daily_dir = Config.ETFS_DAILY_DIR
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
        
        # è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
        last_trading_day = get_last_trading_day()
        end_date = last_trading_day.strftime("%Y-%m-%d")
        
        # è·å–å¾…çˆ¬å–ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        to_crawl_codes = []
        
        # ç²¾ç¡®åˆ¤æ–­å“ªäº›ETFéœ€è¦çˆ¬å–
        for code in all_codes:
            save_path = os.path.join(etf_daily_dir, f"{code}.csv")
            is_first_crawl = not os.path.exists(save_path)
            
            if is_first_crawl:
                # é¦–æ¬¡çˆ¬å–ï¼Œéœ€è¦è·å–æ•°æ®
                to_crawl_codes.append(code)
                continue
            
            # æ£€æŸ¥ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            latest_data_date = get_latest_data_date(save_path)
            if latest_data_date < last_trading_day:
                # æ•°æ®ä¸æ˜¯æœ€æ–°çš„ï¼Œéœ€è¦å¢é‡çˆ¬å–
                to_crawl_codes.append(code)
        
        total = len(to_crawl_codes)
        if total == 0:
            logger.info("æ‰€æœ‰ETFæ—¥çº¿æ•°æ®å‡å·²æœ€æ–°ï¼Œæ— éœ€ç»§ç»­")
            return
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total}åªï¼ˆåŸºäºå®é™…æ•°æ®çŠ¶æ€åˆ¤æ–­ï¼‰")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„ï¼ˆä»…ç”¨äºè®°å½•è¿›åº¦ï¼Œä¸ç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦çˆ¬å–ï¼‰
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨ï¼ˆä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"è¿›åº¦è®°å½•ä¸­å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–è¿›åº¦è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                completed_codes = set()
        
        # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹50åªï¼‰
        batch_size = Config.CRAWL_BATCH_SIZE
        num_batches = (total + batch_size - 1) // batch_size
        
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥è·Ÿè¸ªéœ€è¦æäº¤çš„æ–‡ä»¶
        files_to_commit = []
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total)
            batch_codes = to_crawl_codes[start_idx:end_idx]
            
            logger.info(f"å¤„ç†ç¬¬ {batch_idx+1}/{num_batches} æ‰¹ ETF ({len(batch_codes)}åª)")
            
            for etf_code in batch_codes:
                etf_name = get_etf_name(etf_code)
                logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                
                # ç¡®å®šçˆ¬å–æ—¶é—´èŒƒå›´ï¼ˆå¢é‡çˆ¬å–ï¼‰
                save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                is_first_crawl = not os.path.exists(save_path)
                
                # é¦–æ¬¡çˆ¬å–è·å–ä¸€å¹´æ•°æ®ï¼Œå¢é‡çˆ¬å–åªè·å–æ–°æ•°æ®
                if is_first_crawl:
                    # é¦–æ¬¡çˆ¬å–ï¼šè·å–1å¹´å†å²æ•°æ®
                    start_date = (last_trading_day - timedelta(days=365)).strftime("%Y-%m-%d")
                    logger.info(f"ğŸ“… é¦–æ¬¡çˆ¬å–ï¼Œè·å–1å¹´å†å²æ•°æ®ï¼š{start_date} è‡³ {end_date}")
                else:
                    # å¢é‡çˆ¬å–ï¼šè·å–ä¸Šæ¬¡çˆ¬å–åçš„æ•°æ®
                    start_date = get_last_crawl_date(etf_code, etf_daily_dir)
                    # å¦‚æœä¸Šæ¬¡çˆ¬å–æ—¥æœŸå·²ç»æ˜¯ä»Šå¤©ï¼Œæ— éœ€å†çˆ¬
                    start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
                    end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
                    if start_date_obj > end_date_obj:
                        logger.info(f"ğŸ“… æ— æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆä¸Šæ¬¡çˆ¬å–è‡³{start_date}ï¼‰")
                        # æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
                        with open(completed_file, "a", encoding="utf-8") as f:
                            f.write(f"{etf_code}\n")
                        continue
                    logger.info(f"ğŸ“… å¢é‡çˆ¬å–ï¼Œè·å–æ–°æ•°æ®ï¼š{start_date} è‡³ {end_date}")
                
                # å…ˆå°è¯•AkShareçˆ¬å–
                df = crawl_etf_daily_akshare(etf_code, start_date, end_date, is_first_crawl=is_first_crawl)
                
                # AkShareå¤±è´¥åˆ™å°è¯•æ–°æµªçˆ¬å–
                if df.empty:
                    logger.warning("âš ï¸ AkShareæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ–°æµªæ¥å£")
                    df = crawl_etf_daily_sina(etf_code, start_date, end_date, is_first_crawl=is_first_crawl)
                
                # æ•°æ®æ ¡éªŒ
                if df.empty:
                    logger.warning(f"âš ï¸ æ‰€æœ‰æ¥å£å‡æœªè·å–åˆ°æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                    # è®°å½•å¤±è´¥æ—¥å¿—ï¼Œä½†ä¸æ ‡è®°ä¸ºå·²å®Œæˆï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡è¯•
                    record_failed_etf(etf_daily_dir, etf_code, etf_name)
                    continue
                
                # ç¡®ä¿ä½¿ç”¨ä¸­æ–‡åˆ—å
                df = ensure_chinese_columns(df)
                
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
                df = ensure_required_columns(df)
                
                # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
                df["ETFä»£ç "] = etf_code
                df["ETFåç§°"] = etf_name
                df["çˆ¬å–æ—¶é—´"] = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
                
                # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
                if os.path.exists(save_path):
                    try:
                        existing_df = pd.read_csv(save_path)
                        # ç¡®ä¿ç°æœ‰æ•°æ®ä¹Ÿæ˜¯ä¸­æ–‡åˆ—å
                        existing_df = ensure_chinese_columns(existing_df)
                        
                        # ç¡®ä¿å¿…éœ€åˆ—
                        existing_df = ensure_required_columns(existing_df)
                        
                        # åˆå¹¶æ•°æ®å¹¶å»é‡
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                        combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                        
                        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œï¼Œç¡®ä¿æ•°æ®å®‰å…¨
                        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                        try:
                            combined_df.to_csv(temp_file.name, index=False)
                            # åŸå­æ›¿æ¢ï¼šå…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œå†æ›¿æ¢åŸæ–‡ä»¶
                            shutil.move(temp_file.name, save_path)
                            logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                        finally:
                            if os.path.exists(temp_file.name):
                                os.unlink(temp_file.name)
                    except Exception as e:
                        logger.error(f"åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}ï¼Œå°è¯•è¦†ç›–ä¿å­˜", exc_info=True)
                        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                        try:
                            df.to_csv(temp_file.name, index=False)
                            # åŸå­æ›¿æ¢
                            shutil.move(temp_file.name, save_path)
                            logger.info(f"âœ… æ•°æ®å·²è¦†ç›–ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                        finally:
                            if os.path.exists(temp_file.name):
                                os.unlink(temp_file.name)
                else:
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                    try:
                        df.to_csv(temp_file.name, index=False)
                        # åŸå­æ›¿æ¢
                        shutil.move(temp_file.name, save_path)
                        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                    finally:
                        if os.path.exists(temp_file.name):
                            os.unlink(temp_file.name)
                
                # ===== å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„git_utilså‡½æ•° =====
                # è°ƒç”¨commit_files_in_batchesï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ¯10ä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
                try:
                    from utils.git_utils import commit_files_in_batches
                    commit_files_in_batches(save_path)
                    logger.info(f"âœ… ETF {etf_code} æ•°æ®å·²æ ‡è®°ç”¨äºæ‰¹é‡æäº¤")
                except ImportError:
                    logger.warning("æœªæ‰¾åˆ°git_utilsæ¨¡å—ï¼Œè·³è¿‡Gitæäº¤")
                except Exception as e:
                    logger.error(f"æ ‡è®°ETF {etf_code} æ•°æ®ç”¨äºæ‰¹é‡æäº¤æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                
                # æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
                with open(completed_file, "a", encoding="utf-8") as f:
                    f.write(f"{etf_code}\n")
                
                # é™åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(1)  # ä½¿ç”¨ç¡¬ç¼–ç å€¼ä»£æ›¿Config.CRAWL_INTERVAL
            
            # æ‰¹æ¬¡é—´æš‚åœ
            if batch_idx < num_batches - 1:
                batch_pause_seconds = 2  # ç¡¬ç¼–ç å€¼ï¼Œ10ç§’
                logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæš‚åœ {batch_pause_seconds} ç§’...")
                time.sleep(batch_pause_seconds)
    
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®å¢é‡çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

def save_all_etf_data(etf_data_cache: Dict[str, pd.DataFrame], etf_daily_dir: str) -> None:
    """
    ä¸€æ¬¡æ€§ä¿å­˜æ‰€æœ‰ETFæ•°æ®åˆ°æ–‡ä»¶
    Args:
        etf_data_cache: å†…å­˜ä¸­çš„ETFæ•°æ®ç¼“å­˜
        etf_daily_dir: ETFæ—¥çº¿æ•°æ®ç›®å½•
    """
    logger.info("å¼€å§‹æ‰¹é‡ä¿å­˜ETFæ•°æ®åˆ°æ–‡ä»¶...")
    try:
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥è·Ÿè¸ªéœ€è¦æäº¤çš„æ–‡ä»¶
        files_to_commit = []
        
        for etf_code, df in etf_data_cache.items():
            save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
            try:
                if os.path.exists(save_path):
                    existing_df = pd.read_csv(save_path)
                    # ç¡®ä¿ç°æœ‰æ•°æ®ä¹Ÿæ˜¯ä¸­æ–‡åˆ—å
                    existing_df = ensure_chinese_columns(existing_df)
                    
                    # ç¡®ä¿å¿…éœ€åˆ—
                    existing_df = ensure_required_columns(existing_df)
                    
                    # åˆå¹¶æ•°æ®å¹¶å»é‡
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
                    combined_df.to_csv(save_path, index=False, encoding="utf-8-sig")
                    
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                else:
                    df.to_csv(save_path, index=False, encoding="utf-8-sig")
                    logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                
                # ===== å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„git_utilså‡½æ•° =====
                # è°ƒç”¨commit_files_in_batchesï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ¯10ä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
                try:
                    from utils.git_utils import commit_files_in_batches
                    commit_files_in_batches(save_path)
                    logger.info(f"âœ… ETF {etf_code} æ•°æ®å·²æ ‡è®°ç”¨äºæ‰¹é‡æäº¤")
                except ImportError:
                    logger.warning("æœªæ‰¾åˆ°git_utilsæ¨¡å—ï¼Œè·³è¿‡Gitæäº¤")
                except Exception as e:
                    logger.error(f"æ ‡è®°ETF {etf_code} æ•°æ®ç”¨äºæ‰¹é‡æäº¤æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            except Exception as e:
                logger.error(f"ä¿å­˜ETF {etf_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        logger.info(f"æ‰¹é‡ä¿å­˜å®Œæˆï¼Œå…±å¤„ç† {len(etf_data_cache)} ä¸ªETF")
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¿å­˜ETFæ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        raise

def update_etf_list() -> bool:
    """
    æ›´æ–°ETFåˆ—è¡¨
    :return: æ˜¯å¦æˆåŠŸæ›´æ–°
    """
    try:
        logger.info("å¼€å§‹æ›´æ–°ETFåˆ—è¡¨")
        etf_list = update_all_etf_list()
        if etf_list.empty:
            logger.warning("ETFåˆ—è¡¨æ›´æ–°åä¸ºç©º")
            return False
        
        logger.info(f"ETFåˆ—è¡¨æ›´æ–°æˆåŠŸï¼Œå…±{len(etf_list)}åªETF")
        return True
    except Exception as e:
        logger.error(f"æ›´æ–°ETFåˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_crawl_status() -> Dict[str, Any]:
    """
    è·å–çˆ¬å–çŠ¶æ€ä¿¡æ¯
    :return: åŒ…å«çˆ¬å–çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    try:
        etf_daily_dir = Config.ETFS_DAILY_DIR
        
        # è·å–å·²å®Œæˆåˆ—è¡¨
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        completed_codes = set()
        if os.path.exists(completed_file):
            with open(completed_file, "r", encoding="utf-8") as f:
                completed_codes = set(line.strip() for line in f if line.strip())
        
        # è·å–å¤±è´¥åˆ—è¡¨
        failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
        failed_count = 0
        if os.path.exists(failed_file):
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_count = len(f.readlines())
        
        # è·å–æ‰€æœ‰ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        
        return {
            "total_etfs": len(all_codes),
            "completed_etfs": len(completed_codes),
            "failed_etfs": failed_count,
            "progress": f"{len(completed_codes)}/{len(all_codes)}",
            "percentage": round(len(completed_codes) / len(all_codes) * 100, 2) if all_codes else 0
        }
    except Exception as e:
        logger.error(f"è·å–çˆ¬å–çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "total_etfs": 0,
            "completed_etfs": 0,
            "failed_etfs": 0,
            "progress": "0/0",
            "percentage": 0
        }

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Config.init_dirs()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger.info("æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
except Exception as e:
    error_msg = f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    logger.error(error_msg, exc_info=True)
    
    try:
        # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
        import logging
        logging.basicConfig(
            level="INFO",
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler()]
        )
        logging.error(error_msg)
    except Exception as basic_log_error:
        print(f"åŸºç¡€æ—¥å¿—é…ç½®å¤±è´¥: {str(basic_log_error)}")
        print(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")

def get_latest_data_date(file_path: str) -> date:
    """
    è·å–æ•°æ®æ–‡ä»¶ä¸­çš„æœ€æ–°æ—¥æœŸ
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        date: æœ€æ–°æ—¥æœŸ
    """
    try:
        df = pd.read_csv(file_path)
        if "æ—¥æœŸ" in df.columns and not df.empty:
            # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
            # åˆ é™¤æ— æ•ˆæ—¥æœŸ
            df = df.dropna(subset=["æ—¥æœŸ"])
            # è·å–æœ€å¤§æ—¥æœŸ
            if not df.empty:
                latest_date = df["æ—¥æœŸ"].max()
                if not pd.isna(latest_date):
                    return latest_date.date()
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ {file_path} æœ€æ–°æ—¥æœŸå¤±è´¥: {str(e)}", exc_info=True)
    
    # å‡ºé”™æ—¶è¿”å›ä¸€ä¸ªè¾ƒæ—©çš„æ—¥æœŸï¼Œç¡®ä¿ä¼šé‡æ–°çˆ¬å–
    return date(2024, 9, 1)
