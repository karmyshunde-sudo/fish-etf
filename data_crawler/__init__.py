import os
import time
import pandas as pd
import logging
from datetime import datetime, date
from retrying import retry
import akshare as ak
from pandas.tseries.offsets import CustomBusinessDay
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday
from config import Config
from .etf_list_manager import update_all_etf_list, get_filtered_etf_codes, load_all_etf_list
from utils.date_utils import get_beijing_time
from utils.file_utils import init_dirs

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# å®šä¹‰ä¸­å›½è‚¡å¸‚èŠ‚å‡æ—¥æ—¥å†
class ChinaStockHolidayCalendar(AbstractHolidayCalendar):
    rules = [
        Holiday("2025å…ƒæ—¦", month=1, day=1),
        Holiday("2025æ˜¥èŠ‚", month=1, day=29, observance=lambda d: d + pd.DateOffset(days=+5)),
        Holiday("2025æ¸…æ˜èŠ‚", month=4, day=4),
        Holiday("2025åŠ³åŠ¨èŠ‚", month=5, day=1),
        Holiday("2025ç«¯åˆèŠ‚", month=6, day=2),
        Holiday("2025ä¸­ç§‹èŠ‚", month=9, day=8),
        Holiday("2025å›½åº†èŠ‚", month=10, day=1, observance=lambda d: d + pd.DateOffset(days=+6)),
    ]

# é‡è¯•è£…é¥°å™¨é…ç½®
def retry_if_exception(exception):
    return isinstance(exception, (ConnectionError, TimeoutError, Exception))

@retry(
    stop_max_attempt_number=3,
    wait_exponential_multiplier=1000,
    wait_exponential_max=10000,
    retry_on_exception=retry_if_exception
)
def akshare_retry(func, *args, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„akshareå‡½æ•°è°ƒç”¨å°è£…"""
    return func(*args, **kwargs)

def is_trading_day(check_date: date) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¥"""
    if check_date.weekday() >= 5:
        return False
    
    china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
    try:
        return pd.Timestamp(check_date) == (pd.Timestamp(check_date) + 0 * china_bd)
    except Exception as e:
        logger.error(f"äº¤æ˜“æ—¥åˆ¤æ–­å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_etf_name(etf_code):
    """æ ¹æ®ETFä»£ç è·å–åç§°"""
    etf_list = load_all_etf_list()
    if etf_list.empty:
        return "æœªçŸ¥åç§°"
    
    target_code = str(etf_code).strip().zfill(6)
    name_row = etf_list[
        etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == target_code
    ]
    
    if not name_row.empty:
        return name_row.iloc[0]["ETFåç§°"]
    else:
        return f"æœªçŸ¥åç§°({etf_code})"

def crawl_etf_daily_incremental():
    """å¢é‡çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼ˆå•åªä¿å­˜+æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼‰"""
    logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_etf_daily =====")
    current_time = get_beijing_time()
    logger.info(f"å½“å‰æ—¶é—´ï¼š{current_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰")
    
    if not is_trading_day(current_time.date()):
        logger.info(f"ä»Šæ—¥{current_time.date()}éäº¤æ˜“æ—¥ï¼Œæ— éœ€çˆ¬å–æ—¥çº¿æ•°æ®")
        return
    
    # æ˜¾å¼æ‹¼æ¥ç›®å½•ï¼Œç¡®ä¿åŸºäºå·²çŸ¥å­˜åœ¨çš„è·¯å¾„åˆ›å»º
    etf_daily_dir = os.path.join(os.path.dirname(Config.ALL_ETFS_PATH), "etf_daily")
    os.makedirs(etf_daily_dir, exist_ok=True)
    logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
    
    # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
    completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
    
    # åŠ è½½å·²å®Œæˆåˆ—è¡¨ï¼ˆæ–­ç‚¹ç»­çˆ¬åŸºç¡€ï¼‰
    completed_codes = set()
    if os.path.exists(completed_file):
        with open(completed_file, "r", encoding="utf-8") as f:
            completed_codes = set(line.strip() for line in f if line.strip())
        logger.info(f"å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
    
    # è·å–å¾…çˆ¬å–ETFåˆ—è¡¨ï¼ˆæ’é™¤å·²å®Œæˆçš„ï¼‰
    all_codes = get_filtered_etf_codes()
    to_crawl_codes = [code for code in all_codes if code not in completed_codes]
    total = len(to_crawl_codes)
    
    if total == 0:
        logger.info("æ‰€æœ‰ETFæ—¥çº¿æ•°æ®å‡å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­")
        return
    
    logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total}åª")
    
    # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹50åªï¼‰
    batch_size = 50
    batches = [to_crawl_codes[i:i+batch_size] for i in range(0, total, batch_size)]
    logger.info(f"å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} åªETF")
    
    # é€æ‰¹ã€é€åªçˆ¬å–
    for batch_idx, batch in enumerate(batches, 1):
        batch_num = len(batch)
        logger.info(f"==============================")
        logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(batches)}")
        logger.info(f"ETFèŒƒå›´ï¼š{batch_idx*batch_size - batch_size + 1}-{min(batch_idx*batch_size, total)}åªï¼ˆå…±{batch_num}åªï¼‰")
        logger.info(f"==============================")
        
        for idx, etf_code in enumerate(batch, 1):
            try:
                # æ‰“å°å½“å‰è¿›åº¦
                logger.info(f"--- æ‰¹æ¬¡{batch_idx} - ç¬¬{idx}åª / å…±{batch_num}åª ---")
                etf_name = get_etf_name(etf_code)
                logger.info(f"ETFä»£ç ï¼š{etf_code} | åç§°ï¼š{etf_name}")
                
                # çˆ¬å–æ—¥çº¿æ•°æ®ï¼ˆä½¿ç”¨å¸¦é‡è¯•çš„å°è£…ï¼‰
                df = akshare_retry(
                    ak.fund_etf_hist_em,
                    symbol=etf_code,
                    period="daily",
                    adjust="qfq"
                )
                
                # æ•°æ®æ ¡éªŒ
                if df.empty:
                    logger.warning(f"âš ï¸ çˆ¬å–ç»“æœä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
                    continue
                
                # ç»Ÿä¸€åˆ—åï¼ˆç¡®ä¿å…¼å®¹æ€§ï¼‰
                df = df.rename(columns={
                    "æ—¥æœŸ": "date",
                    "å¼€ç›˜ä»·": "open",
                    "æœ€é«˜ä»·": "high",
                    "æœ€ä½ä»·": "low",
                    "æ”¶ç›˜ä»·": "close",
                    "æˆäº¤é‡": "volume",
                    "æˆäº¤é¢": "amount",
                    "æ¶¨è·Œå¹…": "pct_change"
                })
                
                # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
                df["etf_code"] = etf_code
                df["etf_name"] = etf_name
                df["crawl_time"] = current_time.strftime("%Y-%m-%d %H:%M:%S")
                
                # å•åªä¿å­˜è·¯å¾„ï¼ˆåŸºäºç¡®ä¿å­˜åœ¨çš„ç›®å½•ï¼‰
                save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                # æ‰“å°ç»å¯¹è·¯å¾„ç”¨äºè°ƒè¯•
                logger.info(f"ğŸ“ å®é™…ä¿å­˜è·¯å¾„: {os.path.abspath(save_path)}")
                df.to_csv(save_path, index=False, encoding="utf-8")
                logger.info(f"âœ… ä¿å­˜æˆåŠŸï¼š{save_path}ï¼ˆ{len(df)}æ¡æ•°æ®ï¼‰")
                
                # è®°å½•å·²å®Œæˆï¼ˆç«‹å³æ›´æ–°çŠ¶æ€ï¼‰
                with open(completed_file, "a", encoding="utf-8") as f:
                    f.write(f"{etf_code}\n")
                
                # å•åªçˆ¬å–åçŸ­ä¼‘çœ 
                time.sleep(1)
                
            except Exception as e:
                # å•åªå¤±è´¥ä¸ä¸­æ–­ï¼Œè®°å½•æ—¥å¿—åç»§ç»­
                logger.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}", exc_info=True)
                time.sleep(3)  # å¤±è´¥åå»¶é•¿ä¼‘çœ 
                continue
        
        # æ‰¹æ¬¡é—´é•¿ä¼‘çœ ï¼ˆå‡è½»æœåŠ¡å™¨å‹åŠ›ï¼‰
        if batch_idx < len(batches):
            logger.info(f"æ‰¹æ¬¡{batch_idx}å¤„ç†å®Œæˆï¼Œä¼‘çœ 10ç§’åç»§ç»­...")
            time.sleep(10)
    
    logger.info("===== æ‰€æœ‰å¾…çˆ¬å–ETFå¤„ç†å®Œæ¯• =====")    
