#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ•°æ®çˆ¬å–æ¨¡å—
æä¾›ETFæ—¥çº¿æ•°æ®çˆ¬å–ã€ETFåˆ—è¡¨ç®¡ç†ç­‰åŠŸèƒ½
ç‰¹åˆ«ä¼˜åŒ–äº†å¢é‡ä¿å­˜å’Œæ–­ç‚¹ç»­çˆ¬æœºåˆ¶
"""

import os
import time
import pandas as pd
import logging
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any, Tuple
from retrying import retry
import akshare as ak
from pandas.tseries.offsets import CustomBusinessDay
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday
from config import Config
from .etf_list_manager import update_all_etf_list, get_filtered_etf_codes, load_all_etf_list
from .akshare_crawler import crawl_etf_daily_akshare
from .sina_crawler import crawl_etf_daily_sina
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    is_file_outdated,
    is_trading_day,
    get_last_trading_day  # æ·»åŠ è¿™ä¸€è¡Œ
)
from utils.file_utils import ensure_chinese_columns

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# å®šä¹‰ä¸­å›½è‚¡å¸‚èŠ‚å‡æ—¥æ—¥å†ï¼ˆ2025å¹´ï¼‰
class ChinaStockHolidayCalendar(AbstractHolidayCalendar):
    rules = [
        Holiday("å…ƒæ—¦", month=1, day=1),
        Holiday("æ˜¥èŠ‚", month=1, day=29, observance=lambda d: d + pd.DateOffset(days=+5)),
        Holiday("æ¸…æ˜èŠ‚", month=4, day=4),
        Holiday("åŠ³åŠ¨èŠ‚", month=5, day=1, observance=lambda d: d + pd.DateOffset(days=+2)),
        Holiday("ç«¯åˆèŠ‚", month=6, day=2),
        Holiday("ä¸­ç§‹èŠ‚", month=9, day=8),
        Holiday("å›½åº†èŠ‚", month=10, day=1, observance=lambda d: d + pd.DateOffset(days=+6)),
    ]

# é‡è¯•è£…é¥°å™¨é…ç½®
def retry_if_exception(exception: Exception) -> bool:
    """é‡è¯•æ¡ä»¶ï¼šç½‘ç»œæˆ–æ•°æ®ç›¸å…³é”™è¯¯"""
    return isinstance(exception, (ConnectionError, TimeoutError, ValueError, pd.errors.EmptyDataError))

@retry(
    stop_max_attempt_number=3,
    wait_exponential_multiplier=1000,
    wait_exponential_max=10000,
    retry_on_exception=retry_if_exception
)
def akshare_retry(func, *args, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨å°è£…"""
    return func(*args, **kwargs)

def get_etf_name(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–åç§°
    :param etf_code: ETFä»£ç 
    :return: ETFåç§°
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("å…¨å¸‚åœºETFåˆ—è¡¨ä¸ºç©º")
            return f"ETF-{etf_code}"
        
        target_code = str(etf_code).strip().zfill(6)
        name_row = etf_list[
            etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == target_code
        ]
        
        if not name_row.empty:
            return name_row.iloc[0]["ETFåç§°"]
        else:
            logger.debug(f"æœªåœ¨å…¨å¸‚åœºåˆ—è¡¨ä¸­æ‰¾åˆ°ETFä»£ç : {target_code}")
            return f"ETF-{etf_code}"
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return f"ETF-{etf_code}"

def get_last_crawl_date(etf_code: str, etf_daily_dir: str) -> str:
    """
    è·å–ETFæœ€åçˆ¬å–æ—¥æœŸ
    :param etf_code: ETFä»£ç 
    :param etf_daily_dir: ETFæ—¥çº¿æ•°æ®ç›®å½•
    :return: æœ€åçˆ¬å–æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    """
    try:
        file_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
        if not os.path.exists(file_path):
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›åˆå§‹çˆ¬å–æ—¥æœŸ
            current_date = get_beijing_time().date()
            start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
            logger.debug(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œä½¿ç”¨åˆå§‹æ—¥æœŸ: {start_date}")
            return start_date
        
        df = pd.read_csv(file_path, encoding="utf-8")
        if df.empty or "date" not in df.columns:
            # æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰dateåˆ—ï¼Œè¿”å›åˆå§‹çˆ¬å–æ—¥æœŸ
            current_date = get_beijing_time().date()
            start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
            logger.debug(f"ETF {etf_code} æ•°æ®æ–‡ä»¶å¼‚å¸¸ï¼Œä½¿ç”¨åˆå§‹æ—¥æœŸ: {start_date}")
            return start_date
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        df["date"] = pd.to_datetime(df["date"])
        last_date = df["date"].max().date()
        
        # è®¡ç®—ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºå¼€å§‹æ—¥æœŸ
        china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
        next_trading_day = pd.Timestamp(last_date) + china_bd
        return next_trading_day.date().strftime("%Y-%m-%d")
    except Exception as e:
        logger.error(f"è·å–{etf_code}æœ€åçˆ¬å–æ—¥æœŸå¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶è¿”å›åˆå§‹çˆ¬å–æ—¥æœŸ
        current_date = get_beijing_time().date()
        start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
        logger.debug(f"ETF {etf_code} è·å–æœ€åçˆ¬å–æ—¥æœŸå¤±è´¥ï¼Œä½¿ç”¨åˆå§‹æ—¥æœŸ: {start_date}")
        return start_date

def record_failed_etf(etf_daily_dir: str, etf_code: str, etf_name: str, error_message: Optional[str] = None) -> None:
    """
    è®°å½•å¤±è´¥çš„ETFä¿¡æ¯
    :param etf_daily_dir: ETFæ—¥çº¿æ•°æ®ç›®å½•
    :param etf_code: ETFä»£ç 
    :param etf_name: ETFåç§°
    :param error_message: é”™è¯¯ä¿¡æ¯
    """
    try:
        failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
        timestamp = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(failed_file, "a", encoding="utf-8") as f:
            if error_message:
                f.write(f"{etf_code}|{etf_name}|{timestamp}|{error_message}\n")
            else:
                f.write(f"{etf_code}|{etf_name}|{timestamp}\n")
        
        logger.debug(f"è®°å½•å¤±è´¥ETF: {etf_code} - {etf_name}")
    except Exception as e:
        logger.error(f"è®°å½•å¤±è´¥ETFä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)

def crawl_etf_daily_incremental() -> None:
    """å¢é‡çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼ˆå•åªä¿å­˜+æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼‰
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸å†åŒ…å«æ˜¯å¦æ‰§è¡Œçš„åˆ¤æ–­é€»è¾‘ï¼Œç”±è°ƒç”¨æ–¹å†³å®šæ˜¯å¦æ‰§è¡Œ"""
    try:
        logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_etf_daily =====")
        beijing_time = get_beijing_time()
        logger.info(f"å½“å‰åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        Config.init_dirs()
        etf_daily_dir = Config.ETFS_DAILY_DIR
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–å·²å®Œæˆåˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
                completed_codes = set()
        
        # è·å–å¾…çˆ¬å–ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        to_crawl_codes = [code for code in all_codes if code not in completed_codes]
        total = len(to_crawl_codes)
        if total == 0:
            logger.info("æ‰€æœ‰ETFæ—¥çº¿æ•°æ®å‡å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­")
            return
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total}åª")
        
        # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹50åªï¼‰
        batch_size = Config.CRAWL_BATCH_SIZE
        num_batches = (total + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total)
            batch_codes = to_crawl_codes[start_idx:end_idx]
            
            logger.info(f"å¤„ç†ç¬¬ {batch_idx+1}/{num_batches} æ‰¹ ETF ({len(batch_codes)}åª)")
            
            for etf_code in batch_codes:
                etf_name = get_etf_name(etf_code)
                logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                
                # ç¡®å®šçˆ¬å–æ—¶é—´èŒƒå›´ï¼ˆå¢é‡çˆ¬å–ï¼‰
                start_date = get_last_crawl_date(etf_code, etf_daily_dir)
                
                # è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
                last_trading_day = get_last_trading_day()
                end_date = last_trading_day.strftime("%Y-%m-%d")
                
                if start_date > end_date:
                    logger.info(f"ğŸ“… æ— æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆä¸Šæ¬¡çˆ¬å–è‡³{start_date}ï¼‰")
                    # æ ‡è®°ä¸ºå·²å®Œæˆ
                    with open(completed_file, "a", encoding="utf-8") as f:
                        f.write(f"{etf_code}\n")
                    continue
                
                logger.info(f"ğŸ“… çˆ¬å–æ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
                
                # å…ˆå°è¯•AkShareçˆ¬å–
                df = crawl_etf_daily_akshare(etf_code, start_date, end_date)
                
                # AkShareå¤±è´¥åˆ™å°è¯•æ–°æµªçˆ¬å–
                if df.empty:
                    logger.warning("âš ï¸ AkShareæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ–°æµªæ¥å£")
                    df = crawl_etf_daily_sina(etf_code, start_date, end_date)
                
                # æ•°æ®æ ¡éªŒ
                if df.empty:
                    logger.warning(f"âš ï¸ æ‰€æœ‰æ¥å£å‡æœªè·å–åˆ°æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                    # è®°å½•å¤±è´¥æ—¥å¿—ï¼Œä½†ä¸æ ‡è®°ä¸ºå·²å®Œæˆï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡è¯•
                    record_failed_etf(etf_daily_dir, etf_code, etf_name)
                    continue
                
                # ç¡®ä¿ä½¿ç”¨ä¸­æ–‡åˆ—å
                df = ensure_chinese_columns(df)
                
                # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
                df["ETFä»£ç "] = etf_code
                df["ETFåç§°"] = etf_name
                df["çˆ¬å–æ—¶é—´"] = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
                
                # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
                save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                if os.path.exists(save_path):
                    try:
                        existing_df = pd.read_csv(save_path)
                        # ç¡®ä¿ç°æœ‰æ•°æ®ä¹Ÿæ˜¯ä¸­æ–‡åˆ—å
                        existing_df = ensure_chinese_columns(existing_df)
                        
                        # åˆå¹¶æ•°æ®å¹¶å»é‡
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                        combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                        
                        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
                        combined_df.to_csv(save_path, index=False, encoding="utf-8-sig")
                        
                        logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                    except Exception as e:
                        logger.error(f"åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}ï¼Œå°è¯•è¦†ç›–ä¿å­˜", exc_info=True)
                        df.to_csv(save_path, index=False, encoding="utf-8-sig")
                        logger.info(f"âœ… æ•°æ®å·²è¦†ç›–ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                else:
                    df.to_csv(save_path, index=False, encoding="utf-8-sig")
                    logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                
                # æ ‡è®°ä¸ºå·²å®Œæˆ
                with open(completed_file, "a", encoding="utf-8") as f:
                    f.write(f"{etf_code}\n")
                
                # é™åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(Config.CRAWL_INTERVAL)
            
            # æ‰¹æ¬¡é—´æš‚åœ
            if batch_idx < num_batches - 1:
                logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæš‚åœ {Config.BATCH_INTERVAL} ç§’...")
                time.sleep(Config.BATCH_INTERVAL)
    
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®å¢é‡çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

def update_etf_list() -> bool:
    """
    æ›´æ–°ETFåˆ—è¡¨
    :return: æ˜¯å¦æˆåŠŸæ›´æ–°
    """
    try:
        logger.info("å¼€å§‹æ›´æ–°ETFåˆ—è¡¨")
        etf_list = update_all_etf_list()
        if etf_list.empty:
            logger.warning("ETFåˆ—è¡¨æ›´æ–°åä¸ºç©º")
            return False
        
        logger.info(f"ETFåˆ—è¡¨æ›´æ–°æˆåŠŸï¼Œå…±{len(etf_list)}åªETF")
        return True
    except Exception as e:
        logger.error(f"æ›´æ–°ETFåˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_crawl_status() -> Dict[str, Any]:
    """
    è·å–çˆ¬å–çŠ¶æ€ä¿¡æ¯
    :return: åŒ…å«çˆ¬å–çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    try:
        etf_daily_dir = Config.ETFS_DAILY_DIR
        
        # è·å–å·²å®Œæˆåˆ—è¡¨
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        completed_codes = set()
        if os.path.exists(completed_file):
            with open(completed_file, "r", encoding="utf-8") as f:
                completed_codes = set(line.strip() for line in f if line.strip())
        
        # è·å–å¤±è´¥åˆ—è¡¨
        failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
        failed_count = 0
        if os.path.exists(failed_file):
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_count = len(f.readlines())
        
        # è·å–æ‰€æœ‰ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        
        return {
            "total_etfs": len(all_codes),
            "completed_etfs": len(completed_codes),
            "failed_etfs": failed_count,
            "progress": f"{len(completed_codes)}/{len(all_codes)}",
            "percentage": round(len(completed_codes) / len(all_codes) * 100, 2) if all_codes else 0
        }
    except Exception as e:
        logger.error(f"è·å–çˆ¬å–çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "total_etfs": 0,
            "completed_etfs": 0,
            "failed_etfs": 0,
            "progress": "0/0",
            "percentage": 0
        }

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    if Config.init_dirs():
        logger.info("æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    else:
        logger.warning("æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œä½†å­˜åœ¨è­¦å‘Š")
except Exception as e:
    logger.error(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
    
    # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
    try:
        import logging
        logging.basicConfig(
            level="INFO",
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler()]
        )
        logging.error(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    except Exception as basic_log_error:
        print(f"åŸºç¡€æ—¥å¿—é…ç½®å¤±è´¥: {str(basic_log_error)}")
        print(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
