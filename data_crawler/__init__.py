# crawler_init.py
import os
import time
import pandas as pd
import logging
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any, Tuple
from retrying import retry
import akshare as ak
from pandas.tseries.offsets import CustomBusinessDay
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday
from config import Config
from .etf_list_manager import update_all_etf_list, get_filtered_etf_codes, load_all_etf_list
from .akshare_crawler import crawl_etf_daily_akshare
from .sina_crawler import crawl_etf_daily_sina
from utils.date_utils import get_beijing_time
# åˆ é™¤ä»¥ä¸‹å¯¼å…¥ï¼Œæ”¹ç”¨ Config.init_dirs()
# from utils.file_utils import init_dirs

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# å®šä¹‰ä¸­å›½è‚¡å¸‚èŠ‚å‡æ—¥æ—¥å†ï¼ˆ2025å¹´ï¼‰
class ChinaStockHolidayCalendar(AbstractHolidayCalendar):
    rules = [
        Holiday("å…ƒæ—¦", month=1, day=1),
        Holiday("æ˜¥èŠ‚", month=1, day=29, observance=lambda d: d + pd.DateOffset(days=+5)),
        Holiday("æ¸…æ˜èŠ‚", month=4, day=4),
        Holiday("åŠ³åŠ¨èŠ‚", month=5, day=1, observance=lambda d: d + pd.DateOffset(days=+2)),
        Holiday("ç«¯åˆèŠ‚", month=6, day=2),
        Holiday("ä¸­ç§‹èŠ‚", month=9, day=8),
        Holiday("å›½åº†èŠ‚", month=10, day=1, observance=lambda d: d + pd.DateOffset(days=+6)),
    ]

# é‡è¯•è£…é¥°å™¨é…ç½®
def retry_if_exception(exception: Exception) -> bool:
    """é‡è¯•æ¡ä»¶ï¼šç½‘ç»œæˆ–æ•°æ®ç›¸å…³é”™è¯¯"""
    return isinstance(exception, (ConnectionError, TimeoutError, ValueError, pd.errors.EmptyDataError))

@retry(
    stop_max_attempt_number=3,
    wait_exponential_multiplier=1000,
    wait_exponential_max=10000,
    retry_on_exception=retry_if_exception
)
def akshare_retry(func, *args, **kwargs):
    """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨å°è£…"""
    return func(*args, **kwargs)

def is_trading_day(check_date: date) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¥
    :param check_date: æ£€æŸ¥æ—¥æœŸ
    :return: å¦‚æœæ˜¯äº¤æ˜“æ—¥è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        if check_date.weekday() >= 5:  # å‘¨å…­æˆ–å‘¨æ—¥
            return False
        
        china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
        return pd.Timestamp(check_date) == (pd.Timestamp(check_date) + 0 * china_bd)
    except Exception as e:
        logger.error(f"äº¤æ˜“æ—¥åˆ¤æ–­å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_etf_name(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–åç§°
    :param etf_code: ETFä»£ç 
    :return: ETFåç§°
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("å…¨å¸‚åœºETFåˆ—è¡¨ä¸ºç©º")
            return f"ETF-{etf_code}"
        
        target_code = str(etf_code).strip().zfill(6)
        name_row = etf_list[
            etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == target_code
        ]
        
        if not name_row.empty:
            return name_row.iloc[0]["ETFåç§°"]
        else:
            logger.debug(f"æœªåœ¨å…¨å¸‚åœºåˆ—è¡¨ä¸­æ‰¾åˆ°ETFä»£ç : {target_code}")
            return f"ETF-{etf_code}"
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
        return f"ETF-{etf_code}"

def get_last_crawl_date(etf_code: str, etf_daily_dir: str) -> str:
    """
    è·å–æœ€åä¸€æ¬¡çˆ¬å–çš„æ—¥æœŸ
    :param etf_code: ETFä»£ç 
    :param etf_daily_dir: ETFæ—¥çº¿æ•°æ®ç›®å½•
    :return: å¼€å§‹çˆ¬å–çš„æ—¥æœŸ
    """
    try:
        file_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
        if not os.path.exists(file_path):
            # æ²¡æœ‰å­˜é‡æ•°æ®ï¼Œè¿”å›æœ€è¿‘ä¸€å¹´çš„æ—¥æœŸ
            current_date = get_beijing_time().date()
            start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
            logger.debug(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œä½¿ç”¨åˆå§‹æ—¥æœŸ: {start_date}")
            return start_date
        
        df = pd.read_csv(file_path, encoding="utf-8")
        if df.empty or "date" not in df.columns:
            # æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰dateåˆ—ï¼Œè¿”å›æœ€è¿‘ä¸€å¹´çš„æ—¥æœŸ
            current_date = get_beijing_time().date()
            start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
            logger.debug(f"ETF {etf_code} æ•°æ®æ–‡ä»¶å¼‚å¸¸ï¼Œä½¿ç”¨åˆå§‹æ—¥æœŸ: {start_date}")
            return start_date
        
        last_date = df["date"].max()
        # è®¡ç®—ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºå¼€å§‹æ—¥æœŸ
        china_bd = CustomBusinessDay(calendar=ChinaStockHolidayCalendar())
        next_date = (pd.to_datetime(last_date) + china_bd).strftime("%Y-%m-%d")
        logger.debug(f"ETF {etf_code} æœ€åçˆ¬å–æ—¥æœŸ: {last_date}, ä¸‹æ¬¡å¼€å§‹æ—¥æœŸ: {next_date}")
        return next_date
    except Exception as e:
        logger.warning(f"è·å–ETF {etf_code} æœ€åçˆ¬å–æ—¥æœŸå¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨æœ€è¿‘ä¸€å¹´æ—¥æœŸ")
        current_date = get_beijing_time().date()
        start_date = (current_date - timedelta(days=Config.INITIAL_CRAWL_DAYS)).strftime("%Y-%m-%d")
        return start_date

def record_failed_etf(etf_daily_dir: str, etf_code: str, etf_name: str, error_message: Optional[str] = None) -> None:
    """
    è®°å½•å¤±è´¥çš„ETFä¿¡æ¯
    :param etf_daily_dir: ETFæ—¥çº¿æ•°æ®ç›®å½•
    :param etf_code: ETFä»£ç 
    :param etf_name: ETFåç§°
    :param error_message: é”™è¯¯ä¿¡æ¯
    """
    try:
        failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
        timestamp = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
        
        with open(failed_file, "a", encoding="utf-8") as f:
            if error_message:
                f.write(f"{etf_code}|{etf_name}|{timestamp}|{error_message}\n")
            else:
                f.write(f"{etf_code}|{etf_name}|{timestamp}\n")
        
        logger.debug(f"è®°å½•å¤±è´¥ETF: {etf_code} - {etf_name}")
    except Exception as e:
        logger.error(f"è®°å½•å¤±è´¥ETFä¿¡æ¯å¤±è´¥: {str(e)}")

def crawl_etf_daily_incremental() -> None:
    """
    å¢é‡çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼ˆå•åªä¿å­˜+æ–­ç‚¹ç»­çˆ¬é€»è¾‘ï¼‰
    æ”¯æŒäº¤æ˜“æ—¥åˆ¤æ–­å’Œåˆ†æ‰¹çˆ¬å–ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
    """
    try:
        logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_etf_daily =====")
        current_time = get_beijing_time()
        current_date = current_time.date()
        logger.info(f"å½“å‰æ—¶é—´ï¼š{current_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰")
        
        # éäº¤æ˜“æ—¥ä¸”æœªåˆ°è¡¥çˆ¬æ—¶é—´ï¼ˆ18ç‚¹åå…è®¸è¡¥çˆ¬ï¼‰
        if not is_trading_day(current_date) and current_time.hour < 18:
            logger.info(f"ä»Šæ—¥{current_date}éäº¤æ˜“æ—¥ä¸”æœªåˆ°è¡¥çˆ¬æ—¶é—´ï¼Œæ— éœ€çˆ¬å–æ—¥çº¿æ•°æ®")
            return
        
        # åˆå§‹åŒ–ç›®å½• - ä½¿ç”¨ Config.init_dirs() è€Œä¸æ˜¯ utils.file_utils.init_dirs
        Config.init_dirs()
        root_dir = Config.BASE_DIR
        etf_daily_dir = Config.DATA_DIR
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–å·²å®Œæˆåˆ—è¡¨å¤±è´¥: {str(e)}")
                completed_codes = set()
        
        # è·å–å¾…çˆ¬å–ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        to_crawl_codes = [code for code in all_codes if code not in completed_codes]
        total = len(to_crawl_codes)
        
        if total == 0:
            logger.info("æ‰€æœ‰ETFæ—¥çº¿æ•°æ®å‡å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total}åª")
        
        # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹50åªï¼‰
        batch_size = Config.CRAWL_BATCH_SIZE
        batches = [to_crawl_codes[i:i+batch_size] for i in range(0, total, batch_size)]
        logger.info(f"å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} åªETF")
        
        # é€æ‰¹ã€é€åªçˆ¬å–
        for batch_idx, batch in enumerate(batches, 1):
            batch_num = len(batch)
            logger.info(f"==============================")
            logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(batches)}")
            logger.info(f"ETFèŒƒå›´ï¼š{batch_idx*batch_size - batch_size + 1}-{min(batch_idx*batch_size, total)}åªï¼ˆå…±{batch_num}åªï¼‰")
            logger.info(f"==============================")
            
            for idx, etf_code in enumerate(batch, 1):
                try:
                    # æ‰“å°å½“å‰è¿›åº¦
                    logger.info(f"--- æ‰¹æ¬¡{batch_idx} - ç¬¬{idx}åª / å…±{batch_num}åª ---")
                    etf_name = get_etf_name(etf_code)
                    logger.info(f"ETFä»£ç ï¼š{etf_code} | åç§°ï¼š{etf_name}")
                    
                    # ç¡®å®šçˆ¬å–æ—¶é—´èŒƒå›´ï¼ˆå¢é‡çˆ¬å–ï¼‰
                    start_date = get_last_crawl_date(etf_code, etf_daily_dir)
                    end_date = current_date.strftime("%Y-%m-%d")
                    
                    if start_date > end_date:
                        logger.info(f"ğŸ“… æ— æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆä¸Šæ¬¡çˆ¬å–è‡³{start_date}ï¼‰")
                        # æ ‡è®°ä¸ºå·²å®Œæˆ
                        with open(completed_file, "a", encoding="utf-8") as f:
                            f.write(f"{etf_code}\n")
                        continue
                    
                    logger.info(f"ğŸ“… çˆ¬å–æ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
                    
                    # å…ˆå°è¯•AkShareçˆ¬å–
                    df = crawl_etf_daily_akshare(etf_code, start_date, end_date)
                    
                    # AkShareå¤±è´¥åˆ™å°è¯•æ–°æµªçˆ¬å–
                    if df.empty:
                        logger.warning("âš ï¸ AkShareæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ–°æµªæ¥å£")
                        df = crawl_etf_daily_sina(etf_code, start_date, end_date)
                    
                    # æ•°æ®æ ¡éªŒ
                    if df.empty:
                        logger.warning(f"âš ï¸ æ‰€æœ‰æ¥å£å‡æœªè·å–åˆ°æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                        # è®°å½•å¤±è´¥æ—¥å¿—ï¼Œä½†ä¸æ ‡è®°ä¸ºå·²å®Œæˆï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡è¯•
                        record_failed_etf(etf_daily_dir, etf_code, etf_name)
                        continue
                    
                    # ç»Ÿä¸€åˆ—åï¼ˆè½¬ä¸ºè‹±æ–‡åˆ—åï¼Œä½¿ç”¨config.pyä¸­çš„æ ‡å‡†å®šä¹‰ï¼‰
                    col_map = Config.STANDARD_COLUMNS
                    df = df.rename(columns=col_map)
                    
                    # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
                    df["etf_code"] = etf_code
                    df["etf_name"] = etf_name
                    df["crawl_time"] = current_time.strftime("%Y-%m-%d %H:%M:%S")
                    
                    # ç¡®ä¿æ‰€æœ‰æ ‡å‡†åˆ—éƒ½å­˜åœ¨
                    for col in Config.STANDARD_COLUMNS.values():
                        if col not in df.columns:
                            # å¡«å……ç¼ºå¤±çš„åˆ—ï¼ˆé™¤äº†etf_code, etf_name, crawl_timeå·²ç»åœ¨ä¸Šé¢æ·»åŠ ï¼‰
                            if col == "amplitude" and "æŒ¯å¹…" in df.columns:
                                df[col] = df["æŒ¯å¹…"]
                            elif col == "price_change" and "æ¶¨è·Œé¢" in df.columns:
                                df[col] = df["æ¶¨è·Œé¢"]
                            elif col == "turnover" and "æ¢æ‰‹ç‡" in df.columns:
                                df[col] = df["æ¢æ‰‹ç‡"]
                            else:
                                df[col] = None  # å¡«å……ç©ºå€¼
                    
                    # åªä¿ç•™æ ‡å‡†åˆ—
                    df = df[list(Config.STANDARD_COLUMNS.values())]
                    
                    # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
                    save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                    if os.path.exists(save_path):
                        try:
                            existing_df = pd.read_csv(save_path, encoding="utf-8")
                            # å»é‡ååˆå¹¶
                            combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=["date"], keep="last")
                            # æŒ‰æ—¥æœŸæ’åº
                            combined_df["date"] = pd.to_datetime(combined_df["date"])
                            combined_df = combined_df.sort_values("date").reset_index(drop=True)
                            combined_df["date"] = combined_df["date"].dt.strftime("%Y-%m-%d")
                            df = combined_df
                        except Exception as e:
                            logger.error(f"åˆå¹¶ç°æœ‰æ•°æ®å¤±è´¥: {str(e)}ï¼Œå°†è¦†ç›–åŸæ–‡ä»¶")
                    
                    # ä¿å­˜æ•°æ®
                    df.to_csv(save_path, index=False, encoding="utf-8")
                    logger.info(f"âœ… ä¿å­˜æˆåŠŸï¼š{save_path}ï¼ˆå…±{len(df)}æ¡æ•°æ®ï¼‰")
                    
                    # è®°å½•å·²å®Œæˆ
                    with open(completed_file, "a", encoding="utf-8") as f:
                        f.write(f"{etf_code}\n")
                    
                    # å•åªçˆ¬å–åçŸ­ä¼‘çœ 
                    time.sleep(1)
                    
                except Exception as e:
                    # å•åªå¤±è´¥ä¸ä¸­æ–­ï¼Œè®°å½•æ—¥å¿—åç»§ç»­
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}", exc_info=True)
                    # è®°å½•å¤±è´¥æ—¥å¿—
                    record_failed_etf(etf_daily_dir, etf_code, etf_name, str(e))
                    time.sleep(3)  # å¤±è´¥åå»¶é•¿ä¼‘çœ 
                    continue
            
            # æ‰¹æ¬¡é—´é•¿ä¼‘çœ ï¼ˆå‡è½»æœåŠ¡å™¨å‹åŠ›ï¼‰
            if batch_idx < len(batches):
                logger.info(f"æ‰¹æ¬¡{batch_idx}å¤„ç†å®Œæˆï¼Œä¼‘çœ 10ç§’åç»§ç»­...")
                time.sleep(10)
        
        logger.info("===== æ‰€æœ‰å¾…çˆ¬å–ETFå¤„ç†å®Œæ¯• =====")
        
    except Exception as e:
        logger.error(f"å¢é‡çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

def update_etf_list() -> bool:
    """
    æ›´æ–°ETFåˆ—è¡¨
    :return: æ˜¯å¦æˆåŠŸæ›´æ–°
    """
    try:
        logger.info("å¼€å§‹æ›´æ–°ETFåˆ—è¡¨")
        etf_list = update_all_etf_list()
        if etf_list.empty:
            logger.warning("ETFåˆ—è¡¨æ›´æ–°åä¸ºç©º")
            return False
        
        logger.info(f"ETFåˆ—è¡¨æ›´æ–°æˆåŠŸï¼Œå…±{len(etf_list)}åªETF")
        return True
    except Exception as e:
        logger.error(f"æ›´æ–°ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
        return False

def get_crawl_status() -> Dict[str, Any]:
    """
    è·å–çˆ¬å–çŠ¶æ€ä¿¡æ¯
    :return: åŒ…å«çˆ¬å–çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    try:
        root_dir = Config.BASE_DIR
        etf_daily_dir = Config.DATA_DIR
        
        # è·å–å·²å®Œæˆåˆ—è¡¨
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        completed_codes = set()
        if os.path.exists(completed_file):
            with open(completed_file, "r", encoding="utf-8") as f:
                completed_codes = set(line.strip() for line in f if line.strip())
        
        # è·å–å¤±è´¥åˆ—è¡¨
        failed_file = os.path.join(etf_daily_dir, "failed_etfs.txt")
        failed_count = 0
        if os.path.exists(failed_file):
            with open(failed_file, "r", encoding="utf-8") as f:
                failed_count = len(f.readlines())
        
        # è·å–æ‰€æœ‰ETFåˆ—è¡¨
        all_codes = get_filtered_etf_codes()
        
        return {
            "total_etfs": len(all_codes),
            "completed_etfs": len(completed_codes),
            "failed_etfs": failed_count,
            "progress": f"{len(completed_codes)}/{len(all_codes)}",
            "percentage": round(len(completed_codes) / len(all_codes) * 100, 2) if all_codes else 0
        }
    except Exception as e:
        logger.error(f"è·å–çˆ¬å–çŠ¶æ€å¤±è´¥: {str(e)}")
        return {
            "total_etfs": 0,
            "completed_etfs": 0,
            "failed_etfs": 0,
            "progress": "0/0",
            "percentage": 0
        }

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Config.init_dirs()
    
    # åˆå§‹åŒ–ETFåˆ—è¡¨
    update_etf_list()
    
    logger.info("æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    logger.error(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
    import logging
    logging.basicConfig(level=Config.LOG_LEVEL, format=Config.LOG_FORMAT)
    logging.error(f"æ•°æ®çˆ¬å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
