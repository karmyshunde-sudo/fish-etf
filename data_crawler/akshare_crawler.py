#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨AkShareæ¥å£è·å–ETFæ—¥çº¿æ•°æ®
ç‰¹åˆ«ä¼˜åŒ–äº†åˆ—åæ˜ å°„å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import akshare as ak
import pandas as pd
import logging
import time
from typing import Optional, Dict, Any, Tuple
from datetime import datetime, timedelta, date
from config import Config
from retrying import retry

# ä¿®å¤ï¼šæ­£ç¡®å¯¼å…¥å‡½æ•°
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    get_last_trading_day,
    is_trading_day
)
# ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
from utils.file_utils import (
    ensure_chinese_columns, internal_ensure_chinese_columns
)
from utils.data_processor import (
    ensure_required_columns,
    clean_and_format_data,
    limit_to_one_year_data
)
# ä»…æ·»åŠ å¿…è¦çš„gitå·¥å…·å¯¼å…¥ï¼ˆä¸æ·»åŠ ä»»ä½•æ–°å‡½æ•°ï¼‰
from utils.git_utils import commit_files_in_batches

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRY_ATTEMPTS = 5
RETRY_WAIT_FIXED = 3000
RETRY_WAIT_EXPONENTIAL_MAX = 15000

# æ‰“å°AkShareç‰ˆæœ¬
logger.info(f"AkShareç‰ˆæœ¬: {ak.__version__}")

def empty_result_check(result: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥AkShareè¿”å›ç»“æœæ˜¯å¦ä¸ºç©º
    
    Args:
        result: AkShareè¿”å›çš„DataFrame
        
    Returns:
        bool: å¦‚æœç»“æœä¸ºç©ºè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return result is None or result.empty

def retry_if_akshare_error(exception: Exception) -> bool:
    """
    é‡è¯•æ¡ä»¶ï¼šAkShareç›¸å…³é”™è¯¯
    
    Args:
        exception: å¼‚å¸¸å¯¹è±¡
        
    Returns:
        bool: å¦‚æœæ˜¯AkShareé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    from requests.exceptions import ConnectionError, Timeout
    return isinstance(exception, (ValueError, ConnectionError, Timeout, OSError))

@retry(
    stop_max_attempt_number=MAX_RETRY_ATTEMPTS,
    wait_fixed=RETRY_WAIT_FIXED,
    retry_on_result=empty_result_check,
    retry_on_exception=retry_if_akshare_error
)
def crawl_etf_daily_akshare(etf_code: str, start_date: str, end_date: str, is_first_crawl: bool = False) -> pd.DataFrame:
    """ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    
    Args:
        etf_code: ETFä»£ç  (6ä½æ•°å­—)
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        is_first_crawl: æ˜¯å¦æ˜¯é¦–æ¬¡çˆ¬å–
    
    Returns:
        pd.DataFrame: åŒ…å«ETFæ—¥çº¿æ•°æ®çš„DataFrame
    """
    try:
        # ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdateå¯¹è±¡
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day(end_date_obj)
        # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
        end_date = last_trading_day.strftime("%Y-%m-%d")
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†å•æ—¥è¯·æ±‚é—®é¢˜
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
        if start_date_obj == last_trading_day:
            # å¦‚æœæ˜¯å•æ—¥è¯·æ±‚ï¼Œæ‰©å±•ä¸ºè‡³å°‘3å¤©çš„èŒƒå›´
            start_date_obj = start_date_obj - timedelta(days=2)
            start_date = start_date_obj.strftime("%Y-%m-%d")
            logger.info(f"å•æ—¥è¯·æ±‚æ‰©å±•ä¸º {start_date} è‡³ {end_date}")
        
        logger.info(f"å¼€å§‹çˆ¬å–ETF {etf_code} çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        
        # ä¼˜å…ˆä½¿ç”¨stock_zh_a_histè·å–å®Œæ•´æ•°æ®ï¼ˆæœ€å®Œæ•´æ¥å£ï¼‰
        df = try_stock_zh_a_hist(etf_code, start_date, end_date)
        
        # å¦‚æœstock_zh_a_histå¤±è´¥ï¼Œå°è¯•fund_etf_hist_sina
        if df.empty:
            logger.info(f"stock_zh_a_histè·å–å¤±è´¥ï¼Œå°è¯•fund_etf_hist_sina")
            df = try_fund_etf_hist_sina(etf_code, start_date, end_date)
        
        # å¦‚æœä¸»è¦æ¥å£éƒ½å¤±è´¥ï¼Œå°è¯•fund_etf_spot_emè·å–å®æ—¶æ•°æ®
        if df.empty:
            logger.info(f"ä¸»è¦æ¥å£è·å–å¤±è´¥ï¼Œå°è¯•fund_etf_spot_emè·å–å®æ—¶æ•°æ®")
            df = try_fund_etf_spot_em(etf_code, start_date, end_date)
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦æˆåŠŸè·å–
        if df.empty:
            logger.warning(f"æ‰€æœ‰æ•°æ®æºå‡æœªè·å–åˆ°{etf_code}æ•°æ®ï¼ˆ{start_date}è‡³{end_date}ï¼‰")
            return pd.DataFrame()
        
        # æ ‡å‡†åŒ–åˆ—å
        df = ensure_chinese_columns(df)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨ï¼ˆä¸è¿›è¡Œè®¡ç®—ï¼Œåªæ£€æŸ¥ï¼‰
        df = ensure_required_columns(df)
        
        # æ•°æ®æ¸…æ´—ï¼šå»é‡ã€æ ¼å¼è½¬æ¢
        df = clean_and_format_data(df)
        
        # é¦–æ¬¡çˆ¬å–æ—¶é™åˆ¶æ•°æ®é‡ä¸º1å¹´
        if is_first_crawl:
            df = limit_to_one_year_data(df, end_date)
        
        logger.info(f"æˆåŠŸè·å–ETF {etf_code} æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
        return df
    except Exception as e:
        logger.error(f"çˆ¬å–ETF {etf_code} å¤±è´¥: {str(e)}", exc_info=True)
        raise

def try_stock_zh_a_hist(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """ä½¿ç”¨stock_zh_a_histæ¥å£è·å–ETFæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰"""
    try:
        logger.info(f"å°è¯•ä½¿ç”¨stock_zh_a_histæ¥å£è·å–ETF {etf_code} æ•°æ®")
        # å…³é”®ä¿®å¤ï¼šä¸º510300ç­‰ä¸Šäº¤æ‰€ETFæ·»åŠ "sh"å‰ç¼€
        symbol = f"sh{etf_code}" if etf_code.startswith('5') else f"sz{etf_code}"
        df = ak.index_zh_a_hist(
            symbol=symbol,
            period="daily",
            start_date=start_date,
            end_date=end_date
        )
        
        if not df.empty:
            logger.info(f"stock_zh_a_hist æ¥å£æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š stock_zh_a_hist æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.rename(columns={
                'æ—¥æœŸ': 'æ—¥æœŸ',
                'å¼€ç›˜': 'å¼€ç›˜',
                'æœ€é«˜': 'æœ€é«˜',
                'æœ€ä½': 'æœ€ä½',
                'æ”¶ç›˜': 'æ”¶ç›˜',
                'æˆäº¤é‡': 'æˆäº¤é‡',
                'æˆäº¤é¢': 'æˆäº¤é¢',
                'æŒ¯å¹…': 'æŒ¯å¹…',
                'æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…',
                'æ¶¨è·Œé¢': 'æ¶¨è·Œé¢',
                'æ¢æ‰‹ç‡': 'æ¢æ‰‹ç‡'
            })
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
            
            # æ·»åŠ ETFä»£ç å’Œåç§°ï¼ˆä½†ä¸åœ¨æ­¤å¤„è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼‰
            if 'ETFä»£ç ' not in df.columns:
                df['ETFä»£ç '] = etf_code
            if 'ETFåç§°' not in df.columns:
                # åç§°éœ€è¦åœ¨å¤–éƒ¨è·å–ï¼Œè¿™é‡Œç•™ç©º
                df['ETFåç§°'] = ""
                
            return df
    except Exception as e:
        logger.debug(f"stock_zh_a_histæ¥å£å¤±è´¥: {str(e)}")
    
    return pd.DataFrame()

def try_fund_etf_hist_sina(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """ä½¿ç”¨fund_etf_hist_sinaæ¥å£è·å–ETFæ•°æ®"""
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_hist_sinaæ¥å£è·å–ETF {etf_code} æ•°æ®")
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¸‚åœºå‰ç¼€
        symbol = get_symbol_with_market_prefix(etf_code)
        df = ak.fund_etf_hist_sina(symbol=symbol)
        
        if not df.empty:
            logger.info(f"fund_etf_hist_sina æ¥å£æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š fund_etf_hist_sina æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢',
                'amplitude': 'æŒ¯å¹…',
                'percent': 'æ¶¨è·Œå¹…',
                'change': 'æ¶¨è·Œé¢',
                'turnover_rate': 'æ¢æ‰‹ç‡',
                'trade_date': 'æ—¥æœŸ',
                'open_price': 'å¼€ç›˜',
                'high_price': 'æœ€é«˜',
                'low_price': 'æœ€ä½',
                'close_price': 'æ”¶ç›˜',
                'vol': 'æˆäº¤é‡',
                'amount_volume': 'æˆäº¤é¢',
                'amplitude_percent': 'æŒ¯å¹…',
                'pct_chg': 'æ¶¨è·Œå¹…',
                'price_change': 'æ¶¨è·Œé¢',
                'turnover_ratio': 'æ¢æ‰‹ç‡',
                'net_value': 'å‡€å€¼',
                'iopv': 'IOPV'
            }
            # é‡å‘½ååˆ—
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
            if 'æ—¥æœŸ' not in df.columns and 'date' in df.columns:
                df = df.rename(columns={'date': 'æ—¥æœŸ'})
            
            # æ—¥æœŸè¿‡æ»¤
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
                mask = (df['æ—¥æœŸ'] >= start_date) & (df['æ—¥æœŸ'] <= end_date)
                df = df.loc[mask]
            
            # æ·»åŠ ETFä»£ç å’Œåç§°
            if 'ETFä»£ç ' not in df.columns:
                df['ETFä»£ç '] = etf_code
            if 'ETFåç§°' not in df.columns:
                df['ETFåç§°'] = ""
                
            return df
    except Exception as e:
        logger.debug(f"fund_etf_hist_sinaæ¥å£å¤±è´¥: {str(e)}")
    
    return pd.DataFrame()

def try_fund_etf_spot_em(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """ä½¿ç”¨fund_etf_spot_emæ¥å£è·å–ETFå®æ—¶æ•°æ®ï¼ˆä»…æœ€æ–°æ•°æ®ï¼‰"""
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_spot_emæ¥å£è·å–ETF {etf_code} å®æ—¶æ•°æ®")
        df = ak.fund_etf_spot_em()
        
        if not df.empty:
            logger.info(f"fund_etf_spot_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # è¿‡æ»¤æŒ‡å®šETF
            df = df[df["ä»£ç "] == etf_code]
            
            if not df.empty:
                # æ ‡å‡†åŒ–åˆ—å
                column_mapping = {
                    "ä»£ç ": "ETFä»£ç ",
                    "åç§°": "ETFåç§°",
                    "æœ€æ–°ä»·": "æ”¶ç›˜",
                    "IOPVå®æ—¶ä¼°å€¼": "IOPV",
                    "åŸºé‡‘æŠ˜ä»·ç‡": "æŠ˜æº¢ä»·ç‡",
                    "æ¶¨è·Œé¢": "æ¶¨è·Œé¢",
                    "æ¶¨è·Œå¹…": "æ¶¨è·Œå¹…",
                    "æˆäº¤é‡": "æˆäº¤é‡",
                    "æˆäº¤é¢": "æˆäº¤é¢",
                    "å¼€ç›˜ä»·": "å¼€ç›˜",
                    "æœ€é«˜ä»·": "æœ€é«˜",
                    "æœ€ä½ä»·": "æœ€ä½",
                    "æ˜¨æ”¶": "å‰æ”¶ç›˜",
                    "æŒ¯å¹…": "æŒ¯å¹…",
                    "æ¢æ‰‹ç‡": "æ¢æ‰‹ç‡",
                    "æ•°æ®æ—¥æœŸ": "æ—¥æœŸ"
                }
                df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
                
                # ç¡®ä¿æ—¥æœŸæ ¼å¼
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
                
                # è®¾ç½®æˆäº¤é‡å•ä½ä¸º"æ‰‹"ï¼ˆ1æ‰‹=100è‚¡ï¼‰
                if "æˆäº¤é‡" in df.columns:
                    df["æˆäº¤é‡"] = df["æˆäº¤é‡"] / 100
                
                # ä»…ä¿ç•™åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                if "æ—¥æœŸ" in df.columns:
                    mask = (df["æ—¥æœŸ"] >= start_date) & (df["æ—¥æœŸ"] <= end_date)
                    df = df.loc[mask]
                
                return df
    except Exception as e:
        logger.debug(f"fund_etf_spot_emæ¥å£å¤±è´¥: {str(e)}")
    
    return pd.DataFrame()

def get_symbol_with_market_prefix(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        str: å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    """
    if etf_code.startswith(('5', '6', '9')):
        return f"sh{etf_code}"
    else:
        return f"sz{etf_code}"

def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰å¿…éœ€çš„äº¤æ˜“æ•°æ®åˆ—
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼Œä¸è¿›è¡Œä»»ä½•è®¡ç®—
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    if df.empty:
        return df
    
    # å¿…éœ€åˆ—åˆ—è¡¨
    required_columns = [
        "æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", 
        "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡", 
        "ETFä»£ç ", "ETFåç§°", "æŠ˜æº¢ä»·ç‡"
    ]
    
    # æ£€æŸ¥ç¼ºå¤±åˆ—
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"âŒ æ•°æ®æºç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(missing_columns)}")
        # ä¸å°è¯•ä¿®å¤ï¼Œåªè®°å½•é”™è¯¯
        return pd.DataFrame()
    
    return df

def clean_and_format_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—å¹¶æ ¼å¼åŒ–æ•°æ®
    """
    try:
        # åˆ›å»ºDataFrameçš„æ·±æ‹·è´ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # å¤„ç†æ—¥æœŸåˆ—
        if "æ—¥æœŸ" in df.columns:
            # å°è¯•å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeç±»å‹
            try:
                # å…ˆç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºå¤„ç†å„ç§å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
                # å°è¯•è½¬æ¢ä¸ºdatetime
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                df["æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")
            except Exception as e:
                logger.error(f"æ—¥æœŸåˆ—å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        
        # ä¿æŒåŸå§‹åˆ—é¡ºåº
        required_columns = [
            "æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", 
            "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡", 
            "ETFä»£ç ", "ETFåç§°", "æŠ˜æº¢ä»·ç‡"
        ]
        
        # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        final_columns = [col for col in required_columns if col in df.columns]
        if final_columns:
            df = df[final_columns]
        
        # ç§»é™¤é‡å¤è¡Œ
        df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        
        # æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values("æ—¥æœŸ", ascending=False)
        
        return df
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return pd.DataFrame()

def limit_to_one_year_data(df: pd.DataFrame, end_date: str) -> pd.DataFrame:
    """
    é™åˆ¶æ•°æ®ä¸ºæœ€è¿‘1å¹´çš„æ•°æ®
    
    Args:
        df: åŸå§‹DataFrame
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: é™åˆ¶ä¸º1å¹´æ•°æ®åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸ
        one_year_ago = (pd.to_datetime(end_date) - timedelta(days=365)).strftime("%Y-%m-%d")
        
        # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            logger.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œæ— æ³•é™åˆ¶ä¸º1å¹´æ•°æ®")
            return df
        
        # åˆ›å»ºDataFrameçš„å‰¯æœ¬ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # è½¬æ¢æ—¥æœŸåˆ—
        df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è¿‡æ»¤æ•°æ®
        mask = df["æ—¥æœŸ"] >= pd.to_datetime(one_year_ago)
        df = df.loc[mask]
        
        logger.info(f"æ•°æ®å·²é™åˆ¶ä¸ºæœ€è¿‘1å¹´ï¼ˆä» {one_year_ago} è‡³ {end_date}ï¼‰ï¼Œå‰©ä½™ {len(df)} æ¡æ•°æ®")
        return df
    except Exception as e:
        logger.error(f"é™åˆ¶æ•°æ®ä¸º1å¹´æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return df
