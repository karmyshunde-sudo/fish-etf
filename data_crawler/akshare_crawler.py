#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨AkShareæ¥å£è·å–ETFæ—¥çº¿æ•°æ®
ç‰¹åˆ«ä¼˜åŒ–äº†åˆ—åæ˜ å°„å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import akshare as ak
import pandas as pd
import logging
import time
from typing import Optional, Dict, Any, Tuple
from datetime import datetime, timedelta, date
from config import Config
from retrying import retry
from utils.date_utils import get_beijing_time  # å¯¼å…¥åŒ—äº¬æ—¶é—´å·¥å…·

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRY_ATTEMPTS = 3
RETRY_WAIT_FIXED = 2000  # æ¯«ç§’
RETRY_WAIT_EXPONENTIAL_MAX = 10000  # æ¯«ç§’

# æ‰“å°AkShareç‰ˆæœ¬
logger.info(f"AkShareç‰ˆæœ¬: {ak.__version__}")

def empty_result_check(result: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥AkShareè¿”å›ç»“æœæ˜¯å¦ä¸ºç©º
    
    Args:
        result: AkShareè¿”å›çš„DataFrame
        
    Returns:
        bool: å¦‚æœç»“æœä¸ºç©ºè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return result is None or result.empty

def retry_if_akshare_error(exception: Exception) -> bool:
    """
    é‡è¯•æ¡ä»¶ï¼šAkShareç›¸å…³é”™è¯¯
    
    Args:
        exception: å¼‚å¸¸å¯¹è±¡
        
    Returns:
        bool: å¦‚æœæ˜¯AkShareé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return isinstance(exception, (ValueError, ConnectionError, TimeoutError))

@retry(stop_max_attempt_number=MAX_RETRY_ATTEMPTS,
       wait_fixed=RETRY_WAIT_FIXED,
       retry_on_result=empty_result_check,
       retry_on_exception=retry_if_akshare_error)
def crawl_etf_daily_akshare(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    Args:
        etf_code: ETFä»£ç  (6ä½æ•°å­—)
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    Returns:
        pd.DataFrame: åŒ…å«ETFæ—¥çº¿æ•°æ®çš„DataFrame
    """
    try:
        # ç¡®ä¿ç»“æŸæ—¥æœŸæ˜¯äº¤æ˜“æ—¥
        end_date = get_last_trading_day(end_date).strftime("%Y-%m-%d")
        
        logger.info(f"å¼€å§‹çˆ¬å–ETF {etf_code} çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        
        # å°è¯•å¤šç§AkShareæ¥å£
        df = try_multiple_akshare_interfaces(etf_code, start_date, end_date)
        
        if df.empty:
            logger.warning(f"AkShareæœªè·å–åˆ°{etf_code}æ•°æ®ï¼ˆ{start_date}è‡³{end_date}ï¼‰")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.info(f"ğŸ“Š AkShareæ•°æ®æºè¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—å
        df = standardize_column_names(df)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        df = ensure_required_columns(df)
        
        # æ•°æ®æ¸…æ´—ï¼šå»é‡ã€æ ¼å¼è½¬æ¢
        df = clean_and_format_data(df)
        
        # é™åˆ¶æ•°æ®é‡ä¸º1å¹´ï¼ˆ365å¤©ï¼‰
        df = limit_to_one_year_data(df, end_date)
        
        logger.info(f"AkShareæˆåŠŸè·å–{etf_code}æ•°æ®ï¼Œå…±{len(df)}æ¡ï¼ˆå·²é™åˆ¶ä¸º1å¹´æ•°æ®ï¼‰")
        return df
    except Exception as e:
        logger.error(f"AkShareçˆ¬å–{etf_code}å¤±è´¥ï¼š{str(e)}", exc_info=True)
        # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
        time.sleep(2)
        raise  # è§¦å‘é‡è¯•

def try_multiple_akshare_interfaces(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•å¤šç§AkShareæ¥å£è·å–ETFæ•°æ®
    
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    interfaces = [
        lambda: try_fund_etf_hist_em(etf_code, start_date, end_date),
        lambda: try_fund_etf_hist_sina(etf_code)
    ]
    
    for i, interface in enumerate(interfaces):
        try:
            logger.debug(f"å°è¯•ç¬¬{i+1}ç§æ¥å£è·å–ETF {etf_code} æ•°æ®")
            df = interface()
            
            if not df.empty:
                logger.info(f"ç¬¬{i+1}ç§æ¥å£æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
                
                # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
                logger.info(f"ğŸ“Š ç¬¬{i+1}ç§æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
                
                # å¯¹è¿”å›çš„æ•°æ®è¿›è¡Œæ—¥æœŸè¿‡æ»¤
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'])
                    mask = (df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))
                    df = df.loc[mask]
                elif 'æ—¥æœŸ' in df.columns:
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                    mask = (df['æ—¥æœŸ'] >= pd.to_datetime(start_date)) & (df['æ—¥æœŸ'] <= pd.to_datetime(end_date))
                    df = df.loc[mask]
                
                if not df.empty:
                    logger.info(f"ç¬¬{i+1}ç§æ¥å£æˆåŠŸè·å–ETF {etf_code} æ•°æ®ï¼ˆè¿‡æ»¤åï¼‰")
                    return df
        except Exception as e:
            logger.warning(f"ç¬¬{i+1}ç§æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            continue
    
    logger.warning(f"æ‰€æœ‰AkShareæ¥å£å‡æ— æ³•è·å–ETF {etf_code} æ•°æ®")
    return pd.DataFrame()

def try_fund_etf_hist_em(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£
    
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def try_fund_etf_hist_sina(etf_code: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£è·å–ETF {etf_code} æ•°æ®")
        symbol = get_symbol_with_market_prefix(etf_code)
        df = ak.fund_etf_hist_sina(symbol=symbol)
        
        # æ–°æµªæ¥å£è¿”å›çš„æ•°æ®å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_sina æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ–°æµªæ¥å£è¿”å›çš„åˆ—åå¯èƒ½æ˜¯è‹±æ–‡ï¼Œéœ€è¦è½¬æ¢ä¸ºä¸­æ–‡
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢',
                'amplitude': 'æŒ¯å¹…',
                'percent': 'æ¶¨è·Œå¹…',
                'change': 'æ¶¨è·Œé¢',
                'turnover_rate': 'æ¢æ‰‹ç‡',
                'trade_date': 'æ—¥æœŸ',
                'open_price': 'å¼€ç›˜',
                'high_price': 'æœ€é«˜',
                'low_price': 'æœ€ä½',
                'close_price': 'æ”¶ç›˜',
                'vol': 'æˆäº¤é‡',
                'amount_volume': 'æˆäº¤é¢',
                'amplitude_percent': 'æŒ¯å¹…',
                'pct_chg': 'æ¶¨è·Œå¹…',
                'price_change': 'æ¶¨è·Œé¢',
                'turnover_ratio': 'æ¢æ‰‹ç‡'
            }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
            if 'æ—¥æœŸ' not in df.columns and 'date' in df.columns:
                df = df.rename(columns={'date': 'æ—¥æœŸ'})
                
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_sina æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_symbol_with_market_prefix(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        str: å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    """
    if etf_code.startswith(('5', '6', '9')):
        return f"sh{etf_code}"
    else:
        return f"sz{etf_code}"

def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–åˆ—åï¼Œå°†ä¸åŒæ•°æ®æºçš„åˆ—åè½¬æ¢ä¸ºç»Ÿä¸€çš„ä¸­æ–‡åˆ—å
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pd.DataFrame: æ ‡å‡†åŒ–åˆ—ååçš„DataFrame
    """
    if df.empty:
        return df
    
    # å®šä¹‰å¯èƒ½çš„åˆ—åå˜ä½“
    column_variants = {
        'æ—¥æœŸ': ['date', 'æ—¥æœŸ', 'trade_date', 'dt', 'datetime', 'äº¤æ˜“æ—¥æœŸ', 'time'],
        'å¼€ç›˜': ['open', 'å¼€ç›˜ä»·', 'å¼€', 'open_price', 'openprice', 'openprice_'],
        'æœ€é«˜': ['high', 'æœ€é«˜ä»·', 'é«˜', 'high_price', 'highprice', 'highprice_'],
        'æœ€ä½': ['low', 'æœ€ä½ä»·', 'ä½', 'low_price', 'lowprice', 'lowprice_'],
        'æ”¶ç›˜': ['close', 'æ”¶ç›˜ä»·', 'æ”¶', 'close_price', 'closeprice', 'closeprice_', 'price'],
        'æˆäº¤é‡': ['volume', 'æˆäº¤é‡', 'vol', 'æˆäº¤æ•°é‡', 'amount_volume', 'vol_', 'volume_'],
        'æˆäº¤é¢': ['amount', 'æˆäº¤é¢', 'æˆäº¤é‡‘é¢', 'turnover', 'æˆäº¤æ€»ä»·', 'amount_', 'turnover_'],
        'æŒ¯å¹…': ['amplitude', 'æŒ¯å¹…%', 'æŒ¯å¹…ç™¾åˆ†æ¯”', 'amplitude_percent', 'amplitude_', 'amp_'],
        'æ¶¨è·Œå¹…': ['percent', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œ%', 'change_percent', 'pct_chg', 'changepercent', 'chg_pct', 'pctchange'],
        'æ¶¨è·Œé¢': ['change', 'æ¶¨è·Œé¢', 'ä»·æ ¼å˜åŠ¨', 'price_change', 'change_', 'chg_', 'pricechg'],
        'æ¢æ‰‹ç‡': ['turnover_rate', 'æ¢æ‰‹ç‡', 'turnover_ratio', 'turnover', 'turnoverrate', 'turnover_rate_']
    }
    
    # åˆ›å»ºæ–°çš„åˆ—åæ˜ å°„
    new_columns = {}
    for standard_name, variants in column_variants.items():
        for variant in variants:
            if variant in df.columns and variant not in new_columns:
                new_columns[variant] = standard_name
    
    # é‡å‘½ååˆ—
    df = df.rename(columns=new_columns)
    
    logger.info(f"âœ… æ ‡å‡†åŒ–åçš„åˆ—å: {list(df.columns)}")
    return df

def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰å¿…éœ€çš„äº¤æ˜“æ•°æ®åˆ—ï¼Œç¼ºå¤±çš„åˆ—ç”¨é»˜è®¤å€¼å¡«å……
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    if df.empty:
        return df
    
    # å®šä¹‰æ•°æ®æºå¿…éœ€åˆ—ï¼ˆåŸºç¡€äº¤æ˜“æ•°æ®ï¼‰
    data_source_required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]
    
    # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
    missing_columns = [col for col in data_source_required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"âŒ æ•°æ®æºç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(missing_columns)}ï¼Œæ— æ³•ç»§ç»­")
        return pd.DataFrame()  # å¿…éœ€åˆ—ç¼ºå¤±ï¼Œè¿”å›ç©ºDataFrame
    
    # å®šä¹‰å¯è®¡ç®—çš„è¡ç”Ÿåˆ—
    derived_columns = ["æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
    
    # æ£€æŸ¥è¡ç”Ÿåˆ—æ˜¯å¦å­˜åœ¨
    missing_derived_columns = [col for col in derived_columns if col not in df.columns]
    
    if missing_derived_columns:
        logger.warning(f"âš ï¸ æ•°æ®æºç¼ºå°‘å¯è®¡ç®—åˆ—ï¼š{', '.join(missing_derived_columns)}ï¼Œå°†å°è¯•è®¡ç®—")
        
        # ä¸ºç¼ºå¤±çš„è¡ç”Ÿåˆ—è®¡ç®—å€¼
        for col in missing_derived_columns:
            try:
                if col == 'æˆäº¤é¢':
                    # å¦‚æœæœ‰æˆäº¤é‡ï¼Œå°è¯•ä¼°ç®—æˆäº¤é¢ï¼ˆç®€å•ä¼°ç®—ï¼šæˆäº¤é‡ * æ”¶ç›˜ä»· * 100ï¼‰
                    if 'æˆäº¤é‡' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # è®¡ç®—å‡ºçš„æˆäº¤é¢å•ä½æ˜¯å…ƒï¼Œè½¬æ¢ä¸ºä¸‡å…ƒ
                        df['æˆäº¤é¢'] = (df['æˆäº¤é‡'] * df['æ”¶ç›˜'] * 100 / 10000).round(2)
                    else:
                        df['æˆäº¤é¢'] = 0.0
                elif col == 'æŒ¯å¹…':
                    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / å‰æ”¶ç›˜ * 100%
                    if 'æœ€é«˜' in df.columns and 'æœ€ä½' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # å‡è®¾å‰æ”¶ç›˜ä»·æ˜¯å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['å‰æ”¶ç›˜'] * 100).round(2).fillna(0)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                    else:
                        df['æŒ¯å¹…'] = 0.0
                elif col == 'æ¶¨è·Œå¹…':
                    # æ¶¨è·Œå¹… = (æ”¶ç›˜ - å‰æ”¶ç›˜) / å‰æ”¶ç›˜ * 100%
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        df['æ¶¨è·Œå¹…'] = ((df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']) / df['å‰æ”¶ç›˜'] * 100).round(2).fillna(0)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                    else:
                        df['æ¶¨è·Œå¹…'] = 0.0
                elif col == 'æ¶¨è·Œé¢':
                    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰æ”¶ç›˜
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        df['æ¶¨è·Œé¢'] = (df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']).round(4).fillna(0)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                    else:
                        df['æ¶¨è·Œé¢'] = 0.0
                elif col == 'æ¢æ‰‹ç‡':
                    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / æµé€šè‚¡æœ¬ * 100%
                    # ç”±äºä¸çŸ¥é“æµé€šè‚¡æœ¬ï¼Œæš‚æ—¶ç”¨0å¡«å……
                    df['æ¢æ‰‹ç‡'] = 0.0
            except Exception as e:
                logger.error(f"è®¡ç®—åˆ— {col} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
                df[col] = 0.0
    
    return df

def clean_and_format_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pd.DataFrame: æ¸…æ´—åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # åˆ›å»ºDataFrameçš„å‰¯æœ¬ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # æ—¥æœŸæ ¼å¼è½¬æ¢
        if "æ—¥æœŸ" in df.columns:
            # ä¸¥æ ¼ä½¿ç”¨åŒ—äº¬æ—¶é—´
            df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce').dt.tz_localize(Config.UTC_TIMEZONE, errors='ignore')
            df.loc[:, "æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.tz_convert(Config.BEIJING_TIMEZONE)
            df.loc[:, "æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.date
            
            # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆYYYY-MM-DDï¼‰
            df.loc[:, "æ—¥æœŸ"] = df["æ—¥æœŸ"].apply(lambda x: x.strftime("%Y-%m-%d") if not pd.isna(x) else "")
        
        # æ•°å€¼åˆ—è½¬æ¢
        numeric_cols = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
        for col in numeric_cols:
            if col in df.columns:
                try:
                    # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…SettingWithCopyWarning
                    df = df.copy(deep=True)
                    
                    # å¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸²å€¼ï¼ˆå¦‚"-"ï¼‰
                    if df[col].dtype == 'object':
                        df.loc[:, col] = df[col].replace('-', '0')
                    
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    df.loc[:, col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
                    
                    # ç‰¹æ®Šå¤„ç†ï¼šæ¶¨è·Œå¹…å’ŒæŒ¯å¹…ä¿ç•™2ä½å°æ•°
                    if col in ["æ¶¨è·Œå¹…", "æŒ¯å¹…"]:
                        df.loc[:, col] = df[col].round(2)
                    # å…¶ä»–æ•°å€¼åˆ—æ ¹æ®éœ€è¦ä¿ç•™å°æ•°ä½
                    elif col in ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜"]:
                        df.loc[:, col] = df[col].round(4)
                    elif col in ["æˆäº¤é¢"]:
                        # ä¿®å¤ï¼šå°†æˆäº¤é¢ä»å…ƒè½¬æ¢ä¸ºä¸‡å…ƒ
                        df.loc[:, col] = df[col] / 10000
                        df.loc[:, col] = df[col].round(2)  # ä¿ç•™2ä½å°æ•°
                except Exception as e:
                    logger.error(f"è½¬æ¢åˆ— {col} ä¸ºæ•°å€¼ç±»å‹æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                    df.loc[:, col] = 0.0
        
        # å¤„ç†é‡å¤æ•°æ®
        if "æ—¥æœŸ" in df.columns:
            df = df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
            df = df.sort_values("æ—¥æœŸ", ascending=False)
        
        logger.debug("æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–å®Œæˆ")
        return df
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return df

def limit_to_one_year_data(df: pd.DataFrame, end_date: str) -> pd.DataFrame:
    """
    é™åˆ¶æ•°æ®ä¸ºæœ€è¿‘1å¹´çš„æ•°æ®
    
    Args:
        df: åŸå§‹DataFrame
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: é™åˆ¶ä¸º1å¹´æ•°æ®åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸ
        one_year_ago = (pd.to_datetime(end_date) - timedelta(days=365)).strftime("%Y-%m-%d")
        
        # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            logger.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œæ— æ³•é™åˆ¶ä¸º1å¹´æ•°æ®")
            return df
        
        # åˆ›å»ºDataFrameçš„å‰¯æœ¬ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # è½¬æ¢æ—¥æœŸåˆ—
        df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è¿‡æ»¤æ•°æ®
        mask = df["æ—¥æœŸ"] >= pd.to_datetime(one_year_ago)
        df = df.loc[mask]
        
        logger.info(f"æ•°æ®å·²é™åˆ¶ä¸ºæœ€è¿‘1å¹´ï¼ˆä» {one_year_ago} è‡³ {end_date}ï¼‰ï¼Œå‰©ä½™ {len(df)} æ¡æ•°æ®")
        return df
    except Exception as e:
        logger.error(f"é™åˆ¶æ•°æ®ä¸º1å¹´æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return df
