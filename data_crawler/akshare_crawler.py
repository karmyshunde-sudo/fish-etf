# akshare_crawler.py
import akshare as ak
import pandas as pd
import logging
import time
import re
from typing import Optional, Dict, Any, Tuple
from datetime import datetime
from config import Config
from retrying import retry

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRY_ATTEMPTS = 3
RETRY_WAIT_FIXED = 2000  # æ¯«ç§’
RETRY_WAIT_EXPONENTIAL_MAX = 10000  # æ¯«ç§’

print(f"AkShareç‰ˆæœ¬: {ak.__version__}")

# æŸ¥çœ‹å¯ç”¨æ¥å£
print([func for func in dir(ak) if 'etf' in func or 'fund' in func])

def empty_result_check(result: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥AkShareè¿”å›ç»“æœæ˜¯å¦ä¸ºç©º
    :param result: AkShareè¿”å›çš„DataFrame
    :return: å¦‚æœç»“æœä¸ºç©ºè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return result is None or result.empty

def retry_if_akshare_error(exception: Exception) -> bool:
    """
    é‡è¯•æ¡ä»¶ï¼šAkShareç›¸å…³é”™è¯¯
    :param exception: å¼‚å¸¸å¯¹è±¡
    :return: å¦‚æœæ˜¯AkShareé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return isinstance(exception, (ValueError, ConnectionError, TimeoutError))

@retry(
    stop_max_attempt_number=MAX_RETRY_ATTEMPTS,
    wait_fixed=RETRY_WAIT_FIXED,
    retry_on_result=empty_result_check,
    retry_on_exception=retry_if_akshare_error
)
def crawl_etf_daily_akshare(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    :param etf_code: ETFä»£ç  (6ä½æ•°å­—)
    :param start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
    :param end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    :return: æ ‡å‡†åŒ–ä¸­æ–‡åˆ—åçš„DataFrame
    """
    try:
        logger.info(f"å¼€å§‹çˆ¬å–ETF {etf_code} çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        
        # å°è¯•å¤šç§AkShareæ¥å£
        df = try_multiple_akshare_interfaces(etf_code, start_date, end_date)
        
        if df.empty:
            logger.warning(f"AkShareæœªè·å–åˆ°{etf_code}æ•°æ®ï¼ˆ{start_date}è‡³{end_date}ï¼‰")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.debug(f"AkShareè¿”å›åˆ—å: {list(df.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—å
        df = standardize_column_names(df)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        df = ensure_required_columns(df)
        
        # æ•°æ®æ¸…æ´—ï¼šå»é‡ã€æ ¼å¼è½¬æ¢
        df = clean_and_format_data(df)
        
        logger.info(f"AkShareæˆåŠŸè·å–{etf_code}æ•°æ®ï¼Œå…±{len(df)}æ¡")
        return df
    
    except Exception as e:
        logger.error(f"AkShareçˆ¬å–{etf_code}å¤±è´¥ï¼š{str(e)}")
        # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
        time.sleep(2)
        raise  # è§¦å‘é‡è¯•

def try_multiple_akshare_interfaces(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•å¤šç§AkShareæ¥å£è·å–ETFæ•°æ®
    :param etf_code: ETFä»£ç 
    :param start_date: å¼€å§‹æ—¥æœŸ
    :param end_date: ç»“æŸæ—¥æœŸ
    :return: è·å–åˆ°çš„DataFrame
    """
    interfaces = [
        lambda: try_fund_etf_hist_em(etf_code, start_date, end_date),
        lambda: try_fund_etf_hist_sina(etf_code)  # ç§»é™¤äº†start_dateå’Œend_dateå‚æ•°
    ]
    
    for i, interface in enumerate(interfaces):
        try:
            logger.debug(f"å°è¯•ç¬¬{i+1}ç§æ¥å£è·å–ETF {etf_code} æ•°æ®")
            df = interface()
            if not df.empty:
                # å¯¹è¿”å›çš„æ•°æ®è¿›è¡Œæ—¥æœŸè¿‡æ»¤
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'])
                    mask = (df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))
                    df = df.loc[mask]
                
                if not df.empty:
                    logger.info(f"ç¬¬{i+1}ç§æ¥å£æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
                    return df
        except Exception as e:
            logger.warning(f"ç¬¬{i+1}ç§æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}")
            continue
    
    logger.warning(f"æ‰€æœ‰AkShareæ¥å£å‡æ— æ³•è·å–ETF {etf_code} æ•°æ®")
    return pd.DataFrame()

def try_fund_etf_hist_em(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£
    :param etf_code: ETFä»£ç 
    :param start_date: å¼€å§‹æ—¥æœŸ
    :param end_date: ç»“æŸæ—¥æœŸ
    :return: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.fund_etf_hist_em(
            symbol=etf_code,
            period="daily",
            start_date=start_date,
            end_date=end_date,
            adjust="qfq"
        )
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}")
        return pd.DataFrame()

def try_fund_etf_hist_sina(etf_code: str) -> pd.DataFrame:
    """å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£
    :param etf_code: ETFä»£ç 
    :return: è·å–åˆ°çš„DataFrame
    """
    try:
        # æ·»åŠ å¸‚åœºå‰ç¼€ï¼ˆä¸Šæµ·æˆ–æ·±åœ³ï¼‰
        symbol = get_symbol_with_market_prefix(etf_code)
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£è·å–ETF {symbol} æ•°æ®")
        # è°ƒç”¨æ–°æµªæ¥å£

        df = ak.fund_etf_hist_sina(symbol=symbol)
        # æ–°æµªæ¥å£è¿”å›çš„æ•°æ®å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
        if not df.empty:
            # æ–°æµªæ¥å£è¿”å›çš„åˆ—åå¯èƒ½æ˜¯è‹±æ–‡ï¼Œéœ€è¦è½¬æ¢ä¸ºä¸­æ–‡
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢'
            }
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
            if 'æ—¥æœŸ' not in df.columns and 'date' in df.columns:
                df = df.rename(columns={'date': 'æ—¥æœŸ'})
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_sina æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}")
        return pd.DataFrame()

def get_symbol_with_market_prefix(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    :param etf_code: ETFä»£ç 
    :return: å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    """
    if etf_code.startswith('5') or etf_code.startswith('6') or etf_code.startswith('9'):
        return f"sh{etf_code}"
    else:
        return f"sz{etf_code}"

def standardize_column_names(df: pd.DataFrame, source: str = "akshare") -> pd.DataFrame:
    """æ ‡å‡†åŒ–åˆ—åï¼ˆä¸­æ–‡æ˜ å°„ï¼‰
    :param df: åŸå§‹DataFrame
    :param source: æ•°æ®æºåç§°ï¼ˆ"akshare"ã€"sina"ç­‰ï¼‰
    :return: æ ‡å‡†åŒ–åˆ—åçš„DataFrame
    """
    if df.empty:
        return df
    
    # ã€æ–°å¢ã€‘å…³é”®æ—¥å¿—ï¼šè¾“å‡ºåŸå§‹åˆ—åï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
    logger.info(f"ğŸ“Š {source}æ•°æ®æºè¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
    
    # é’ˆå¯¹ä¸åŒæ•°æ®æºçš„ç‰¹æ®Šå¤„ç†
    if source == "sina":
        # ã€æ–°å¢ã€‘æ–°æµªæ¥å£çš„ç‰¹å®šåˆ—åæ˜ å°„è§„åˆ™
        sina_col_map = {
            "date": "æ—¥æœŸ",
            "open": "å¼€ç›˜",
            "close": "æ”¶ç›˜",
            "high": "æœ€é«˜",
            "low": "æœ€ä½",
            "volume": "æˆäº¤é‡",
            "amount": "æˆäº¤é¢",
            "pre_close": "å‰æ”¶ç›˜"
        }
        for src, tgt in sina_col_map.items():
            if src in df.columns:
                df = df.rename(columns={src: tgt})
                logger.debug(f"ğŸ”„ æ–°æµªåˆ—åæ˜ å°„: {src} -> {tgt}")
    
    elif source == "akshare":
        # ã€æ–°å¢ã€‘AkShareæ¥å£çš„ç‰¹å®šåˆ—åæ˜ å°„è§„åˆ™
        akshare_col_map = {
            "æ—¥æœŸ": "æ—¥æœŸ",
            "date": "æ—¥æœŸ",
            "datetime": "æ—¥æœŸ",
            "open": "å¼€ç›˜",
            "op": "å¼€ç›˜",
            "close": "æ”¶ç›˜",
            "cl": "æ”¶ç›˜",
            "high": "æœ€é«˜",
            "hi": "æœ€é«˜",
            "low": "æœ€ä½",
            "lo": "æœ€ä½",
            "volume": "æˆäº¤é‡",
            "vol": "æˆäº¤é‡",
            "amount": "æˆäº¤é¢",
            "amt": "æˆäº¤é¢",
            "change": "æ¶¨è·Œé¢",
            "pct_chg": "æ¶¨è·Œå¹…",
            "pre_close": "å‰æ”¶ç›˜"
        }
        for src, tgt in akshare_col_map.items():
            if src in df.columns:
                df = df.rename(columns={src: tgt})
                logger.debug(f"ğŸ”„ AkShareåˆ—åæ˜ å°„: {src} -> {tgt}")
    
    # ã€æ”¹è¿›ã€‘æ›´ç²¾ç¡®çš„æ¨¡ç³ŠåŒ¹é…é€»è¾‘
    col_map = {}
    for target_col in Config.STANDARD_COLUMNS.keys():
        # æ’é™¤ä¸éœ€è¦å¤„ç†çš„åˆ—
        if target_col in ["ETFä»£ç ", "ETFåç§°", "çˆ¬å–æ—¶é—´"]:
            continue
            
        # ç²¾ç¡®åŒ¹é…
        if target_col in df.columns:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼åˆ—å
        similar_cols = []
        for actual_col in df.columns:
            # æ›´ç²¾ç¡®çš„åŒ¹é…é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ ‡è¯†
            if (target_col == "æ—¥æœŸ" and ("date" in actual_col.lower() or "time" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "å¼€ç›˜" and ("open" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "æ”¶ç›˜" and ("close" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "æœ€é«˜" and ("high" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "æœ€ä½" and ("low" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "æˆäº¤é‡" and ("vol" in actual_col.lower())):
                similar_cols.append(actual_col)
            elif (target_col == "æˆäº¤é¢" and ("amount" in actual_col.lower() or "amt" in actual_col.lower())):
                similar_cols.append(actual_col)
        
        if similar_cols:
            # é€‰æ‹©æœ€å¯èƒ½çš„åˆ—ï¼ˆé€šå¸¸æ˜¯æœ€çŸ­çš„åˆ—åï¼‰
            best_match = min(similar_cols, key=len)
            col_map[best_match] = target_col
            logger.info(f"ğŸ” è‡ªåŠ¨åŒ¹é…åˆ—å: {best_match} -> {target_col}")
    
    # é‡å‘½ååˆ—
    if col_map:
        df = df.rename(columns=col_map)
    
    # ã€æ–°å¢ã€‘å…³é”®æ—¥å¿—ï¼šæ˜¾ç¤ºæ˜ å°„åçš„åˆ—å
    logger.info(f"âœ… æ ‡å‡†åŒ–åçš„åˆ—å: {list(df.columns)}")
    
    # æ£€æŸ¥å“ªäº›å¿…éœ€åˆ—ä»ç„¶ç¼ºå¤±
    missing_cols = []
    for col in Config.STANDARD_COLUMNS.keys():
        if col not in df.columns and col not in ["ETFä»£ç ", "ETFåç§°", "çˆ¬å–æ—¶é—´"]:
            missing_cols.append(col)
    
    if missing_cols:
        logger.warning(f"âš ï¸ æ•°æ®æºä»ç¼ºå°‘å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
    
    return df
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨ï¼Œç¼ºå¤±çš„åˆ—è¿›è¡Œè®¡ç®—æˆ–å¡«å……
    :param df: åŸå§‹DataFrame
    :return: åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    if df.empty:
        return df
    
    # ç¡®å®šå…³é”®åˆ—ï¼ˆç¼ºå°‘è¿™äº›åˆ—çš„æ•°æ®ä¸å¯ç”¨ï¼‰
    critical_cols = ["æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡"]
    
    # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
    missing_critical = [col for col in critical_cols if col not in df.columns]
    if missing_critical:
        logger.error(f"âŒ å…³é”®åˆ—ç¼ºå¤±: {', '.join(missing_critical)} - æ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
        return pd.DataFrame()  # è¿”å›ç©ºDataFrameï¼Œé¿å…åç»­å¤„ç†
    
    # å¸¸è§„åˆ—å¤„ç†
    for col in Config.STANDARD_COLUMNS.keys():
        if col in ["ETFä»£ç ", "ETFåç§°", "çˆ¬å–æ—¶é—´"]:
            continue
            
        if col not in df.columns:
            try:
                if col == "æ¶¨è·Œé¢" and "æ”¶ç›˜" in df.columns and "å‰æ”¶ç›˜" in df.columns:
                    df.loc[:, col] = (df["æ”¶ç›˜"] - df["å‰æ”¶ç›˜"]).round(4)
                elif col == "æ¶¨è·Œå¹…" and "æ”¶ç›˜" in df.columns and "å‰æ”¶ç›˜" in df.columns:
                    df.loc[:, col] = ((df["æ”¶ç›˜"] - df["å‰æ”¶ç›˜"]) / df["å‰æ”¶ç›˜"] * 100).round(2)
                elif col == "æŒ¯å¹…" and "æœ€é«˜" in df.columns and "æœ€ä½" in df.columns and "å‰æ”¶ç›˜" in df.columns:
                    df.loc[:, col] = ((df["æœ€é«˜"] - df["æœ€ä½"]) / df["å‰æ”¶ç›˜"] * 100).round(2)
                else:
                    # éå…³é”®åˆ—å¯ä»¥å®‰å…¨å¡«å……
                    df.loc[:, col] = 0.0
                    logger.debug(f"â„¹ï¸ å¡«å……éå…³é”®åˆ— {col} ä¸ºé»˜è®¤å€¼ 0.0")
            except Exception as e:
                logger.error(f"âŒ è®¡ç®—åˆ— {col} æ—¶å‡ºé”™: {str(e)}")
                df.loc[:, col] = 0.0
    
    return df

def clean_and_format_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®æ¸…æ´—ï¼šå»é‡ã€æ ¼å¼è½¬æ¢"""
    if df.empty:
        return df
    
    # å»é‡
    df = df.drop_duplicates()
    
    # æ ¼å¼è½¬æ¢ - ä½¿ç”¨locé¿å…SettingWithCopyWarning
    numeric_cols = ["å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢"]
    for col in numeric_cols:
        if col in df.columns:
            # ä½¿ç”¨locç¡®ä¿ä¿®æ”¹åŸå§‹DataFrame
            df.loc[:, col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
    
    # æ—¥æœŸæ ¼å¼åŒ–
    if "æ—¥æœŸ" in df.columns:
        df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
    
    return df

def validate_date_range(start_date: str, end_date: str) -> bool:
    """
    éªŒè¯æ—¥æœŸèŒƒå›´æ˜¯å¦æœ‰æ•ˆ
    :param start_date: å¼€å§‹æ—¥æœŸ
    :param end_date: ç»“æŸæ—¥æœŸ
    :return: å¦‚æœæ—¥æœŸèŒƒå›´æœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        if start > end:
            logger.error(f"å¼€å§‹æ—¥æœŸ {start_date} ä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ {end_date}")
            return False
            
        if start > datetime.now():
            logger.error(f"å¼€å§‹æ—¥æœŸ {start_date} ä¸èƒ½æ™šäºå½“å‰æ—¥æœŸ")
            return False
            
        return True
    except ValueError:
        logger.error(f"æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œåº”ä¸º YYYY-MM-DD: {start_date} æˆ– {end_date}")
        return False

def validate_etf_code(etf_code: str) -> bool:
    """
    éªŒè¯ETFä»£ç æ˜¯å¦æœ‰æ•ˆ
    :param etf_code: ETFä»£ç 
    :return: å¦‚æœETFä»£ç æœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not etf_code or not isinstance(etf_code, str):
        logger.error("ETFä»£ç ä¸èƒ½ä¸ºç©º")
        return False
        
    # ç§»é™¤å¯èƒ½çš„å‰ç¼€
    clean_code = re.sub(r"^(sh|sz)?", "", etf_code)
    
    # æ£€æŸ¥æ˜¯å¦ä¸º6ä½æ•°å­—
    if not re.match(r"^\d{6}$", clean_code):
        logger.error(f"ETFä»£ç æ ¼å¼æ— æ•ˆ: {etf_code}")
        return False
        
    return True

# æ¨¡å—åˆå§‹åŒ–
try:
    logger.info("AkShareçˆ¬è™«æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    print(f"AkShareçˆ¬è™«æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
