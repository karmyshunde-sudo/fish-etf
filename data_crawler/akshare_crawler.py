#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨AkShareæ¥å£è·å–ETFæ—¥çº¿æ•°æ®
ç‰¹åˆ«ä¼˜åŒ–äº†åˆ—åæ˜ å°„å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import akshare as ak
import pandas as pd
import logging
import time
from typing import Optional, Dict, Any, Tuple
from datetime import datetime, timedelta, date  # ä¿®å¤ï¼šæ·»åŠ dateå¯¼å…¥
from config import Config
from retrying import retry

# ä¿®å¤ï¼šæ­£ç¡®å¯¼å…¥å‡½æ•°
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    get_last_trading_day,
    is_trading_day
)
# ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
from utils.file_utils import (
    ensure_chinese_columns, internal_ensure_chinese_columns
)
from utils.data_processor import (
    ensure_required_columns,
    clean_and_format_data,
    limit_to_one_year_data
)

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRY_ATTEMPTS = 5  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼Œä»3å¢åŠ åˆ°5
RETRY_WAIT_FIXED = 3000  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œä»2000æ¯«ç§’å¢åŠ åˆ°3000æ¯«ç§’
RETRY_WAIT_EXPONENTIAL_MAX = 15000  # å¢åŠ æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œä»10000æ¯«ç§’å¢åŠ åˆ°15000æ¯«ç§’

# æ‰“å°AkShareç‰ˆæœ¬
logger.info(f"AkShareç‰ˆæœ¬: {ak.__version__}")

def empty_result_check(result: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥AkShareè¿”å›ç»“æœæ˜¯å¦ä¸ºç©º
    
    Args:
        result: AkShareè¿”å›çš„DataFrame
        
    Returns:
        bool: å¦‚æœç»“æœä¸ºç©ºè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return result is None or result.empty

def retry_if_akshare_error(exception: Exception) -> bool:
    """
    é‡è¯•æ¡ä»¶ï¼šAkShareç›¸å…³é”™è¯¯
    
    Args:
        exception: å¼‚å¸¸å¯¹è±¡
        
    Returns:
        bool: å¦‚æœæ˜¯AkShareé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # æ‰©å±•å¼‚å¸¸ç±»å‹ï¼ŒåŒ…æ‹¬requestsåº“çš„ç½‘ç»œé”™è¯¯
    from requests.exceptions import ConnectionError, Timeout
    return isinstance(exception, (ValueError, ConnectionError, Timeout, OSError))

@retry(
    stop_max_attempt_number=MAX_RETRY_ATTEMPTS,
    wait_fixed=RETRY_WAIT_FIXED,
    retry_on_result=empty_result_check,
    retry_on_exception=retry_if_akshare_error
)
def crawl_etf_daily_akshare(etf_code: str, start_date: str, end_date: str, is_first_crawl: bool = False) -> pd.DataFrame:
    """ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    Args:
        etf_code: ETFä»£ç  (6ä½æ•°å­—)
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        is_first_crawl: æ˜¯å¦æ˜¯é¦–æ¬¡çˆ¬å–
    
    Returns:
        pd.DataFrame: åŒ…å«ETFæ—¥çº¿æ•°æ®çš„DataFrame
    """
    try:
        # ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdateå¯¹è±¡
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day(end_date_obj)
        # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
        end_date = last_trading_day.strftime("%Y-%m-%d")
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†å•æ—¥è¯·æ±‚é—®é¢˜
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
        if start_date_obj == last_trading_day:
            # å¦‚æœæ˜¯å•æ—¥è¯·æ±‚ï¼Œæ‰©å±•ä¸ºè‡³å°‘3å¤©çš„èŒƒå›´
            start_date_obj = start_date_obj - timedelta(days=2)
            start_date = start_date_obj.strftime("%Y-%m-%d")
            logger.info(f"å•æ—¥è¯·æ±‚æ‰©å±•ä¸º {start_date} è‡³ {end_date}")
        
        logger.info(f"å¼€å§‹çˆ¬å–ETF {etf_code} çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        
        # ========== ä»¥ä¸‹æ˜¯å…³é”®ä¿®å¤ ==========
        # 1. å°è¯•å¤šç§AkShareæ¥å£ï¼ˆä¼˜å…ˆä½¿ç”¨æ—§ä»£ç çš„è¯¦ç»†é€»è¾‘ï¼‰
        df = try_multiple_akshare_interfaces(etf_code, start_date, end_date)
        
        # 2. å¦‚æœæ—§ä»£ç é€»è¾‘å¤±è´¥ï¼Œå°è¯•æ–°ä»£ç ä¸­çš„ç‰¹æ®Šå¤„ç†é€»è¾‘
        if df.empty:
            logger.info(f"æ—§ä»£ç é€»è¾‘è·å–ETF {etf_code} æ•°æ®å¤±è´¥ï¼Œå°è¯•æ–°ä»£ç é€»è¾‘")
            df = try_fund_etf_hist_em_with_net_value(etf_code, start_date, end_date)
            
            # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œå°è¯•æ’ç”ŸæŒ‡æ•°ç‰¹æ®Šå¤„ç†
            if df.empty and etf_code == "513750":
                logger.info(f"å°è¯•ç‰¹æ®Šå¤„ç†ETF {etf_code}")
                df = try_fund_etf_spot_em_with_premium(etf_code)
        
        # 3. å¦‚æœAkShareæ¥å£å…¨éƒ¨å¤±è´¥ï¼Œå°è¯•yfinanceä½œä¸ºå¤‡é€‰ï¼ˆä»…é™ç¾è‚¡æŒ‡æ•°ï¼‰
        if df.empty and etf_code.startswith('^'):
            logger.info(f"å°è¯•é€šè¿‡yfinanceè·å–ç¾è‚¡æŒ‡æ•° {etf_code} æ•°æ®")
            df = fetch_us_index_from_yfinance(etf_code, start_date, end_date)
        
        # 4. å¦‚æœæ˜¯Aè‚¡ETFï¼Œå°è¯•ä½¿ç”¨æŒ‡æ•°æ•°æ®ä½œä¸ºæœ€åå¤‡é€‰
        if df.empty and etf_code.startswith(("51", "159", "50", "510", "512", "513", "515", "518")):
            logger.info(f"å°è¯•é€šè¿‡æŒ‡æ•°æ•°æ®è·å–ETF {etf_code} æ•°æ®ä½œä¸ºæœ€åå¤‡é€‰")
            df = try_index_data_as_etf_backup(etf_code, start_date, end_date)
        
        if df.empty:
            logger.warning(f"æ‰€æœ‰æ•°æ®æºå‡æœªè·å–åˆ°{etf_code}æ•°æ®ï¼ˆ{start_date}è‡³{end_date}ï¼‰")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.info(f"ğŸ“Š æ•°æ®æºè¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—å - ä¼˜å…ˆä½¿ç”¨æ—§ä»£ç ä¸­çš„è¯¦ç»†åˆ—åæ˜ å°„
        df = ensure_chinese_columns(df)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨ - ä½¿ç”¨æ—§ä»£ç ä¸­çš„å®Œæ•´éªŒè¯é€»è¾‘
        df = ensure_required_columns(df)
        
        # æ•°æ®æ¸…æ´—ï¼šå»é‡ã€æ ¼å¼è½¬æ¢
        df = clean_and_format_data(df)
        
        # é¦–æ¬¡çˆ¬å–æ—¶é™åˆ¶æ•°æ®é‡ä¸º1å¹´ï¼ˆ365å¤©ï¼‰
        if is_first_crawl:
            df = limit_to_one_year_data(df, end_date)
        
        logger.info(f"æˆåŠŸè·å–ETF {etf_code} æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
        return df
    except Exception as e:
        logger.error(f"çˆ¬å–ETF {etf_code} å¤±è´¥: {str(e)}", exc_info=True)
        raise  # è§¦å‘é‡è¯•

def try_multiple_akshare_interfaces(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """å°è¯•å¤šç§AkShareæ¥å£è·å–ETFæ•°æ®"""
    logger.info(f"å°è¯•è·å–ETF {etf_code} æ•°æ®ï¼Œæœ€å¤š 3 ç§æ¥å£")
    
    # æ¥å£1: fund_etf_hist_em (æä¾›IOPVå’ŒæŠ˜æº¢ä»·ç‡)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_hist_emæ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", 
                                start_date=start_date, end_date=end_date, adjust="")
        
        if not df.empty:
            logger.info(f"ç¬¬1ç§æ¥å£ï¼ˆfund_etf_hist_emï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            if 'å‡€å€¼æ—¥æœŸ' in df.columns:
                df = df.rename(columns={
                    'å‡€å€¼æ—¥æœŸ': 'æ—¥æœŸ',
                    'å•ä½å‡€å€¼': 'IOPV',
                    'æŠ˜ä»·ç‡': 'æŠ˜æº¢ä»·ç‡'
                })
            elif 'å‡€å€¼ä¼°ç®—æ—¥æœŸ' in df.columns:
                df = df.rename(columns={
                    'å‡€å€¼ä¼°ç®—æ—¥æœŸ': 'æ—¥æœŸ',
                    'å•ä½å‡€å€¼ä¼°ç®—': 'IOPV',
                    'æŠ˜ä»·ç‡ä¼°ç®—': 'æŠ˜æº¢ä»·ç‡'
                })
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
            
            return df
    except Exception as e:
        logger.debug(f"fund_etf_hist_emæ¥å£å¤±è´¥: {str(e)}")
    
    # æ¥å£2: stock_zh_index_daily_js (æä¾›åŸºç¡€æ•°æ®)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨stock_zh_index_daily_jsæ¥å£è·å–ETF {etf_code} æ•°æ®")
        # å…³é”®ä¿®å¤ï¼šä¸º510300ç­‰ä¸Šäº¤æ‰€ETFæ·»åŠ "sh"å‰ç¼€
        symbol = f"sh{etf_code}" if etf_code.startswith('5') else f"sz{etf_code}"
        df = ak.stock_zh_index_daily_js(symbol=symbol)
        
        if not df.empty:
            logger.info(f"ç¬¬2ç§æ¥å£ï¼ˆstock_zh_index_daily_jsï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š stock_zh_index_daily_js æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡'
            })
            
            # å°è¯•è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if 'IOPV' in df.columns and 'æ”¶ç›˜' in df.columns:
                df['æŠ˜æº¢ä»·ç‡'] = (df['æ”¶ç›˜'] - df['IOPV']) / df['IOPV'] * 100
            
            return df
    except Exception as e:
        logger.debug(f"stock_zh_index_daily_jsæ¥å£å¤±è´¥: {str(e)}")
    
    # æ¥å£3: fund_etf_hist_sina (åŸºç¡€æ•°æ®)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_hist_sinaæ¥å£è·å–ETF {etf_code} æ•°æ®")
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¸‚åœºå‰ç¼€
        symbol = get_symbol_with_market_prefix(etf_code)
        df = ak.fund_etf_hist_sina(symbol=symbol)
        
        if not df.empty:
            logger.info(f"ç¬¬3ç§æ¥å£ï¼ˆfund_etf_hist_sinaï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š fund_etf_hist_sina æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢',
                'amplitude': 'æŒ¯å¹…',
                'percent': 'æ¶¨è·Œå¹…',
                'change': 'æ¶¨è·Œé¢',
                'turnover_rate': 'æ¢æ‰‹ç‡',
                'trade_date': 'æ—¥æœŸ',
                'open_price': 'å¼€ç›˜',
                'high_price': 'æœ€é«˜',
                'low_price': 'æœ€ä½',
                'close_price': 'æ”¶ç›˜',
                'vol': 'æˆäº¤é‡',
                'amount_volume': 'æˆäº¤é¢',
                'amplitude_percent': 'æŒ¯å¹…',
                'pct_chg': 'æ¶¨è·Œå¹…',
                'price_change': 'æ¶¨è·Œé¢',
                'turnover_ratio': 'æ¢æ‰‹ç‡',
                'net_value': 'å‡€å€¼',
                'iopv': 'IOPV'
            }
            # é‡å‘½ååˆ—
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
            if 'æ—¥æœŸ' not in df.columns and 'date' in df.columns:
                df = df.rename(columns={'date': 'æ—¥æœŸ'})
            
            # æ—¥æœŸè¿‡æ»¤ï¼ˆå…³é”®ä¿®å¤ï¼‰
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                mask = (df['æ—¥æœŸ'] >= pd.to_datetime(start_date)) & (df['æ—¥æœŸ'] <= pd.to_datetime(end_date))
                df = df.loc[mask]
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_sina æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

    logger.warning(f"æ‰€æœ‰AkShareæ¥å£å‡æœªè·å–åˆ°ETF {etf_code} æ•°æ®")
    return pd.DataFrame()

def get_symbol_with_market_prefix(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        str: å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    """
    if etf_code.startswith(('5', '6', '9')):
        return f"sh{etf_code}"
    else:
        return f"sz{etf_code}"

def fetch_us_index_from_yfinance(index_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    ä½¿ç”¨YFinanceè·å–ç¾è‚¡æŒ‡æ•°æ•°æ®ï¼ˆæœ€å¯é çš„æ›¿ä»£æ–¹æ¡ˆï¼‰
    
    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚"^NDX"ï¼‰
        start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰
        
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").strftime("%Y-%m-%d")
        
        # æŒ‡æ•°ä»£ç æ˜ å°„
        symbol_map = {
            '^NDX': '^NDX',  # çº³æ–¯è¾¾å…‹100
            '^DJI': '^DJI',  # é“ç¼æ–¯å·¥ä¸šæŒ‡æ•°
            '^GSPC': '^GSPC' # æ ‡å‡†æ™®å°”500
        }
        
        symbol = symbol_map.get(index_code, index_code)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…yfinance
        try:
            import yfinance as yf
        except ImportError:
            logger.error("éœ€è¦å®‰è£…yfinance: pip install yfinance")
            return pd.DataFrame()
        
        # è·å–æ•°æ®
        df = yf.download(symbol, start=start_dt, end=end_dt)
        
        if df.empty:
            logger.warning(f"é€šè¿‡yfinanceè·å–{index_code}æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # æ ‡å‡†åŒ–åˆ—å
        df = df.reset_index()
        df = df.rename(columns={
            'Date': 'æ—¥æœŸ',
            'Open': 'å¼€ç›˜',
            'High': 'æœ€é«˜',
            'Low': 'æœ€ä½',
            'Close': 'æ”¶ç›˜',
            'Volume': 'æˆäº¤é‡',
            'Adj Close': 'å¤æƒæ”¶ç›˜'
        })
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
        
        logger.info(f"æˆåŠŸé€šè¿‡yfinanceè·å–{index_code}æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
        return df
    
    except Exception as e:
        logger.error(f"é€šè¿‡yfinanceè·å–{index_code}å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def try_fund_etf_hist_em_with_net_value(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–åŒ…å«å‡€å€¼çš„æ•°æ®
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–ETF {etf_code} æ•°æ®ï¼ˆåŒ…å«å‡€å€¼ï¼‰")
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‡€å€¼æ•°æ®ï¼ˆfund_etf_hist_em å¯èƒ½è¿”å›çš„å‡€å€¼åˆ—ï¼‰
            net_value_columns = [col for col in df.columns if "å‡€å€¼" in col or "net" in col.lower()]
            if net_value_columns:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªå‡€å€¼åˆ—
                net_value_col = net_value_columns[0]
                df["å‡€å€¼"] = df[net_value_col]
                logger.info(f"âœ… fund_etf_hist_em æ¥å£æˆåŠŸè·å–å‡€å€¼æ•°æ®ï¼ˆåˆ—å: {net_value_col}ï¼‰")
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def try_fund_etf_spot_em_with_premium(etf_code: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_spot_em æ¥å£è·å–åŒ…å«æŠ˜ä»·ç‡çš„æ•°æ®ï¼ˆä»…æœ€æ–°æ•°æ®ï¼‰
    Args:
        etf_code: ETFä»£ç 
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_spot_em æ¥å£è·å–ETF {etf_code} æ•°æ®ï¼ˆåŒ…å«æŠ˜ä»·ç‡ï¼‰")
        df = ak.fund_etf_spot_em()
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_spot_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # è¿‡æ»¤æŒ‡å®šETF
            df = df[df["ä»£ç "] == etf_code]
            
            if not df.empty:
                # æ ‡å‡†åŒ–åˆ—å
                column_mapping = {
                    "ä»£ç ": "ETFä»£ç ",
                    "åç§°": "ETFåç§°",
                    "æœ€æ–°ä»·": "æ”¶ç›˜",
                    "IOPVå®æ—¶ä¼°å€¼": "IOPV",
                    "åŸºé‡‘æŠ˜ä»·ç‡": "æŠ˜æº¢ä»·ç‡",
                    "æ¶¨è·Œé¢": "æ¶¨è·Œé¢",
                    "æ¶¨è·Œå¹…": "æ¶¨è·Œå¹…",
                    "æˆäº¤é‡": "æˆäº¤é‡",
                    "æˆäº¤é¢": "æˆäº¤é¢",
                    "å¼€ç›˜ä»·": "å¼€ç›˜",
                    "æœ€é«˜ä»·": "æœ€é«˜",
                    "æœ€ä½ä»·": "æœ€ä½",
                    "æ˜¨æ”¶": "å‰æ”¶ç›˜",
                    "æŒ¯å¹…": "æŒ¯å¹…",
                    "æ¢æ‰‹ç‡": "æ¢æ‰‹ç‡",
                    "æ•°æ®æ—¥æœŸ": "æ—¥æœŸ"
                }
                df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
                
                # ç¡®ä¿æ—¥æœŸæ ¼å¼
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
                
                # è®¾ç½®æˆäº¤é‡å•ä½ä¸º"æ‰‹"ï¼ˆ1æ‰‹=100è‚¡ï¼‰
                if "æˆäº¤é‡" in df.columns:
                    df["æˆäº¤é‡"] = df["æˆäº¤é‡"] / 100
                
                logger.info("âœ… fund_etf_spot_em æ¥å£æˆåŠŸè·å–æŠ˜ä»·ç‡æ•°æ®")
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_spot_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def try_index_data_as_etf_backup(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨æŒ‡æ•°æ•°æ®ä½œä¸ºETFæ•°æ®çš„æœ€åå¤‡é€‰æ–¹æ¡ˆ
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        # å®½åŸºETFä¸æŒ‡æ•°çš„æ˜ å°„å…³ç³»
        index_mapping = {
            "510300": "000300",  # æ²ªæ·±300ETF -> æ²ªæ·±300æŒ‡æ•°
            "510500": "000905",  # ä¸­è¯500ETF -> ä¸­è¯500æŒ‡æ•°
            "510050": "000016",  # ä¸Šè¯50ETF -> ä¸Šè¯50æŒ‡æ•°
            "588000": "000688",  # ç§‘åˆ›50ETF -> ç§‘åˆ›50æŒ‡æ•°
            "159915": "399006",  # åˆ›ä¸šæ¿ETF -> åˆ›ä¸šæ¿æŒ‡æ•°
            "512880": "399975",  # è¯åˆ¸ETF -> è¯åˆ¸å…¬å¸æŒ‡æ•°
            "512660": "399967",  # å†›å·¥ETF -> å†›å·¥æŒ‡æ•°
            "512400": "399395",  # æœ‰è‰²é‡‘å±ETF -> æœ‰è‰²é‡‘å±æŒ‡æ•°
            "515070": "930713",  # AIäº§ä¸šETF -> AIäº§ä¸šæŒ‡æ•°
            "512800": "399965",  # é“¶è¡ŒETF -> é“¶è¡ŒæŒ‡æ•°
            "512890": "399986",  # ç¯ä¿ETF -> ç¯ä¿äº§ä¸šæŒ‡æ•°
            "515220": "930606",  # çº¢åˆ©ä½æ³¢ETF -> çº¢åˆ©ä½æ³¢æŒ‡æ•°
            "515790": "930972",  # å…‰ä¼ETF -> å…‰ä¼äº§ä¸šæŒ‡æ•°
            "159855": "931151",  # æ–°èƒ½æºè½¦ETF -> æ–°èƒ½æºè½¦æŒ‡æ•°
            "159995": "399812",  # é€šä¿¡ETF -> é€šä¿¡è®¾å¤‡æŒ‡æ•°
            "159928": "399007",  # æ¶ˆè´¹ETF -> ä¸»è¦æ¶ˆè´¹æŒ‡æ•°
            "512690": "930917",  # æ¸¯è‚¡é€š50ETF -> æ¸¯è‚¡é€š50æŒ‡æ•°
            "513050": "H30533.CSI",  # ä¸­æ¦‚äº’è”ETF -> ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°
            "513100": "^NDX",  # çº³æŒ‡100ETF -> çº³æ–¯è¾¾å…‹100æŒ‡æ•°
            "513500": "H30533.CSI",  # ä¸­æ¦‚äº’è”ETF -> ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°
            "513400": "HSNDXIT.HI"  # æ’ç”Ÿäº’è”ç½‘ETF -> æ’ç”Ÿäº’è”ç½‘ç§‘æŠ€ä¸šæŒ‡æ•°
        }
        
        index_code = index_mapping.get(etf_code)
        if not index_code:
            logger.info(f"ETF {etf_code} æ²¡æœ‰å¯¹åº”çš„æŒ‡æ•°æ˜ å°„ï¼Œæ— æ³•ä½¿ç”¨æŒ‡æ•°æ•°æ®ä½œä¸ºå¤‡é€‰")
            return pd.DataFrame()
        
        logger.info(f"å°è¯•ä½¿ç”¨æŒ‡æ•° {index_code} æ•°æ®ä½œä¸ºETF {etf_code} çš„å¤‡é€‰æ•°æ®")
        
        # æ ¹æ®æŒ‡æ•°ç±»å‹ä½¿ç”¨ä¸åŒçš„æ•°æ®æ¥å£
        if index_code.startswith('^'):
            # ç¾è‚¡æŒ‡æ•°
            return fetch_us_index_from_yfinance(index_code, start_date, end_date)
        
        elif index_code.endswith('.CSI'):
            # ä¸­è¯ç³»åˆ—æŒ‡æ•°
            index_name = index_code.replace('.CSI', '')
            return ak.index_zh_a_hist(
                symbol=index_name,
                period="daily",
                start_date=start_date,
                end_date=end_date
            )
        
        elif index_code.endswith('.HI'):
            # æ’ç”Ÿç³»åˆ—æŒ‡æ•°
            index_name = index_code.replace('.HI', '')
            
            # å°è¯•ä½¿ç”¨ index_hk_hist æ–¹æ³•
            try:
                df = ak.index_hk_hist(symbol=index_name, period="daily", 
                                     start_date=start_date, end_date=end_date)
                if not df.empty:
                    logger.info(f"ğŸ“Š index_hk_hist æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
                    logger.info(f"âœ… é€šè¿‡ index_hk_hist æ–¹æ³•æˆåŠŸè·å–æ’ç”ŸæŒ‡æ•° {index_code} æ•°æ®")
                    return df
            except Exception as e:
                logger.warning(f"index_hk_hist æ–¹æ³•å¤±è´¥: {str(e)}")
            
            # å°è¯•ä½¿ç”¨ stock_hk_index_hist æ–¹æ³•
            try:
                df = ak.stock_hk_index_hist(symbol=index_name, period="daily", 
                                          start_date=start_date, end_date=end_date)
                if not df.empty:
                    logger.info(f"ğŸ“Š stock_hk_index_hist æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
                    logger.info(f"âœ… é€šè¿‡ stock_hk_index_hist æ–¹æ³•æˆåŠŸè·å–æ’ç”ŸæŒ‡æ•° {index_code} æ•°æ®")
                    return df
            except Exception as e:
                logger.warning(f"stock_hk_index_hist æ–¹æ³•å¤±è´¥: {str(e)}")
            
            logger.warning(f"æ— æ³•è·å–æ’ç”ŸæŒ‡æ•° {index_code} æ•°æ®")
            return pd.DataFrame()
        
        else:
            # Aè‚¡æŒ‡æ•°
            return ak.index_zh_a_hist(
                symbol=index_code,
                period="daily",
                start_date=start_date,
                end_date=end_date
            )
    
    except Exception as e:
        logger.error(f"é€šè¿‡æŒ‡æ•°æ•°æ®è·å–ETF {etf_code} å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰å¿…éœ€çš„äº¤æ˜“æ•°æ®åˆ—ï¼Œç¼ºå¤±çš„åˆ—ç”¨é»˜è®¤å€¼å¡«å……
    Args:
        df: åŸå§‹DataFrame
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    if df.empty:
        return df
    
    # å®šä¹‰åŸºç¡€å¿…éœ€åˆ—ï¼ˆåŒ…å«"æŠ˜æº¢ä»·ç‡"ï¼‰
    required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æŠ˜æº¢ä»·ç‡"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"âŒ æ•°æ®æºç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(missing_columns)}ï¼Œå°†å°è¯•ä¿®å¤")
    
    # 1. ä¼˜å…ˆä½¿ç”¨æ•°æ®æºæä¾›çš„åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®
    if "æŠ˜æº¢ä»·ç‡" not in df.columns:
        logger.warning("âš ï¸ æ•°æ®æºä¸æä¾›æŠ˜æº¢ä»·ç‡åˆ—ï¼Œå°†å°è¯•é€šè¿‡å‡€å€¼æˆ–IOPVè®¡ç®—")
        
        # å°è¯•ä»fund_etf_hist_emè·å–çš„å‡€å€¼æ•°æ®è®¡ç®—
        if "å‡€å€¼" in df.columns and "æ”¶ç›˜" in df.columns:
            df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["å‡€å€¼"]) / df["å‡€å€¼"] * 100).round(2)
            logger.info("âœ… é€šè¿‡å‡€å€¼æˆåŠŸè®¡ç®—æŠ˜æº¢ä»·ç‡")
        # å°è¯•ä»fund_etf_hist_sinaè·å–çš„IOPVæ•°æ®è®¡ç®—
        elif "IOPV" in df.columns and "æ”¶ç›˜" in df.columns:
            df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["IOPV"]) / df["IOPV"] * 100).round(2)
            logger.info("âœ… é€šè¿‡IOPVæˆåŠŸè®¡ç®—æŠ˜æº¢ä»·ç‡")
        else:
            logger.error("âŒ æ— æ³•è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼Œæ•°æ®ä¸å¯ç”¨")
            # ä»ç„¶åˆ›å»ºæŠ˜æº¢ä»·ç‡åˆ—ï¼Œä½†ç”¨NaNå¡«å……
            df["æŠ˜æº¢ä»·ç‡"] = float('nan')
    else:
        # 2. æ£€æŸ¥åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if df["æŠ˜æº¢ä»·ç‡"].isna().all() or (df["æŠ˜æº¢ä»·ç‡"] == 0).all():
            logger.warning("âš ï¸ åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®å…¨ä¸º0æˆ–ç©ºå€¼ï¼Œå°†å°è¯•é‡æ–°è®¡ç®—")
            # å°è¯•ä»å‡€å€¼é‡æ–°è®¡ç®—
            if "å‡€å€¼" in df.columns and "æ”¶ç›˜" in df.columns:
                df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["å‡€å€¼"]) / df["å‡€å€¼"] * 100).round(2)
                logger.info("âœ… é€šè¿‡å‡€å€¼é‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡")
            # å°è¯•ä»IOPVé‡æ–°è®¡ç®—
            elif "IOPV" in df.columns and "æ”¶ç›˜" in df.columns:
                df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["IOPV"]) / df["IOPV"] * 100).round(2)
                logger.info("âœ… é€šè¿‡IOPVé‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡")
            else:
                logger.warning("â„¹ï¸ æ— æ³•é‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼Œä¿ç•™åŸå§‹æ•°æ®ï¼ˆå¯èƒ½å…¨ä¸º0ï¼‰")
    
    # 3. å¤„ç†å…¶ä»–è¡ç”Ÿåˆ—
    derived_columns = ["æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
    missing_derived_columns = [col for col in derived_columns if col not in df.columns]
    
    if missing_derived_columns:
        logger.info(f"â„¹ï¸ æ•°æ®æºç¼ºå°‘å¯è®¡ç®—åˆ—ï¼š{', '.join(missing_derived_columns)}ï¼Œå°†å°è¯•è®¡ç®—")
        
        for col in missing_derived_columns:
            try:
                if col == 'æˆäº¤é¢':
                    # å¦‚æœæœ‰æˆäº¤é‡å’Œæ”¶ç›˜ä»·ï¼Œå¯ä»¥è®¡ç®—æˆäº¤é¢
                    if 'æˆäº¤é‡' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # æ³¨æ„ï¼šAè‚¡æˆäº¤é‡å•ä½æ˜¯"æ‰‹"ï¼ˆ1æ‰‹=100è‚¡ï¼‰
                        df['æˆäº¤é¢'] = (df['æˆäº¤é‡'] * df['æ”¶ç›˜'] * 100 / 10000).round(2)
                        logger.info("âœ… æˆåŠŸè®¡ç®—æˆäº¤é¢")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æˆäº¤é¢ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æŒ¯å¹…':
                    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / å‰æ”¶ç›˜ * 100%
                    if 'æœ€é«˜' in df.columns and 'æœ€ä½' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # ä½¿ç”¨å‰ä¸€å¤©æ”¶ç›˜ä»·ä½œä¸ºå‰æ”¶ç›˜
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['å‰æ”¶ç›˜'] * 100).round(2)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æŒ¯å¹…")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æŒ¯å¹…ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¶¨è·Œå¹…':
                    # æ¶¨è·Œå¹… = (æ”¶ç›˜ - å‰æ”¶ç›˜) / å‰æ”¶ç›˜ * 100%
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æ¶¨è·Œå¹…'] = ((df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']) / df['å‰æ”¶ç›˜'] * 100).round(2)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æ¶¨è·Œå¹…")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æ¶¨è·Œå¹…ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¶¨è·Œé¢':
                    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰æ”¶ç›˜
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æ¶¨è·Œé¢'] = (df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']).round(4)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æ¶¨è·Œé¢")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æ¶¨è·Œé¢ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¢æ‰‹ç‡':
                    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / æµé€šè‚¡æœ¬ * 100%
                    # ç”±äºä¸çŸ¥é“æµé€šè‚¡æœ¬ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—
                    logger.warning("âš ï¸ æ— æ³•å‡†ç¡®è®¡ç®—æ¢æ‰‹ç‡ï¼Œç¼ºå°‘æµé€šè‚¡æœ¬æ•°æ®")
                    # ä¸å¡«å……æ¢æ‰‹ç‡ï¼Œå› ä¸ºä¸å‡†ç¡®
            
            except Exception as e:
                logger.error(f"è®¡ç®—åˆ— {col} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
    
    # 4. å†æ¬¡æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
    final_missing_columns = [col for col in required_columns if col not in df.columns]
    if final_missing_columns:
        logger.error(f"âŒ ä¿®å¤åä»ç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(final_missing_columns)}")
        return pd.DataFrame()
    
    return df

def clean_and_format_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—å¹¶æ ¼å¼åŒ–æ•°æ®
    """
    try:
        # åˆ›å»ºDataFrameçš„æ·±æ‹·è´ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        # å¤„ç†æ—¥æœŸåˆ—
        if "æ—¥æœŸ" in df.columns:
            # å°è¯•å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeç±»å‹
            try:
                # å…ˆç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºå¤„ç†å„ç§å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
                # å°è¯•è½¬æ¢ä¸ºdatetime
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè½¬æ¢
                if pd.api.types.is_datetime64_any_dtype(df["æ—¥æœŸ"]):
                    # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")
                else:
                    logger.warning("æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeå¤±è´¥ï¼Œä¿ç•™åŸå§‹å€¼")
            except Exception as e:
                logger.error(f"æ—¥æœŸåˆ—å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        df = ensure_required_columns(df)
        # å¤„ç†æ•°å€¼åˆ—
        numeric_cols = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æŠ˜æº¢ä»·ç‡"]
        for col in numeric_cols:
            if col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                except Exception as e:
                    logger.error(f"åˆ— {col} è½¬æ¢ä¸ºæ•°å€¼å¤±è´¥: {str(e)}", exc_info=True)
        # è®¡ç®—ç¼ºå¤±åˆ—
        if "æˆäº¤é‡" in df.columns and "æ”¶ç›˜" in df.columns:
            # å¦‚æœæœ‰æˆäº¤é‡å’Œæ”¶ç›˜ä»·ï¼Œå¯ä»¥è®¡ç®—æˆäº¤é¢
            if "æˆäº¤é¢" not in df.columns:
                df["æˆäº¤é¢"] = df["æˆäº¤é‡"] * df["æ”¶ç›˜"]
        # è®¡ç®—æ¶¨è·Œå¹…ç­‰
        if "æ”¶ç›˜" in df.columns:
            if "æ¶¨è·Œå¹…" not in df.columns:
                df["æ¶¨è·Œå¹…"] = df["æ”¶ç›˜"].pct_change() * 100
            if "æ¶¨è·Œé¢" not in df.columns:
                df["æ¶¨è·Œé¢"] = df["æ”¶ç›˜"].diff()
        # å¤„ç†NaNå€¼
        if "æ—¥æœŸ" in df.columns and "æ”¶ç›˜" in df.columns:
            df = df.dropna(subset=["æ—¥æœŸ", "æ”¶ç›˜"])
        return df
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

def limit_to_one_year_data(df: pd.DataFrame, end_date: str) -> pd.DataFrame:
    """
    é™åˆ¶æ•°æ®ä¸ºæœ€è¿‘1å¹´çš„æ•°æ®
    
    Args:
        df: åŸå§‹DataFrame
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: é™åˆ¶ä¸º1å¹´æ•°æ®åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸ
        one_year_ago = (pd.to_datetime(end_date) - timedelta(days=365)).strftime("%Y-%m-%d")
        
        # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            logger.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œæ— æ³•é™åˆ¶ä¸º1å¹´æ•°æ®")
            return df
        
        # åˆ›å»ºDataFrameçš„å‰¯æœ¬ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # è½¬æ¢æ—¥æœŸåˆ—
        df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è¿‡æ»¤æ•°æ®
        mask = df["æ—¥æœŸ"] >= pd.to_datetime(one_year_ago)
        df = df.loc[mask]
        
        logger.info(f"æ•°æ®å·²é™åˆ¶ä¸ºæœ€è¿‘1å¹´ï¼ˆä» {one_year_ago} è‡³ {end_date}ï¼‰ï¼Œå‰©ä½™ {len(df)} æ¡æ•°æ®")
        return df
    except Exception as e:
        logger.error(f"é™åˆ¶æ•°æ®ä¸º1å¹´æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return df
