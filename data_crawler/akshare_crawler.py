#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨AkShareæ¥å£è·å–ETFæ—¥çº¿æ•°æ®
ç‰¹åˆ«ä¼˜åŒ–äº†åˆ—åæ˜ å°„å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import akshare as ak
import pandas as pd
import logging
import time
from typing import Optional, Dict, Any, Tuple
from datetime import datetime, timedelta, date  # ä¿®å¤ï¼šæ·»åŠ dateå¯¼å…¥
from config import Config
from retrying import retry

# ä¿®å¤ï¼šæ­£ç¡®å¯¼å…¥å‡½æ•°
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    get_last_trading_day,
    is_trading_day
)
# ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
from utils.file_utils import (
    ensure_chinese_columns,internal_ensure_chinese_columns
)
from utils.data_processor import (
    ensure_required_columns,
    clean_and_format_data,
    limit_to_one_year_data
)

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# é‡è¯•é…ç½®
MAX_RETRY_ATTEMPTS = 5  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼Œä»3å¢åŠ åˆ°5
RETRY_WAIT_FIXED = 3000  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œä»2000æ¯«ç§’å¢åŠ åˆ°3000æ¯«ç§’
RETRY_WAIT_EXPONENTIAL_MAX = 15000  # å¢åŠ æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œä»10000æ¯«ç§’å¢åŠ åˆ°15000æ¯«ç§’

# æ‰“å°AkShareç‰ˆæœ¬
logger.info(f"AkShareç‰ˆæœ¬: {ak.__version__}")

def empty_result_check(result: pd.DataFrame) -> bool:
    """
    æ£€æŸ¥AkShareè¿”å›ç»“æœæ˜¯å¦ä¸ºç©º
    
    Args:
        result: AkShareè¿”å›çš„DataFrame
        
    Returns:
        bool: å¦‚æœç»“æœä¸ºç©ºè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    return result is None or result.empty

def retry_if_akshare_error(exception: Exception) -> bool:
    """
    é‡è¯•æ¡ä»¶ï¼šAkShareç›¸å…³é”™è¯¯
    
    Args:
        exception: å¼‚å¸¸å¯¹è±¡
        
    Returns:
        bool: å¦‚æœæ˜¯AkShareé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # æ‰©å±•å¼‚å¸¸ç±»å‹ï¼ŒåŒ…æ‹¬requestsåº“çš„ç½‘ç»œé”™è¯¯
    from requests.exceptions import ConnectionError, Timeout
    return isinstance(exception, (ValueError, ConnectionError, Timeout, OSError))

@retry(
    stop_max_attempt_number=MAX_RETRY_ATTEMPTS,
    wait_fixed=RETRY_WAIT_FIXED,
    retry_on_result=empty_result_check,
    retry_on_exception=retry_if_akshare_error
)
def crawl_etf_daily_akshare(etf_code: str, start_date: str, end_date: str, is_first_crawl: bool = False) -> pd.DataFrame:
    """ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    Args:
        etf_code: ETFä»£ç  (6ä½æ•°å­—)
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        is_first_crawl: æ˜¯å¦æ˜¯é¦–æ¬¡çˆ¬å–
    
    Returns:
        pd.DataFrame: åŒ…å«ETFæ—¥çº¿æ•°æ®çš„DataFrame
    """
    try:
        # ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdateå¯¹è±¡
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day(end_date_obj)
        # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
        end_date = last_trading_day.strftime("%Y-%m-%d")
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†å•æ—¥è¯·æ±‚é—®é¢˜
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
        if start_date_obj == last_trading_day:
            # å¦‚æœæ˜¯å•æ—¥è¯·æ±‚ï¼Œæ‰©å±•ä¸ºè‡³å°‘3å¤©çš„èŒƒå›´
            start_date_obj = start_date_obj - timedelta(days=2)
            start_date = start_date_obj.strftime("%Y-%m-%d")
            logger.info(f"å•æ—¥è¯·æ±‚æ‰©å±•ä¸º {start_date} è‡³ {end_date}")
        
        logger.info(f"å¼€å§‹çˆ¬å–ETF {etf_code} çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        
        # ç‰¹æ®Šå¤„ç†ETF 513750 - ä½¿ç”¨æ›´çµæ´»çš„æ—¥æœŸèŒƒå›´
        if etf_code == "513750":
            # å°è¯•è·å–æœ€è¿‘30å¤©æ•°æ®
            extended_start_date = (last_trading_day - timedelta(days=30)).strftime("%Y-%m-%d")
            logger.info(f"ç‰¹æ®Šå¤„ç†ETF 513750ï¼Œæ‰©å±•æ—¥æœŸèŒƒå›´ä¸º {extended_start_date} è‡³ {end_date}")
            df = try_multiple_akshare_interfaces(etf_code, extended_start_date, end_date)
        else:
            # å°è¯•å¤šç§AkShareæ¥å£
            df = try_multiple_akshare_interfaces(etf_code, start_date, end_date)
        
        if df.empty:
            logger.warning(f"AkShareæœªè·å–åˆ°{etf_code}æ•°æ®ï¼ˆ{start_date}è‡³{end_date}ï¼‰")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.info(f"ğŸ“Š AkShareæ•°æ®æºè¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
        
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨å†…éƒ¨åˆ—åæ ‡å‡†åŒ–å‡½æ•°ï¼ˆä¸ä¾èµ–utils.file_utilsï¼‰
        # åŸå› ï¼šé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜ï¼Œä¸”è¯¥å‡½æ•°é’ˆå¯¹ETFæ•°æ®çˆ¬å–åœºæ™¯ä¼˜åŒ–
        df = internal_ensure_chinese_columns(df)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # å°è¯•ä½¿ç”¨æ›¿ä»£åˆ—
            if "æ—¥æœŸ" not in df.columns and "äº¤æ˜“æ—¥æœŸ" in df.columns:
                df["æ—¥æœŸ"] = df["äº¤æ˜“æ—¥æœŸ"]
            if "æ”¶ç›˜" not in df.columns and "æœ€æ–°ä»·" in df.columns:
                df["æ”¶ç›˜"] = df["æœ€æ–°ä»·"]
            if "æˆäº¤é‡" not in df.columns and "æˆäº¤æ•°é‡" in df.columns:
                df["æˆäº¤é‡"] = df["æˆäº¤æ•°é‡"]
            # å†æ¬¡æ£€æŸ¥å…³é”®åˆ—
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.error(f"ETF {etf_code} æ•°æ®ä»ç„¶ç¼ºå°‘å¿…è¦åˆ—ï¼Œæ— æ³•ç»§ç»­å¤„ç†: {', '.join(missing_columns)}")
                return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ç»Ÿä¸€ä¸ºYYYY-MM-DD
        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
        
        # ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        if "æ—¥æœŸ" in df.columns:
            df = df[(df["æ—¥æœŸ"] >= start_date) & (df["æ—¥æœŸ"] <= end_date)]
        
        # ç‰¹æ®Šå¤„ç†ETF 513750 - ä¿®å¤å¯èƒ½çš„åˆ—åé—®é¢˜
        if etf_code == "513750" and "æŠ˜æº¢ä»·ç‡" not in df.columns and "åŸºé‡‘æŠ˜ä»·ç‡" in df.columns:
            df["æŠ˜æº¢ä»·ç‡"] = df["åŸºé‡‘æŠ˜ä»·ç‡"]
        
        # æ•°æ®æ¸…æ´—ï¼šå»é‡
        if "æ—¥æœŸ" in df.columns:
            df = df.sort_values("æ—¥æœŸ", ascending=False).drop_duplicates(subset=["æ—¥æœŸ"], keep="first")
        
        # é¦–æ¬¡çˆ¬å–æ—¶é™åˆ¶æ•°æ®é‡ä¸º1å¹´ï¼ˆ365å¤©ï¼‰
        if is_first_crawl:
            if "æ—¥æœŸ" in df.columns and len(df) > 365:
                df = df.head(365)
                logger.info(f"é¦–æ¬¡çˆ¬å–é™åˆ¶æ•°æ®é‡ä¸º365æ¡ï¼Œå½“å‰ETF {etf_code} æ•°æ®å·²æˆªå–å‰365æ¡")
        
        logger.info(f"æˆåŠŸè·å–ETF {etf_code} æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
        return df
    except Exception as e:
        logger.error(f"çˆ¬å–ETF {etf_code} å¤±è´¥: {str(e)}", exc_info=True)
        raise  # è§¦å‘é‡è¯•

def try_multiple_akshare_interfaces(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """å°è¯•å¤šç§AkShareæ¥å£è·å–ETFæ•°æ®"""
    logger.info(f"å°è¯•è·å–ETF {etf_code} æ•°æ®ï¼Œæœ€å¤š 3 ç§æ¥å£")
    
    # æ¥å£1: fund_etf_hist_em (æä¾›IOPVå’ŒæŠ˜æº¢ä»·ç‡)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_hist_emæ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", 
                                start_date=start_date, end_date=end_date, adjust="")
        
        if not df.empty:
            logger.info(f"ç¬¬1ç§æ¥å£ï¼ˆfund_etf_hist_emï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            if 'å‡€å€¼æ—¥æœŸ' in df.columns:
                df = df.rename(columns={
                    'å‡€å€¼æ—¥æœŸ': 'æ—¥æœŸ',
                    'å•ä½å‡€å€¼': 'IOPV',
                    'æŠ˜ä»·ç‡': 'æŠ˜æº¢ä»·ç‡'
                })
            elif 'å‡€å€¼ä¼°ç®—æ—¥æœŸ' in df.columns:
                df = df.rename(columns={
                    'å‡€å€¼ä¼°ç®—æ—¥æœŸ': 'æ—¥æœŸ',
                    'å•ä½å‡€å€¼ä¼°ç®—': 'IOPV',
                    'æŠ˜ä»·ç‡ä¼°ç®—': 'æŠ˜æº¢ä»·ç‡'
                })
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
            
            return df
    except Exception as e:
        logger.debug(f"fund_etf_hist_emæ¥å£å¤±è´¥: {str(e)}")
    
    # æ¥å£2: stock_zh_index_daily_js (æä¾›åŸºç¡€æ•°æ®)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨stock_zh_index_daily_jsæ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.stock_zh_index_daily_js(symbol=f"sh{etf_code}" if etf_code.startswith('5') else f"sz{etf_code}")
        
        if not df.empty:
            logger.info(f"ç¬¬2ç§æ¥å£ï¼ˆstock_zh_index_daily_jsï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š stock_zh_index_daily_js æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡'
            })
            
            # å°è¯•è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if 'IOPV' in df.columns and 'æ”¶ç›˜' in df.columns:
                df['æŠ˜æº¢ä»·ç‡'] = (df['æ”¶ç›˜'] - df['IOPV']) / df['IOPV'] * 100
            
            return df
    except Exception as e:
        logger.debug(f"stock_zh_index_daily_jsæ¥å£å¤±è´¥: {str(e)}")
    
    # æ¥å£3: fund_etf_hist_sina (åŸºç¡€æ•°æ®)
    try:
        logger.info(f"å°è¯•ä½¿ç”¨fund_etf_hist_sinaæ¥å£è·å–ETF {etf_code} æ•°æ®")
        df = ak.fund_etf_hist_sina(symbol=etf_code, period="daily", 
                                 start_date=start_date, end_date=end_date)
        
        if not df.empty:
            logger.info(f"ç¬¬3ç§æ¥å£ï¼ˆfund_etf_hist_sinaï¼‰æˆåŠŸè·å–ETF {etf_code} æ•°æ®")
            logger.info(f"ğŸ“Š fund_etf_hist_sina æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡'
            })
            
            return df
    except Exception as e:
        logger.debug(f"fund_etf_hist_sinaæ¥å£å¤±è´¥: {str(e)}")
    
    logger.warning(f"æ‰€æœ‰AkShareæ¥å£å‡æœªè·å–åˆ°ETF {etf_code} æ•°æ®")
    return pd.DataFrame()

def try_fund_etf_hist_em(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–ETF {etf_code} æ•°æ®")
        
        # ç‰¹æ®Šå¤„ç†ETF 513750 - ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥
        if etf_code == "513750":
            end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
            last_trading_day = get_last_trading_day(end_date_obj)
            end_date = last_trading_day.strftime("%Y-%m-%d")
        
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", 
                               start_date=start_date, end_date=end_date, adjust="qfq")
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # ä½¿ç”¨å†…éƒ¨åˆ—åæ ‡å‡†åŒ–å‡½æ•°ï¼ˆæ›¿æ¢åŸæœ‰çš„ensure_chinese_columnsï¼‰
            df = internal_ensure_chinese_columns(df)
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if "æ—¥æœŸ" in df.columns:
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
            
            # ç¡®ä¿æ•°æ®åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…
            if "æ—¥æœŸ" in df.columns:
                df = df[(df["æ—¥æœŸ"] >= start_date) & (df["æ—¥æœŸ"] <= end_date)]
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©é‡è¯•æœºåˆ¶å¤„ç†

def try_fund_etf_hist_sina(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_sina æ¥å£è·å–ETF {etf_code} æ•°æ®")
        symbol = get_symbol_with_market_prefix(etf_code)
        df = ak.fund_etf_hist_sina(symbol=symbol)
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_sina æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ–°æµªæ¥å£è¿”å›çš„åˆ—åå¯èƒ½æ˜¯è‹±æ–‡ï¼Œéœ€è¦è½¬æ¢ä¸ºä¸­æ–‡
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢',
                'amplitude': 'æŒ¯å¹…',
                'percent': 'æ¶¨è·Œå¹…',
                'change': 'æ¶¨è·Œé¢',
                'turnover_rate': 'æ¢æ‰‹ç‡',
                'trade_date': 'æ—¥æœŸ',
                'open_price': 'å¼€ç›˜',
                'high_price': 'æœ€é«˜',
                'low_price': 'æœ€ä½',
                'close_price': 'æ”¶ç›˜',
                'vol': 'æˆäº¤é‡',
                'amount_volume': 'æˆäº¤é¢',
                'amplitude_percent': 'æŒ¯å¹…',
                'pct_chg': 'æ¶¨è·Œå¹…',
                'price_change': 'æ¶¨è·Œé¢',
                'turnover_ratio': 'æ¢æ‰‹ç‡',
                'net_value': 'å‡€å€¼',
                'iopv': 'IOPV'
            }
            # é‡å‘½ååˆ—
            df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
            if 'æ—¥æœŸ' not in df.columns and 'date' in df.columns:
                df = df.rename(columns={'date': 'æ—¥æœŸ'})
            
            # æ—¥æœŸè¿‡æ»¤ï¼ˆå…³é”®ä¿®å¤ï¼‰
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                mask = (df['æ—¥æœŸ'] >= pd.to_datetime(start_date)) & (df['æ—¥æœŸ'] <= pd.to_datetime(end_date))
                df = df.loc[mask]
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_sina æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_symbol_with_market_prefix(etf_code: str) -> str:
    """
    æ ¹æ®ETFä»£ç è·å–å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        str: å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç 
    """
    if etf_code.startswith(('5', '6', '9')):
        return f"sh{etf_code}"
    else:
        return f"sz{etf_code}"

def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–åˆ—åï¼Œå°†ä¸åŒæ•°æ®æºçš„åˆ—åè½¬æ¢ä¸ºç»Ÿä¸€çš„ä¸­æ–‡åˆ—å
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pd.DataFrame: æ ‡å‡†åŒ–åˆ—ååçš„DataFrame
    """
    if df.empty:
        return df
    
    # å®šä¹‰å¯èƒ½çš„åˆ—åå˜ä½“
    column_variants = {
        'æ—¥æœŸ': ['date', 'æ—¥æœŸ', 'trade_date', 'dt', 'datetime', 'äº¤æ˜“æ—¥æœŸ', 'time'],
        'å¼€ç›˜': ['open', 'å¼€ç›˜ä»·', 'å¼€', 'open_price', 'openprice', 'openprice_'],
        'æœ€é«˜': ['high', 'æœ€é«˜ä»·', 'é«˜', 'high_price', 'highprice', 'highprice_'],
        'æœ€ä½': ['low', 'æœ€ä½ä»·', 'ä½', 'low_price', 'lowprice', 'lowprice_'],
        'æ”¶ç›˜': ['close', 'æ”¶ç›˜ä»·', 'æ”¶', 'close_price', 'closeprice', 'closeprice_', 'price'],
        'æˆäº¤é‡': ['volume', 'æˆäº¤é‡', 'vol', 'æˆäº¤æ•°é‡', 'amount_volume', 'vol_', 'volume_'],
        'æˆäº¤é¢': ['amount', 'æˆäº¤é¢', 'æˆäº¤é‡‘é¢', 'turnover', 'æˆäº¤æ€»ä»·', 'amount_', 'turnover_'],
        'æŒ¯å¹…': ['amplitude', 'æŒ¯å¹…%', 'æŒ¯å¹…ç™¾åˆ†æ¯”', 'amplitude_percent', 'amplitude_', 'amp_'],
        'æ¶¨è·Œå¹…': ['percent', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œ%', 'change_percent', 'pct_chg', 'changepercent', 'chg_pct', 'pctchange'],
        'æ¶¨è·Œé¢': ['change', 'æ¶¨è·Œé¢', 'ä»·æ ¼å˜åŠ¨', 'price_change', 'change_', 'chg_', 'pricechg'],
        'æ¢æ‰‹ç‡': ['turnover_rate', 'æ¢æ‰‹ç‡', 'turnover_ratio', 'turnover', 'turnoverrate', 'turnover_rate_']
    }
    
    # åˆ›å»ºæ–°çš„åˆ—åæ˜ å°„
    new_columns = {}
    for standard_name, variants in column_variants.items():
        for variant in variants:
            if variant in df.columns and variant not in new_columns:
                new_columns[variant] = standard_name
    
    # é‡å‘½ååˆ—
    df = df.rename(columns=new_columns)
    
    logger.info(f"âœ… æ ‡å‡†åŒ–åçš„åˆ—å: {list(df.columns)}")
    return df

def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰å¿…éœ€çš„äº¤æ˜“æ•°æ®åˆ—ï¼Œç¼ºå¤±çš„åˆ—ç”¨é»˜è®¤å€¼å¡«å……
    Args:
        df: åŸå§‹DataFrame
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    if df.empty:
        return df
    
    # å®šä¹‰åŸºç¡€å¿…éœ€åˆ—ï¼ˆåŒ…å«"æŠ˜æº¢ä»·ç‡"ï¼‰
    required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æŠ˜æº¢ä»·ç‡"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"âŒ æ•°æ®æºç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(missing_columns)}ï¼Œå°†å°è¯•ä¿®å¤")
    
    # 1. ä¼˜å…ˆä½¿ç”¨æ•°æ®æºæä¾›çš„åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®
    if "æŠ˜æº¢ä»·ç‡" not in df.columns:
        logger.warning("âš ï¸ æ•°æ®æºä¸æä¾›æŠ˜æº¢ä»·ç‡åˆ—ï¼Œå°†å°è¯•é€šè¿‡å‡€å€¼æˆ–IOPVè®¡ç®—")
        
        # å°è¯•ä»fund_etf_hist_emè·å–çš„å‡€å€¼æ•°æ®è®¡ç®—
        if "å‡€å€¼" in df.columns and "æ”¶ç›˜" in df.columns:
            df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["å‡€å€¼"]) / df["å‡€å€¼"] * 100).round(2)
            logger.info("âœ… é€šè¿‡å‡€å€¼æˆåŠŸè®¡ç®—æŠ˜æº¢ä»·ç‡")
        # å°è¯•ä»fund_etf_hist_sinaè·å–çš„IOPVæ•°æ®è®¡ç®—
        elif "IOPV" in df.columns and "æ”¶ç›˜" in df.columns:
            df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["IOPV"]) / df["IOPV"] * 100).round(2)
            logger.info("âœ… é€šè¿‡IOPVæˆåŠŸè®¡ç®—æŠ˜æº¢ä»·ç‡")
        else:
            logger.error("âŒ æ— æ³•è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼Œæ•°æ®ä¸å¯ç”¨")
            # ä»ç„¶åˆ›å»ºæŠ˜æº¢ä»·ç‡åˆ—ï¼Œä½†ç”¨NaNå¡«å……
            df["æŠ˜æº¢ä»·ç‡"] = float('nan')
    else:
        # 2. æ£€æŸ¥åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if df["æŠ˜æº¢ä»·ç‡"].isna().all() or (df["æŠ˜æº¢ä»·ç‡"] == 0).all():
            logger.warning("âš ï¸ åŸå§‹æŠ˜æº¢ä»·ç‡æ•°æ®å…¨ä¸º0æˆ–ç©ºå€¼ï¼Œå°†å°è¯•é‡æ–°è®¡ç®—")
            # å°è¯•ä»å‡€å€¼é‡æ–°è®¡ç®—
            if "å‡€å€¼" in df.columns and "æ”¶ç›˜" in df.columns:
                df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["å‡€å€¼"]) / df["å‡€å€¼"] * 100).round(2)
                logger.info("âœ… é€šè¿‡å‡€å€¼é‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡")
            # å°è¯•ä»IOPVé‡æ–°è®¡ç®—
            elif "IOPV" in df.columns and "æ”¶ç›˜" in df.columns:
                df["æŠ˜æº¢ä»·ç‡"] = ((df["æ”¶ç›˜"] - df["IOPV"]) / df["IOPV"] * 100).round(2)
                logger.info("âœ… é€šè¿‡IOPVé‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡")
            else:
                logger.warning("â„¹ï¸ æ— æ³•é‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼Œä¿ç•™åŸå§‹æ•°æ®ï¼ˆå¯èƒ½å…¨ä¸º0ï¼‰")
    
    # 3. å¤„ç†å…¶ä»–è¡ç”Ÿåˆ—
    derived_columns = ["æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
    missing_derived_columns = [col for col in derived_columns if col not in df.columns]
    
    if missing_derived_columns:
        logger.info(f"â„¹ï¸ æ•°æ®æºç¼ºå°‘å¯è®¡ç®—åˆ—ï¼š{', '.join(missing_derived_columns)}ï¼Œå°†å°è¯•è®¡ç®—")
        
        for col in missing_derived_columns:
            try:
                if col == 'æˆäº¤é¢':
                    # å¦‚æœæœ‰æˆäº¤é‡å’Œæ”¶ç›˜ä»·ï¼Œå¯ä»¥è®¡ç®—æˆäº¤é¢
                    if 'æˆäº¤é‡' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # æ³¨æ„ï¼šAè‚¡æˆäº¤é‡å•ä½æ˜¯"æ‰‹"ï¼ˆ1æ‰‹=100è‚¡ï¼‰
                        df['æˆäº¤é¢'] = (df['æˆäº¤é‡'] * df['æ”¶ç›˜'] * 100 / 10000).round(2)
                        logger.info("âœ… æˆåŠŸè®¡ç®—æˆäº¤é¢")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æˆäº¤é¢ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æŒ¯å¹…':
                    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / å‰æ”¶ç›˜ * 100%
                    if 'æœ€é«˜' in df.columns and 'æœ€ä½' in df.columns and 'æ”¶ç›˜' in df.columns:
                        # ä½¿ç”¨å‰ä¸€å¤©æ”¶ç›˜ä»·ä½œä¸ºå‰æ”¶ç›˜
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['å‰æ”¶ç›˜'] * 100).round(2)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æŒ¯å¹…")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æŒ¯å¹…ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¶¨è·Œå¹…':
                    # æ¶¨è·Œå¹… = (æ”¶ç›˜ - å‰æ”¶ç›˜) / å‰æ”¶ç›˜ * 100%
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æ¶¨è·Œå¹…'] = ((df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']) / df['å‰æ”¶ç›˜'] * 100).round(2)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æ¶¨è·Œå¹…")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æ¶¨è·Œå¹…ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¶¨è·Œé¢':
                    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰æ”¶ç›˜
                    if 'æ”¶ç›˜' in df.columns:
                        df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
                        # å¤„ç†ç¬¬ä¸€å¤©ï¼ˆæ²¡æœ‰å‰æ”¶ç›˜ï¼‰çš„æƒ…å†µ
                        df['å‰æ”¶ç›˜'] = df['å‰æ”¶ç›˜'].fillna(df['å¼€ç›˜'])
                        df['æ¶¨è·Œé¢'] = (df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']).round(4)
                        df = df.drop(columns=['å‰æ”¶ç›˜'])
                        logger.info("âœ… æˆåŠŸè®¡ç®—æ¶¨è·Œé¢")
                    else:
                        logger.warning("âš ï¸ æ— æ³•è®¡ç®—æ¶¨è·Œé¢ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
                
                elif col == 'æ¢æ‰‹ç‡':
                    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / æµé€šè‚¡æœ¬ * 100%
                    # ç”±äºä¸çŸ¥é“æµé€šè‚¡æœ¬ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—
                    logger.warning("âš ï¸ æ— æ³•å‡†ç¡®è®¡ç®—æ¢æ‰‹ç‡ï¼Œç¼ºå°‘æµé€šè‚¡æœ¬æ•°æ®")
                    # ä¸å¡«å……æ¢æ‰‹ç‡ï¼Œå› ä¸ºä¸å‡†ç¡®
            
            except Exception as e:
                logger.error(f"è®¡ç®—åˆ— {col} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
    
    # 4. å†æ¬¡æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
    final_missing_columns = [col for col in required_columns if col not in df.columns]
    if final_missing_columns:
        logger.error(f"âŒ ä¿®å¤åä»ç¼ºå°‘å¿…éœ€åˆ—ï¼š{', '.join(final_missing_columns)}")
        return pd.DataFrame()
    
    return df

def clean_and_format_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—å¹¶æ ¼å¼åŒ–æ•°æ®
    """
    try:
        # åˆ›å»ºDataFrameçš„æ·±æ‹·è´ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        # å¤„ç†æ—¥æœŸåˆ—
        if "æ—¥æœŸ" in df.columns:
            # å°è¯•å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeç±»å‹
            try:
                # å…ˆç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºå¤„ç†å„ç§å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
                # å°è¯•è½¬æ¢ä¸ºdatetime
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè½¬æ¢
                if pd.api.types.is_datetime64_any_dtype(df["æ—¥æœŸ"]):
                    # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")
                else:
                    logger.warning("æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeå¤±è´¥ï¼Œä¿ç•™åŸå§‹å€¼")
            except Exception as e:
                logger.error(f"æ—¥æœŸåˆ—å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        df = ensure_required_columns(df)
        # å¤„ç†æ•°å€¼åˆ—
        numeric_cols = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æŠ˜æº¢ä»·ç‡"]
        for col in numeric_cols:
            if col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                except Exception as e:
                    logger.error(f"åˆ— {col} è½¬æ¢ä¸ºæ•°å€¼å¤±è´¥: {str(e)}", exc_info=True)
        # è®¡ç®—ç¼ºå¤±åˆ—
        if "æˆäº¤é‡" in df.columns and "æ”¶ç›˜" in df.columns:
            # å¦‚æœæœ‰æˆäº¤é‡å’Œæ”¶ç›˜ä»·ï¼Œå¯ä»¥è®¡ç®—æˆäº¤é¢
            if "æˆäº¤é¢" not in df.columns:
                df["æˆäº¤é¢"] = df["æˆäº¤é‡"] * df["æ”¶ç›˜"]
        # è®¡ç®—æ¶¨è·Œå¹…ç­‰
        if "æ”¶ç›˜" in df.columns:
            if "æ¶¨è·Œå¹…" not in df.columns:
                df["æ¶¨è·Œå¹…"] = df["æ”¶ç›˜"].pct_change() * 100
            if "æ¶¨è·Œé¢" not in df.columns:
                df["æ¶¨è·Œé¢"] = df["æ”¶ç›˜"].diff()
        # å¤„ç†NaNå€¼
        if "æ—¥æœŸ" in df.columns and "æ”¶ç›˜" in df.columns:
            df = df.dropna(subset=["æ—¥æœŸ", "æ”¶ç›˜"])
        return df
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        raise

def limit_to_one_year_data(df: pd.DataFrame, end_date: str) -> pd.DataFrame:
    """
    é™åˆ¶æ•°æ®ä¸ºæœ€è¿‘1å¹´çš„æ•°æ®
    
    Args:
        df: åŸå§‹DataFrame
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        pd.DataFrame: é™åˆ¶ä¸º1å¹´æ•°æ®åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸ
        one_year_ago = (pd.to_datetime(end_date) - timedelta(days=365)).strftime("%Y-%m-%d")
        
        # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            logger.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œæ— æ³•é™åˆ¶ä¸º1å¹´æ•°æ®")
            return df
        
        # åˆ›å»ºDataFrameçš„å‰¯æœ¬ï¼Œé¿å…SettingWithCopyWarning
        df = df.copy(deep=True)
        
        # è½¬æ¢æ—¥æœŸåˆ—
        df.loc[:, "æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è¿‡æ»¤æ•°æ®
        mask = df["æ—¥æœŸ"] >= pd.to_datetime(one_year_ago)
        df = df.loc[mask]
        
        logger.info(f"æ•°æ®å·²é™åˆ¶ä¸ºæœ€è¿‘1å¹´ï¼ˆä» {one_year_ago} è‡³ {end_date}ï¼‰ï¼Œå‰©ä½™ {len(df)} æ¡æ•°æ®")
        return df
    except Exception as e:
        logger.error(f"é™åˆ¶æ•°æ®ä¸º1å¹´æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return df

def try_fund_etf_hist_em_with_net_value(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–åŒ…å«å‡€å€¼çš„æ•°æ®
    Args:
        etf_code: ETFä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_hist_em æ¥å£è·å–ETF {etf_code} æ•°æ®ï¼ˆåŒ…å«å‡€å€¼ï¼‰")
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_hist_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‡€å€¼æ•°æ®ï¼ˆfund_etf_hist_em å¯èƒ½è¿”å›çš„å‡€å€¼åˆ—ï¼‰
            net_value_columns = [col for col in df.columns if "å‡€å€¼" in col or "net" in col.lower()]
            if net_value_columns:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªå‡€å€¼åˆ—
                net_value_col = net_value_columns[0]
                df["å‡€å€¼"] = df[net_value_col]
                logger.info(f"âœ… fund_etf_hist_em æ¥å£æˆåŠŸè·å–å‡€å€¼æ•°æ®ï¼ˆåˆ—å: {net_value_col}ï¼‰")
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_hist_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def try_fund_etf_spot_em_with_premium(etf_code: str) -> pd.DataFrame:
    """
    å°è¯•ä½¿ç”¨ fund_etf_spot_em æ¥å£è·å–åŒ…å«æŠ˜ä»·ç‡çš„æ•°æ®ï¼ˆä»…æœ€æ–°æ•°æ®ï¼‰
    Args:
        etf_code: ETFä»£ç 
    Returns:
        pd.DataFrame: è·å–åˆ°çš„DataFrame
    """
    try:
        logger.debug(f"å°è¯•ä½¿ç”¨ fund_etf_spot_em æ¥å£è·å–ETF {etf_code} æ•°æ®ï¼ˆåŒ…å«æŠ˜ä»·ç‡ï¼‰")
        df = ak.fund_etf_spot_em()
        
        if not df.empty:
            # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š fund_etf_spot_em æ¥å£è¿”å›çš„åŸå§‹åˆ—å: {list(df.columns)}")
            
            # ç‰¹æ®Šå¤„ç†ETF 513750 - ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
            if etf_code == "513750":
                # å°è¯•å¤šç§å¯èƒ½çš„ETFä»£ç æ ¼å¼
                possible_codes = [etf_code, f"SH{etf_code}", f"SZ{etf_code}", f"{etf_code}.SH", f"{etf_code}.SZ"]
                df = df[df["ä»£ç "].isin(possible_codes)]
            else:
                # è¿‡æ»¤æŒ‡å®šETF
                df = df[df["ä»£ç "] == etf_code]
            
            if not df.empty:
                # ä½¿ç”¨å†…éƒ¨åˆ—åæ ‡å‡†åŒ–å‡½æ•°
                df = internal_ensure_chinese_columns(df)
                
                # ç¡®ä¿æ—¥æœŸæ ¼å¼
                if "æ—¥æœŸ" not in df.columns:
                    df["æ—¥æœŸ"] = datetime.now().strftime("%Y-%m-%d")
                
                # ç¡®ä¿æˆäº¤é‡å•ä½ä¸º"æ‰‹"ï¼ˆ1æ‰‹=100è‚¡ï¼‰
                if "æˆäº¤é‡" in df.columns and df["æˆäº¤é‡"].dtype == float:
                    # å¦‚æœæˆäº¤é‡æ˜¯æµ®ç‚¹æ•°ä¸”å°äº1000ï¼Œå¯èƒ½æ˜¯å·²ç»è½¬æ¢ä¸º"æ‰‹"çš„å•ä½
                    if df["æˆäº¤é‡"].max() < 1000:
                        pass  # ä¿æŒåŸæ ·
                    else:
                        df["æˆäº¤é‡"] = df["æˆäº¤é‡"] / 100
                
                logger.info("âœ… fund_etf_spot_em æ¥å£æˆåŠŸè·å–æŠ˜ä»·ç‡æ•°æ®")
        
        return df
    except Exception as e:
        logger.warning(f"fund_etf_spot_em æ¥å£è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()
