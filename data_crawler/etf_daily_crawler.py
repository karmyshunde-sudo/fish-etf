#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - 100%åŒ¹é…è‚¡ç¥¨çˆ¬å–æœºåˆ¶
ã€å…³é”®ä¿®å¤ã€‘
- ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
- æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() ä½†ä¸ç«‹å³æäº¤
- æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import akshare as ak  # ä¿®æ”¹1ï¼šå°†yfinanceæ”¹ä¸ºakshare
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")  # ç›®å½•è·¯å¾„å·²æ­£ç¡®è®¾ç½®
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘ä¸è‚¡ç¥¨çˆ¬å–ä»£ç å®Œå…¨ä¸€è‡´
BATCH_SIZE = 80  # ä¸€ä¸ªæ‰¹æ¬¡å¤„ç†çš„ETFæ•°é‡
COMMIT_BATCH_SIZE = 10  # æ¯COMMIT_BATCH_SIZEä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
BASE_DELAY = 0.8
MAX_RETRIES = 3
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

def get_etf_name(etf_code):
    """è·å–ETFåç§°ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            return etf_code
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        return etf_row["ETFåç§°"].values[0] if not etf_row.empty else etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code

def get_etf_fund_size(etf_code: str) -> float:
    """
    ä»ETFåˆ—è¡¨ä¸­è·å–åŸºé‡‘è§„æ¨¡ï¼ˆåªè¯»ï¼‰
    """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0.0
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "ETFä»£ç " not in basic_info_df.columns or "åŸºé‡‘è§„æ¨¡" not in basic_info_df.columns:
            logger.warning(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼ˆETFä»£ç /åŸºé‡‘è§„æ¨¡ï¼‰")
            return 0.0
        
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        if etf_row.empty:
            logger.warning(f"ETF {etf_code} åœ¨åˆ—è¡¨ä¸­ä¸å­˜åœ¨")
            return 0.0
        
        fund_size = float(etf_row["åŸºé‡‘è§„æ¨¡"].values[0])
        return fund_size * 100000000  # äº¿å…ƒè½¬è‚¡
    
    except Exception as e:
        logger.error(f"è·å–ETF {etf_code} åŸºé‡‘è§„æ¨¡å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_next_crawl_index() -> int:
    """è·å–è¿›åº¦ç´¢å¼•ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return 0
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        return int(basic_info_df["next_crawl_index"].iloc[0])
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """ä¿å­˜è¿›åº¦ï¼ˆä»…æ›´æ–°è¿›åº¦å­—æ®µï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        
        # é€šè¿‡ commit_files_in_batches æäº¤
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(BASIC_INFO_FILE, commit_message)
    except Exception as e:
        logger.error(f"ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def to_naive_datetime(dt):
    """è½¬æ¢ä¸ºæ— æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=None) if dt.tzinfo else dt

def to_aware_datetime(dt):
    """è½¬æ¢ä¸ºåŒ—äº¬æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=Config.BEIJING_TIMEZONE) if dt.tzinfo is None else dt

def to_datetime(date_input):
    """ç»Ÿä¸€æ—¥æœŸè½¬æ¢"""
    if isinstance(date_input, datetime): return date_input
    if isinstance(date_input, str):
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try: return datetime.strptime(date_input, fmt)
            except: continue
    return None

def get_valid_trading_date_range(start_date, end_date):
    """è·å–æœ‰æ•ˆäº¤æ˜“æ—¥èŒƒå›´"""
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    if not start_date or not end_date: return None, None
    
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(get_beijing_time())
    if end_date > now: end_date = now
    
    # æŸ¥æ‰¾æœ‰æ•ˆç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    for _ in range(30):
        if is_trading_day(valid_end_date.date()): break
        valid_end_date -= timedelta(days=1)
    else:
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆèµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    for _ in range(30):
        if is_trading_day(valid_start_date.date()): break
        valid_start_date += timedelta(days=1)
    else:
        valid_start_date = valid_end_date
    
    if to_naive_datetime(valid_start_date) > to_naive_datetime(valid_end_date):
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """åŠ è½½æœ¬åœ°æ•°æ®"""
    try:
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if not os.path.exists(file_path):
            return pd.DataFrame()
        
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float,
                "æŒ¯å¹…": float,
                "æ¶¨è·Œå¹…": float,
                "æ¶¨è·Œé¢": float,
                "æ¢æ‰‹ç‡": float,
                "IOPV": float,
                "æŠ˜ä»·ç‡": float,
                "æº¢ä»·ç‡": float
            }
        )
        
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns):
            return pd.DataFrame()
        
        df = df[[col for col in standard_columns if col in df.columns]]
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        today = datetime.now().strftime("%Y-%m-%d")
        return df[df["æ—¥æœŸ"] <= today]
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
# 1. æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() 
# 2. æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
class RequestThrottler:
    """è¯·æ±‚é™æµå™¨ - åŠ¨æ€è°ƒæ•´è¯·æ±‚é—´éš”"""
    def __init__(self, base_delay=0.8, max_delay=3.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.current_delay = base_delay
        self.success_count = 0
        self.failure_count = 0
        self.last_request_time = None
    
    def wait(self):
        """ç­‰å¾…é€‚å½“æ—¶é—´å†è¯·æ±‚"""
        if self.last_request_time:
            elapsed = time.time() - self.last_request_time
            if elapsed < self.current_delay:
                time.sleep(self.current_delay - elapsed)
        
        self.last_request_time = time.time()    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.success_count += 1
        self.failure_count = 0
        
        if self.success_count % 10 == 0 and self.current_delay > self.base_delay:
            self.current_delay = max(self.base_delay, self.current_delay - 0.1)
    
    def record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self.failure_count += 1
        self.success_count = 0
        
        if self.failure_count >= 3:
            self.current_delay = min(self.max_delay, self.current_delay + 0.5)
            self.failure_count = 0

throttler = RequestThrottler(base_delay=BASE_DELAY)

def apply_precision_control(df: pd.DataFrame) -> pd.DataFrame:
    """
    åº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
    """
    # éœ€è¦ä¿ç•™4ä½å°æ•°çš„å­—æ®µ
    precision_fields = [
        'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é¢',
        'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
        'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡'
    ]
    
    for field in precision_fields:
        if field in df.columns:
            # ä¿ç•™4ä½å°æ•°
            df[field] = df[field].round(4)
    
    # æˆäº¤é‡é€šå¸¸ä¸ºæ•´æ•°
    if 'æˆäº¤é‡' in df.columns:
        df['æˆäº¤é‡'] = df['æˆäº¤é‡'].round(0).astype(int)
    
    return df

def process_akshare_data(df: pd.DataFrame, etf_code: str) -> pd.DataFrame:
    """
    å¤„ç†akshareè¿”å›çš„DataFrame
    """
    # 1. ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
    if "æ—¥æœŸ" in df.columns:
        df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
    
    # 2. æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢']
    for col in required_columns:
        if col not in df.columns:
            logger.error(f"ETF {etf_code} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return pd.DataFrame()  # å…³é”®ä¿®å¤ï¼šç¼ºå¤±å¿…è¦åˆ—ï¼Œç›´æ¥è¿”å›ç©ºDataFrame
    
    # 3. è®¡ç®—è¡ç”Ÿå­—æ®µ
    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / æœ€ä½ * 100%
    if 'æœ€é«˜' in df.columns and 'æœ€ä½' in df.columns:
        df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æœ€ä½'] * 100).round(2)
    else:
        df['æŒ¯å¹…'] = 0.0
    
    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰ä¸€æ—¥æ”¶ç›˜
    if 'æ”¶ç›˜' in df.columns:
        df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'].diff().fillna(0)
    else:
        df['æ¶¨è·Œé¢'] = 0.0
    
    # æ¶¨è·Œå¹… = æ¶¨è·Œé¢ / å‰ä¸€æ—¥æ”¶ç›˜ * 100%
    if 'æ”¶ç›˜' in df.columns:
        prev_close = df['æ”¶ç›˜'].shift(1)
        # é¿å…é™¤ä»¥0
        valid_prev_close = prev_close.replace(0, float('nan'))
        df['æ¶¨è·Œå¹…'] = (df['æ¶¨è·Œé¢'] / valid_prev_close * 100).round(2)
        df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'].fillna(0)
    else:
        df['æ¶¨è·Œå¹…'] = 0.0
    
    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / åŸºé‡‘è§„æ¨¡
    fund_size = get_etf_fund_size(etf_code)
    if fund_size > 0 and 'æˆäº¤é‡' in df.columns:
        df['æ¢æ‰‹ç‡'] = (df['æˆäº¤é‡'] / fund_size * 100).round(2)
    else:
        df['æ¢æ‰‹ç‡'] = 0.0
    
    # 4. IOPV/æŠ˜ä»·ç‡/æº¢ä»·ç‡
    # ä»akshareè·å–æŠ˜ä»·ç‡
    try:
        fund_df = ak.fund_etf_fund_daily_em()
        if not fund_df.empty and "åŸºé‡‘ä»£ç " in fund_df.columns and "æŠ˜ä»·ç‡" in fund_df.columns:
            etf_fund_data = fund_df[fund_df["åŸºé‡‘ä»£ç "] == etf_code]
            if not etf_fund_data.empty:
                df["æŠ˜ä»·ç‡"] = etf_fund_data["æŠ˜ä»·ç‡"].values[0]
    except Exception as e:
        logger.warning(f"è·å–ETF {etf_code} æŠ˜ä»·ç‡æ•°æ®å¤±è´¥: {str(e)}")
    
    # IOPVæ— æ³•ä»akshareè·å–ï¼Œä¿ç•™ä¸º0
    df['IOPV'] = 0.0
    df['æº¢ä»·ç‡'] = 0.0
    
    # å…³é”®ä¿®å¤ï¼šåº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶
    df = apply_precision_control(df)
    
    return df

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨akshareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯")
            return pd.DataFrame()
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç†
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è½¬æ¢ä¸ºakshareæ‰€éœ€çš„æ ¼å¼
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        
        # 1. æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        max_retries = MAX_RETRIES
        for retry in range(max_retries):
            try:
                throttler.wait()
                
                # akshare API
                df = ak.fund_etf_hist_em(
                    symbol=etf_code,
                    period="daily",
                    start_date=start_str,
                    end_date=end_str
                )
                
                # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ•°æ®
                if df is None or df.empty:
                    raise ValueError("No data returned")
                
                throttler.record_success()
                break
                
            except Exception as e:
                throttler.record_failure()
                if retry == max_retries - 1:
                    logger.error(f"ETF {etf_code} æ¥å£è¯·æ±‚å¤±è´¥ (é‡è¯• {max_retries} æ¬¡): {str(e)}")
                    return pd.DataFrame()
                
                wait_time = BASE_DELAY * (2 ** retry) + random.uniform(0.1, 0.5)
                logger.warning(f"ETF {etf_code} è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯•: {str(e)}")
                time.sleep(wait_time)
        
        # 2. å¤„ç†æ•°æ®
        df = process_akshare_data(df, etf_code)
        
        # 3. ä¸¥æ ¼æ•°æ®éªŒè¯
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns) or df.empty:
            logger.error(f"ETF {etf_code} æ•°æ®éªŒè¯å¤±è´¥ - æ— æ³•ä¿å­˜")
            return pd.DataFrame()
        
        # 4. è¡¥å……å¿…è¦å­—æ®µ
        df['ETFä»£ç '] = etf_code
        df['ETFåç§°'] = get_etf_name(etf_code)
        df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 5. ç¡®ä¿å­—æ®µé¡ºåº
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """å¢é‡æ—¥æœŸèŒƒå›´ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        current_time = get_beijing_time()
        if end_date > current_time:
            end_date = current_time
        
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
        
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                if "æ—¥æœŸ" not in df.columns:
                    return last_trading_day - timedelta(days=365), end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    return last_trading_day - timedelta(days=365), end_date
                
                latest_date = valid_dates.max()
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    if start_date > end_date:
                        return None, None
                    return start_date, end_date
                return None, None
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                return last_trading_day - timedelta(days=365), end_date
        else:
            return last_trading_day - timedelta(days=365), end_date
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        return last_trading_day - timedelta(days=365), last_trading_day

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
# 1. æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() ä½†ä¸ç«‹å³æäº¤
# 2. æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """ä¿å­˜æ•°æ®ï¼ˆå…³é”®ä¿®å¤ï¼šä¸è‚¡ç¥¨çˆ¬å–ä»£ç ç›¸åŒï¼‰"""
    if df.empty: 
        logger.error(f"ETF {etf_code} æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        return
    
    os.makedirs(DAILY_DIR, exist_ok=True)
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            # ä¿å­˜æ•°æ®
            df.to_csv(temp_file.name, index=False)
        
        # ç§»åŠ¨æ–‡ä»¶
        shutil.move(temp_file.name, save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
        
        # å…³é”®ä¿®å¤ï¼šè°ƒç”¨ commit_files_in_batchesï¼Œä½†ä¸æ£€æŸ¥è¿”å›å€¼
        commit_message = f"è‡ªåŠ¨æ›´æ–°ETF {etf_code} æ—¥çº¿æ•°æ®"
        commit_files_in_batches(save_path, commit_message)
        logger.debug(f"å·²æ·»åŠ ETF {etf_code} æ—¥çº¿æ•°æ®åˆ°æäº¤é˜Ÿåˆ—")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

def crawl_all_etfs_daily_data() -> None:
    """ä¸»çˆ¬å–é€»è¾‘"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        next_index = get_next_crawl_index()
        
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        actual_end_idx = end_idx % total_count
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
        
        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æœ¬åœ°è®¡æ•°å™¨ï¼Œä¸å†ä¾èµ–å…¨å±€å˜é‡
        processed_count = 0
        for i, etf_code in enumerate(batch_codes):
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            if df.empty:
                logger.error(f"âŒ ETF {etf_code} æ•°æ®è·å–å¤±è´¥ - æ— æ³•ä¿å­˜")
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æ•°æ®éªŒè¯å¤±è´¥\n")
                continue
            
            # ä¿å­˜æ•°æ®ï¼ˆä½†ä¸æäº¤ï¼‰
            save_etf_daily_data(etf_code, df)
            
            processed_count += 1
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            # å…³é”®ä¿®å¤ï¼šæ¯å¤„ç†10ä¸ªETFï¼Œæ‰‹åŠ¨è§¦å‘æäº¤
            if processed_count % COMMIT_BATCH_SIZE == 0:
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæ‰§è¡Œæ‰¹é‡æäº¤...")
                if not force_commit_remaining_files():
                    logger.error("æäº¤æ‰¹é‡æ–‡ä»¶å¤±è´¥")
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶
        logger.info("å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
        if not force_commit_remaining_files():
            logger.error("å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±")
        
        # æ›´æ–°è¿›åº¦
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                if not force_commit_remaining_files():
                    logger.error("å¼·åˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """è·å–ETFä»£ç åˆ—è¡¨ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return []
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶æ ¼å¼é”™è¯¯")
            return []
        
        return basic_info_df["ETFä»£ç "].tolist()
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

if __name__ == "__main__":
    try:
        # é¦–æ¬¡è¿è¡Œæ—¶ç¡®ä¿å®‰è£…ä¾èµ–
        try:
            import akshare
        except ImportError:
            logger.error("ç¼ºå°‘akshareä¾èµ–ï¼Œè¯·å…ˆå®‰è£…: pip install akshare")
            raise SystemExit(1)
        
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
