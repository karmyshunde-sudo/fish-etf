#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨æŒ‡å®šæ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
ã€æœ€ç»ˆä¿®å¤ç‰ˆã€‘
- ç¡®ä¿è¿›åº¦ç´¢å¼•æ€»æ˜¯å‰è¿›ï¼Œå³ä½¿æ²¡æœ‰æ–°æ•°æ®
- æ— è®ºæ˜¯å¦çˆ¬å–åˆ°æ–°æ•°æ®ï¼Œè¿›åº¦æ–‡ä»¶éƒ½ä¼šæ›´æ–°å¹¶æäº¤
- æ­£ç¡®å¤„ç†ç´¢å¼•é‡ç½®é€»è¾‘
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import akshare as ak
import pandas as pd
import logging
import os
import time
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.file_utils import ensure_dir_exists, get_last_crawl_date
from data_crawler.all_etfs import get_all_etf_codes, get_etf_name
from wechat_push.push import send_wechat_message
from utils.git_utils import commit_files_in_batches

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# è¿›åº¦æ–‡ä»¶è·¯å¾„ - ä¸è‚¡ç¥¨æ—¥çº¿çˆ¬å–ç›¸åŒ
PROGRESS_FILE = os.path.join(Config.ETFS_DAILY_DIR, "etf_daily_crawl_progress.txt")

def save_progress(etf_code: str, processed_count: int, total_count: int, next_index: int):
    """
    ä¿å­˜çˆ¬å–è¿›åº¦å¹¶ç¡®ä¿æäº¤åˆ°Git
    Args:
        etf_code: æœ€åæˆåŠŸçˆ¬å–çš„ETFä»£ç 
        processed_count: å·²å¤„ç†ETFæ•°é‡
        total_count: ETFæ€»æ•°
        next_index: ä¸‹æ¬¡åº”å¤„ç†çš„ç´¢å¼•ä½ç½®
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
        
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            f.write(f"last_etf={etf_code}\n")
            f.write(f"processed={processed_count}\n")
            f.write(f"total={total_count}\n")
            f.write(f"next_index={next_index}\n")
            f.write(f"timestamp={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # æäº¤è¿›åº¦æ–‡ä»¶
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(PROGRESS_FILE, commit_message)
        logger.info(f"âœ… è¿›åº¦æ–‡ä»¶å·²æˆåŠŸæäº¤åˆ°ä»“åº“: {PROGRESS_FILE}")
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜å¹¶æäº¤ï¼šå¤„ç†äº† {processed_count}/{total_count} åªETFï¼Œä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def load_progress() -> dict:
    """
    åŠ è½½çˆ¬å–è¿›åº¦
    Returns:
        dict: è¿›åº¦ä¿¡æ¯
    """
    progress = {
        "last_etf": None,
        "processed": 0,
        "total": 0,
        "next_index": 0,
        "timestamp": None
    }
    
    if not os.path.exists(PROGRESS_FILE):
        return progress
    
    try:
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if "=" in line:
                    key, value = line.strip().split("=", 1)
                    if key in progress:
                        if key == "processed" or key == "total" or key == "next_index":
                            try:
                                progress[key] = int(value)
                            except:
                                pass
                        elif key == "timestamp":
                            try:
                                progress[key] = datetime.strptime(value, "%Y-%m-%d %H:%M:%S")
                            except:
                                pass
                        else:
                            progress[key] = value
        logger.info(f"åŠ è½½è¿›åº¦ï¼šå·²å¤„ç† {progress['processed']}/{progress['total']} åªETFï¼Œä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {progress['next_index']}")
        return progress
    except Exception as e:
        logger.error(f"âŒ åŠ è½½è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)
        return progress

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    df = None
    
    try:
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸå‚æ•°æ˜¯datetimeç±»å‹
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºdatetimeç±»å‹")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸå¯¹è±¡æœ‰æ­£ç¡®çš„æ—¶åŒºä¿¡æ¯
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # ç›´æ¥è·å–åŸºç¡€ä»·æ ¼æ•°æ®ï¼ˆæ— é‡è¯•æœºåˆ¶ï¼Œç®€åŒ–é€»è¾‘ï¼‰
        df = ak.fund_etf_hist_em(
            symbol=etf_code,
            period="daily",
            start_date=start_date.strftime("%Y%m%d"),
            end_date=end_date.strftime("%Y%m%d")
        )
        
        # æ£€æŸ¥åŸºç¡€æ•°æ®
        if df is None or df.empty:
            logger.warning(f"ETF {etf_code} åŸºç¡€æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è·å–æŠ˜ä»·ç‡
        try:
            fund_df = ak.fund_etf_fund_daily_em()
            if not fund_df.empty and "åŸºé‡‘ä»£ç " in fund_df.columns and "æŠ˜ä»·ç‡" in fund_df.columns:
                etf_fund_data = fund_df[fund_df["åŸºé‡‘ä»£ç "] == etf_code]
                if not etf_fund_data.empty:
                    # ä»fund_dfæå–æŠ˜ä»·ç‡
                    df["æŠ˜ä»·ç‡"] = etf_fund_data["æŠ˜ä»·ç‡"].values[0]
        except Exception as e:
            logger.warning(f"è·å–ETF {etf_code} æŠ˜ä»·ç‡æ•°æ®å¤±è´¥: {str(e)}")
        
        # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
        df["ETFä»£ç "] = etf_code
        df["ETFåç§°"] = get_etf_name(etf_code)
        df["çˆ¬å–æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ç¡®ä¿åˆ—é¡ºåºä¸ç›®æ ‡ç»“æ„ä¸€è‡´
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'ETFä»£ç ', 'ETFåç§°',
            'çˆ¬å–æ—¶é—´', 'æŠ˜ä»·ç‡'
        ]
        
        # åªä¿ç•™ç›®æ ‡åˆ—
        df = df[[col for col in standard_columns if col in df.columns]]
        
        return df
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_next_trading_day(date_obj: datetime) -> datetime:
    """
    è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
    
    Args:
        date_obj: æ—¥æœŸå¯¹è±¡
    
    Returns:
        datetime: ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
    """
    try:
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåœ¨å†…å­˜ä¸­æ˜¯datetimeç±»å‹
        if not isinstance(date_obj, datetime):
            if isinstance(date_obj, datetime.date):
                date_obj = datetime.combine(date_obj, datetime.min.time())
            else:
                date_obj = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¿¡æ¯
        if date_obj.tzinfo is None:
            date_obj = date_obj.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # å¾ªç¯æŸ¥æ‰¾ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
        next_day = date_obj + timedelta(days=1)
        while not is_trading_day(next_day):
            next_day += timedelta(days=1)
            # é˜²æ­¢æ— é™å¾ªç¯
            if (next_day - date_obj).days > 30:
                logger.warning(f"åœ¨30å¤©å†…æ‰¾ä¸åˆ°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ {next_day} ä½œä¸ºä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥")
                break
        
        return next_day
    
    except Exception as e:
        logger.error(f"è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶è¿”å›æ˜å¤©
        return date_obj + timedelta(days=1)

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """
    è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´
    è¿”å›ï¼š(start_date, end_date)
    
    é‡ç‚¹ï¼šä»æ•°æ®æ–‡ä»¶çš„"æ—¥æœŸ"åˆ—è·å–æœ€æ–°æ—¥æœŸï¼Œè€Œä¸æ˜¯æœ€åçˆ¬å–æ—¥æœŸ
    """
    try:
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåœ¨å†…å­˜ä¸­æ˜¯datetimeç±»å‹
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            if isinstance(last_trading_day, datetime.date):
                last_trading_day = datetime.combine(last_trading_day, datetime.min.time())
            else:
                last_trading_day = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¿¡æ¯
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day
        
        # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
        current_time = get_beijing_time()
        # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡éƒ½æœ‰æ—¶åŒºä¿¡æ¯
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if current_time.tzinfo is None:
            current_time = current_time.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        if end_date > current_time:
            logger.warning(f"ç»“æŸæ—¥æœŸ {end_date} æ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´")
            end_date = current_time
        
        save_path = os.path.join(Config.ETFS_DAILY_DIR, f"{etf_code}.csv")
        
        # å¦‚æœæ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œè·å–æ•°æ®æ–‡ä»¶ä¸­çš„æœ€æ–°æ—¥æœŸ
        if os.path.exists(save_path):
            try:
                # è¯»å–æ•°æ®æ–‡ä»¶
                df = pd.read_csv(save_path)
                
                # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                
                # ç¡®ä¿"æ—¥æœŸ"åˆ—å­˜åœ¨
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    return None, None
                
                # è·å–æœ€æ–°æ—¥æœŸ
                latest_date = df["æ—¥æœŸ"].max()
                if pd.isna(latest_date):
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶æ—¥æœŸåˆ—ä¸ºç©º")
                    return None, None
                
                # ç¡®ä¿æ˜¯datetimeç±»å‹
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                # ç¡®ä¿æ—¶åŒºä¿¡æ¯
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                # ä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¼€å§‹
                # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåœ¨å†…å­˜ä¸­ä¿æŒä¸ºdatetimeç±»å‹
                next_trading_day = get_next_trading_day(latest_date)
                
                start_date = next_trading_day
                
                # ç¡®ä¿æ—¥æœŸæ¯”è¾ƒåŸºäºç›¸åŒç±»å‹
                # å¦‚æœèµ·å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸï¼Œè¯´æ˜æ•°æ®å·²ç»æ˜¯æœ€æ–°
                if start_date >= end_date:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œæ— éœ€çˆ¬å–")
                    return None, None
                
                # ç¡®ä¿ä¸è¶…è¿‡ä¸€å¹´
                one_year_ago = last_trading_day - timedelta(days=365)
                if one_year_ago.tzinfo is None:
                    one_year_ago = one_year_ago.replace(tzinfo=Config.BEIJING_TIMEZONE)
                if start_date < one_year_ago:
                    logger.info(f"ETF {etf_code} çˆ¬å–æ—¥æœŸå·²è¶…è¿‡ä¸€å¹´ï¼Œä»{one_year_ago}å¼€å§‹")
                    start_date = one_year_ago
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # å‡ºé”™æ—¶ä½¿ç”¨å…¨é‡çˆ¬å–ä¸€å¹´æ•°æ®
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        else:
            # é¦–æ¬¡çˆ¬å–ï¼Œè·å–ä¸€å¹´æ•°æ®
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # ç¡®ä¿è¿”å›çš„æ—¥æœŸå¯¹è±¡éƒ½æœ‰æ—¶åŒºä¿¡æ¯
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        logger.info(f"ETF {etf_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date} è‡³ {end_date}")
        return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶ä½¿ç”¨å…¨é‡çˆ¬å–ä¸€å¹´æ•°æ®
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            if isinstance(last_trading_day, datetime.date):
                last_trading_day = datetime.combine(last_trading_day, datetime.min.time())
            else:
                last_trading_day = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¿¡æ¯
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        return start_date, end_date

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """
    ä¿å­˜ETFæ—¥çº¿æ•°æ®
    """
    if df.empty:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    etf_daily_dir = Config.ETFS_DAILY_DIR
    ensure_dir_exists(etf_daily_dir)
    
    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ä¿å­˜å‰å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if "æ—¥æœŸ" in df.columns:
        df_save = df.copy()
        df_save["æ—¥æœŸ"] = df_save["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
    else:
        df_save = df
    
    # ä¿å­˜åˆ°CSV
    save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
    try:
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
        df_save.to_csv(temp_file.name, index=False)
        # åŸå­æ›¿æ¢
        shutil.move(temp_file.name, save_path)
        
        # ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨gitå·¥å…·æ¨¡å—æäº¤å˜æ›´
        commit_files_in_batches(save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
    finally:
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

def crawl_all_etfs_daily_data() -> None:
    """
    çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ®
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        Config.init_dirs()
        etf_daily_dir = Config.ETFS_DAILY_DIR
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
        
        # è·å–æ‰€æœ‰ETFä»£ç 
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        # åŠ è½½è¿›åº¦
        progress = load_progress()
        next_index = progress["next_index"]
        
        # ç¡®å®šå¤„ç†èŒƒå›´
        batch_size = 100
        start_idx = next_index
        end_idx = min(start_idx + batch_size, len(etf_codes))
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if next_index >= total_count:
            logger.warning(f"æ£€æµ‹åˆ°ç´¢å¼• {next_index} è¶…è¿‡æ€»æ•° {total_count}ï¼Œå·²é‡ç½®ä¸º0")
            next_index = 0
            start_idx = 0
            end_idx = min(start_idx + batch_size, total_count)
        
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({end_idx - start_idx}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"è¿›åº¦è®°å½•ä¸­å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–è¿›åº¦è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                completed_codes = set()
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        processed_count = 0
        last_processed_code = None
        for i in range(start_idx, end_idx):
            etf_code = etf_codes[i]
            etf_name = get_etf_name(etf_code)
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if df.empty:
                logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                # è®°å½•å¤±è´¥æ—¥å¿—
                with open(os.path.join(etf_daily_dir, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                continue
            
            # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
            save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                    if "æ—¥æœŸ" in existing_df.columns:
                        existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors='coerce')
                    
                    # åˆå¹¶æ•°æ®å¹¶å»é‡
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                    combined_df.to_csv(temp_file.name, index=False)
                    # åŸå­æ›¿æ¢
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            else:
                # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                try:
                    df.to_csv(temp_file.name, index=False)
                    # åŸå­æ›¿æ¢
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            
            # æ ‡è®°ä¸ºå·²å®Œæˆ
            with open(completed_file, "a", encoding="utf-8") as f:
                f.write(f"{etf_code}\n")
            
            # æ¯10åªETFæäº¤ä¸€æ¬¡
            processed_count += 1
            if processed_count % 10 == 0 or processed_count == (end_idx - start_idx):
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæ‰§è¡Œæäº¤æ“ä½œ...")
                try:
                    from utils.git_utils import commit_final
                    commit_final()
                    logger.info(f"å·²æäº¤å‰ {processed_count} åªETFçš„æ•°æ®åˆ°ä»“åº“")
                except Exception as e:
                    logger.error(f"æäº¤æ–‡ä»¶æ—¶å‡ºé”™ï¼Œç»§ç»­æ‰§è¡Œ: {str(e)}")
            
            # æ›´æ–°è¿›åº¦
            last_processed_code = etf_code
            save_progress(etf_code, start_idx + processed_count, total_count, i + 1)
            
            # è®°å½•è¿›åº¦
            logger.info(f"è¿›åº¦: {start_idx + processed_count}/{total_count} ({(start_idx + processed_count)/total_count*100:.1f}%)")
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿›åº¦ç´¢å¼•æ€»æ˜¯å‰è¿›
        # æ— è®ºæ˜¯å¦å¤„ç†äº†ETFï¼Œéƒ½æ›´æ–°è¿›åº¦ç´¢å¼•
        if processed_count == 0:
            logger.info("æœ¬æ‰¹æ¬¡æ— æ–°æ•°æ®éœ€è¦çˆ¬å–")
            # å¼ºåˆ¶æ›´æ–°è¿›åº¦ç´¢å¼•
            new_index = end_idx
            # å¦‚æœåˆ°è¾¾æ€»æ•°ï¼Œé‡ç½®ä¸º0
            if new_index >= total_count:
                new_index = 0
            # ä¿å­˜è¿›åº¦
            save_progress(last_processed_code, start_idx + processed_count, total_count, new_index)
            logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        else:
            # å·²ç»åœ¨å¾ªç¯ä¸­æ›´æ–°äº†è¿›åº¦
            pass
        
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²æäº¤
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETF")
        logger.info("ç¨‹åºå°†é€€å‡ºï¼Œç­‰å¾…å·¥ä½œæµå†æ¬¡è°ƒç”¨")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # ä¿å­˜è¿›åº¦ï¼ˆå¦‚æœå¤±è´¥ï¼‰
        try:
            save_progress(None, next_index, total_count, next_index)
        except:
            pass
        raise

def get_all_etf_codes() -> list:
    """
    è·å–æ‰€æœ‰ETFä»£ç 
    """
    try:
        etf_list_file = os.path.join(Config.DATA_DIR, "all_etfs.csv")
        if not os.path.exists(etf_list_file):
            logger.info("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            from data_crawler.all_etfs import update_all_etf_list
            update_all_etf_list()
        
        etf_list = pd.read_csv(etf_list_file)
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿ETFä»£ç æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if "ETFä»£ç " in etf_list.columns:
            etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str)
        return etf_list["ETFä»£ç "].tolist()
    
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

def get_next_trading_day(date_obj: datetime) -> datetime:
    """
    è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
    
    Args:
        date_obj: æ—¥æœŸå¯¹è±¡
    
    Returns:
        datetime: ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
    """
    try:
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåœ¨å†…å­˜ä¸­æ˜¯datetimeç±»å‹
        if not isinstance(date_obj, datetime):
            if isinstance(date_obj, datetime.date):
                date_obj = datetime.combine(date_obj, datetime.min.time())
            else:
                date_obj = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¿¡æ¯
        if date_obj.tzinfo is None:
            date_obj = date_obj.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # å¾ªç¯æŸ¥æ‰¾ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
        next_day = date_obj + timedelta(days=1)
        while not is_trading_day(next_day):
            next_day += timedelta(days=1)
            # é˜²æ­¢æ— é™å¾ªç¯
            if (next_day - date_obj).days > 30:
                logger.warning(f"åœ¨30å¤©å†…æ‰¾ä¸åˆ°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ {next_day} ä½œä¸ºä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥")
                break
        
        return next_day
    
    except Exception as e:
        logger.error(f"è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶è¿”å›æ˜å¤©
        return date_obj + timedelta(days=1)

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    finally:
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²ä¿å­˜
        try:
            progress = load_progress()
            logger.info(f"å½“å‰è¿›åº¦: {progress['next_index']}/{progress['total']}")
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
