#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨æŒ‡å®šæ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
ã€yFinanceæ•°æ®DS-etf_daily_crawler-3.pyã€‘
ã€ç”Ÿäº§çº§å®ç°ã€‘
- ä¸¥æ ¼éµå¾ª"å„å¸å…¶èŒ"åŸåˆ™
- ä¸è‚¡ç¥¨çˆ¬å–ç³»ç»Ÿå®Œå…¨ä¸€è‡´çš„è¿›åº¦ç®¡ç†é€»è¾‘
- ä¸“ä¸šé‡‘èç³»ç»Ÿå¯é æ€§ä¿éšœ
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘å¯åœ¨æ­¤å¤„ä¿®æ”¹æ¯æ¬¡å¤„ç†çš„ETFæ•°é‡
BATCH_SIZE = 40  # å¯æ ¹æ®éœ€è¦è°ƒæ•´ä¸º60ã€100ã€150ã€200ç­‰
COMMIT_BATCH_SIZE = 10  # æ¯COMMIT_BATCH_SIZEä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
BASE_DELAY = 0.8
MAX_RETRIES = 3
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

class RequestThrottler:
    """è¯·æ±‚é™æµå™¨ - åŠ¨æ€è°ƒæ•´è¯·æ±‚é—´éš”"""
    def __init__(self, base_delay=0.8, max_delay=3.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.current_delay = base_delay
        self.success_count = 0
        self.failure_count = 0
        self.last_request_time = None
    
    def wait(self):
        """ç­‰å¾…é€‚å½“æ—¶é—´å†è¯·æ±‚"""
        if self.last_request_time:
            elapsed = time.time() - self.last_request_time
            if elapsed < self.current_delay:
                time.sleep(self.current_delay - elapsed)
        
        self.last_request_time = time.time()
    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.success_count += 1
        self.failure_count = 0
        
        if self.success_count % 10 == 0 and self.current_delay > self.base_delay:
            self.current_delay = max(self.base_delay, self.current_delay - 0.1)
    
    def record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self.failure_count += 1
        self.success_count = 0
        
        if self.failure_count >= 3:
            self.current_delay = min(self.max_delay, self.current_delay + 0.5)
            self.failure_count = 0

throttler = RequestThrottler(base_delay=BASE_DELAY)

def get_etf_name(etf_code):
    """
    è·å–ETFåç§°
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return etf_code
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        if "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            return etf_code
        
        # ç¡®ä¿æ¯”è¾ƒæ—¶æ•°æ®ç±»å‹ä¸€è‡´ï¼ˆéƒ½è½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
        etf_code_str = str(etf_code).strip()
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == etf_code_str]
        
        if not etf_row.empty:
            return etf_row["ETFåç§°"].values[0]
        
        logger.warning(f"ETF {etf_code_str} ä¸åœ¨åˆ—è¡¨ä¸­")
        return etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code

def get_etf_fund_size(etf_code: str) -> float:
    """
    ä»ETFåˆ—è¡¨ä¸­è·å–åŸºé‡‘è§„æ¨¡ï¼ˆåªè¯»ï¼‰
    """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0.0
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "ETFä»£ç " not in basic_info_df.columns or "åŸºé‡‘è§„æ¨¡" not in basic_info_df.columns:
            logger.warning(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼ˆETFä»£ç /åŸºé‡‘è§„æ¨¡ï¼‰")
            return 0.0
        
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        if etf_row.empty:
            logger.warning(f"ETF {etf_code} åœ¨åˆ—è¡¨ä¸­ä¸å­˜åœ¨")
            return 0.0
        
        fund_size = float(etf_row["åŸºé‡‘è§„æ¨¡"].values[0])
        return fund_size * 100000000  # äº¿å…ƒè½¬è‚¡
    
    except Exception as e:
        logger.error(f"è·å–ETF {etf_code} åŸºé‡‘è§„æ¨¡å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_next_crawl_index() -> int:
    """
    è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    Returns:
        int: ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0
        
        # ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°åï¼ˆæ·»åŠ ä¸‹åˆ’çº¿ï¼‰
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½")
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•è·å–è¿›åº¦")
            return 0
        
        # ç¡®ä¿"next_crawl_index"åˆ—å­˜åœ¨
        if "next_crawl_index" not in basic_info_df.columns:
            # æ·»åŠ åˆ—å¹¶åˆå§‹åŒ–
            basic_info_df["next_crawl_index"] = 0
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            if not _verify_git_file_content(BASIC_INFO_FILE):
                logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°æäº¤")
            logger.info("å·²æ·»åŠ next_crawl_indexåˆ—å¹¶åˆå§‹åŒ–ä¸º0")
        
        # è·å–ç¬¬ä¸€ä¸ªETFçš„next_crawl_indexå€¼
        next_index = int(basic_info_df["next_crawl_index"].iloc[0])
        logger.info(f"å½“å‰è¿›åº¦ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
        return next_index
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """
    ä¿å­˜ETFçˆ¬å–è¿›åº¦
    Args:
        next_index: ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
            return
        
        # ç¡®ä¿"next_crawl_index"åˆ—å­˜åœ¨
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        
        # æ›´æ–°æ‰€æœ‰è¡Œçš„next_crawl_indexå€¼
        basic_info_df["next_crawl_index"] = next_index
        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡è¯•æäº¤")
        
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
        # ã€å…³é”®ä¿®å¤ã€‘è¿›åº¦æ–‡ä»¶ä¸å•ç‹¬æäº¤ï¼Œç­‰å¾…æ‰¹é‡æäº¤
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def to_naive_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºnaive datetimeï¼ˆæ— æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: naive datetime
    """
    if dt is None:
        return None
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

def to_aware_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºaware datetimeï¼ˆæœ‰æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: aware datetimeï¼ˆåŒ—äº¬æ—¶åŒºï¼‰
    """
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=Config.BEIJING_TIMEZONE)
    return dt

def to_datetime(date_input):
    """
    ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    Args:
        date_input: æ—¥æœŸè¾“å…¥ï¼Œå¯ä»¥æ˜¯strã€dateã€datetimeç­‰ç±»å‹
    Returns:
        datetime.datetime: ç»Ÿä¸€çš„datetimeç±»å‹
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try:
                return datetime.strptime(date_input, fmt)
            except:
                continue
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_input}")
        return None
    return None

def get_valid_trading_date_range(start_date, end_date):
    """
    è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´ï¼Œç¡®ä¿åªåŒ…å«å†å²äº¤æ˜“æ—¥
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
    
    Returns:
        tuple: (valid_start_date, valid_end_date) - æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´
    """
    # ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    
    if start_date is None or end_date is None:
        logger.error("æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
        return None, None
    
    # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
    now = get_beijing_time()
    # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡ç±»å‹ä¸€è‡´
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(now)
    
    if end_date > now:
        end_date = now
        logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    days_back = 0
    while days_back < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_end_date.date()):
            break
        valid_end_date -= timedelta(days=1)
        days_back += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼Œè¿”å›ç©ºèŒƒå›´
    if days_back >= 30:
        logger.warning(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼ˆä» {end_date.strftime('%Y-%m-%d')} å¼€å§‹ï¼‰")
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    days_forward = 0
    while days_forward < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_start_date.date()):
            break
        valid_start_date += timedelta(days=1)
        days_forward += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ç»“æŸäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹æ—¥
    if days_forward >= 30:
        valid_start_date = valid_end_date
    
    # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰ç±»å‹ä¸€è‡´
    start_naive = to_naive_datetime(valid_start_date)
    end_naive = to_naive_datetime(valid_end_date)
    
    if start_naive > end_naive:
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """
    åŠ è½½ETFæ—¥çº¿æ•°æ®
    """
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.warning(f"ETF {etf_code} æ—¥çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        # æ ‡å‡†åˆ—å®šä¹‰
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        # è¯»å–CSVæ–‡ä»¶ï¼Œæ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float,
                "æŒ¯å¹…": float,
                "æ¶¨è·Œå¹…": float,
                "æ¶¨è·Œé¢": float,
                "æ¢æ‰‹ç‡": float,
                "IOPV": float,
                "æŠ˜ä»·ç‡": float,
                "æº¢ä»·ç‡": float
            }
        )
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        # æŒ‰æ—¥æœŸæ’åºå¹¶å»é‡
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        # ç§»é™¤æœªæ¥æ—¥æœŸçš„æ•°æ®
        today = datetime.now().strftime("%Y-%m-%d")
        df = df[df["æ—¥æœŸ"] <= today]
        
        # åªè¿”å›æ ‡å‡†åˆ—ä¸­å­˜åœ¨çš„åˆ—
        return df[[col for col in standard_columns if col in df.columns]]
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def apply_precision_control(df: pd.DataFrame) -> pd.DataFrame:
    """
    åº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
    """
    # éœ€è¦ä¿ç•™4ä½å°æ•°çš„å­—æ®µ
    precision_fields = [
        'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é¢',
        'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
        'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡'
    ]
    
    for field in precision_fields:
        if field in df.columns:
            # ä¿ç•™4ä½å°æ•°
            df[field] = df[field].round(4)
    
    # æˆäº¤é‡é€šå¸¸ä¸ºæ•´æ•°
    if 'æˆäº¤é‡' in df.columns:
        df['æˆäº¤é‡'] = df['æˆäº¤é‡'].round(0).astype(int)
    
    return df

def process_yfinance_data(df: pd.DataFrame, etf_code: str) -> pd.DataFrame:
    """
    å¤„ç†Yahoo Financeè¿”å›çš„DataFrame
    """
    # 1. ç¡®ä¿DataFrameæ˜¯æ‰å¹³ç»“æ„
    if isinstance(df.columns, pd.MultiIndex):
        # æå–ç¬¬ä¸€çº§åˆ—å
        columns = []
        for col in df.columns:
            if isinstance(col, tuple) and len(col) > 0:
                columns.append(col[0])
            else:
                columns.append(col)
        df.columns = columns
    
    # 2. ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
    if 'Date' in df.columns:
        df = df.reset_index(drop=True)
    elif df.index.name == 'Date':
        df = df.reset_index()
    elif 'date' in df.columns:
        df = df.rename(columns={'date': 'Date'})
    else:
        return pd.DataFrame()  # æ— æœ‰æ•ˆæ—¥æœŸåˆ—ï¼Œè¿”å›ç©ºDataFrame
    
    # 3. æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    for col in required_columns:
        if col not in df.columns:
            logger.error(f"ETF {etf_code} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return pd.DataFrame()  # å…³é”®ä¿®å¤ï¼šç¼ºå¤±å¿…è¦åˆ—ï¼Œç›´æ¥è¿”å›ç©ºDataFrame
    
    # 4. åˆ›å»ºä¸´æ—¶å•åˆ—DataFrame
    result_df = pd.DataFrame()
    result_df['æ—¥æœŸ'] = df['Date'].dt.strftime('%Y-%m-%d')
    result_df['å¼€ç›˜'] = df['Open'].astype(float)
    result_df['æœ€é«˜'] = df['High'].astype(float)
    result_df['æœ€ä½'] = df['Low'].astype(float)
    result_df['æ”¶ç›˜'] = df['Close'].astype(float)
    result_df['æˆäº¤é‡'] = df['Volume'].astype(float)
    
    # 5. è®¡ç®—è¡ç”Ÿå­—æ®µ
    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / æœ€ä½ * 100%
    result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['æœ€ä½'] * 100).round(2)
    
    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰ä¸€æ—¥æ”¶ç›˜
    result_df['æ¶¨è·Œé¢'] = result_df['æ”¶ç›˜'].diff().fillna(0)
    
    # æ¶¨è·Œå¹… = æ¶¨è·Œé¢ / å‰ä¸€æ—¥æ”¶ç›˜ * 100%
    prev_close = result_df['æ”¶ç›˜'].shift(1)
    # é¿å…é™¤ä»¥0
    valid_prev_close = prev_close.replace(0, float('nan'))
    result_df['æ¶¨è·Œå¹…'] = (result_df['æ¶¨è·Œé¢'] / valid_prev_close * 100).round(2)
    result_df['æ¶¨è·Œå¹…'] = result_df['æ¶¨è·Œå¹…'].fillna(0)
    
    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / åŸºé‡‘è§„æ¨¡
    fund_size = get_etf_fund_size(etf_code)
    if fund_size > 0:
        result_df['æ¢æ‰‹ç‡'] = (result_df['æˆäº¤é‡'] / fund_size * 100).round(2)
    else:
        result_df['æ¢æ‰‹ç‡'] = 0.0
    
    # 6. IOPV/æŠ˜ä»·ç‡/æº¢ä»·ç‡ï¼ˆYahoo Financeä¸æä¾›ï¼‰
    result_df['IOPV'] = 0.0
    result_df['æŠ˜ä»·ç‡'] = 0.0
    result_df['æº¢ä»·ç‡'] = 0.0
    
    # 7. æˆäº¤é¢ = æ”¶ç›˜ * æˆäº¤é‡
    result_df['æˆäº¤é¢'] = (result_df['æ”¶ç›˜'] * result_df['æˆäº¤é‡']).round(2)
    
    # å…³é”®ä¿®å¤ï¼šåº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶
    result_df = apply_precision_control(result_df)
    
    return result_df

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨Yahoo Financeçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # ç¡®ä¿æ—¥æœŸå‚æ•°æ˜¯datetimeç±»å‹
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºdatetimeç±»å‹")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸå¯¹è±¡æœ‰æ­£ç¡®çš„æ—¶åŒºä¿¡æ¯
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è½¬æ¢ä¸ºYahoo Financeæ‰€éœ€çš„æ ¼å¼
        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")
        
        # 1. æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        max_retries = MAX_RETRIES
        for retry in range(max_retries):
            try:
                throttler.wait()
                
                # Yahoo Finance API
                symbol = etf_code
                if etf_code.startswith(('51', '56', '57', '58')):
                    symbol = f"{etf_code}.SS"
                elif etf_code.startswith('15'):
                    symbol = f"{etf_code}.SZ"
                
                # è·å–æ•°æ®
                df = yf.download(
                    symbol,
                    start=start_str,
                    end=end_str,
                    progress=False,
                    auto_adjust=True,
                    timeout=15
                )
                
                # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ•°æ®
                if df is None or df.empty:
                    raise ValueError("No data returned")
                
                throttler.record_success()
                break
                
            except Exception as e:
                throttler.record_failure()
                if retry == max_retries - 1:
                    logger.error(f"ETF {etf_code} æ¥å£è¯·æ±‚å¤±è´¥ (é‡è¯• {max_retries} æ¬¡): {str(e)}")
                    return pd.DataFrame()
                
                wait_time = BASE_DELAY * (2 ** retry) + random.uniform(0.1, 0.5)
                logger.warning(f"ETF {etf_code} è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯•: {str(e)}")
                time.sleep(wait_time)
        
        # 2. å¤„ç†æ•°æ®
        df = process_yfinance_data(df, etf_code)
        
        # 3. ä¸¥æ ¼æ•°æ®éªŒè¯
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns) or df.empty:
            logger.error(f"ETF {etf_code} æ•°æ®éªŒè¯å¤±è´¥ - æ— æ³•ä¿å­˜")
            return pd.DataFrame()
        
        # 4. è¡¥å……å¿…è¦å­—æ®µ
        df['ETFä»£ç '] = etf_code
        df['ETFåç§°'] = get_etf_name(etf_code)
        df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 5. ç¡®ä¿å­—æ®µé¡ºåº
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """
    è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´
    ä¸“ä¸šä¿®å¤ï¼šè§£å†³ETFå…¨éƒ¨è·³è¿‡é—®é¢˜
    """
    try:
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¸€è‡´
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è®¾ç½®ç»“æŸæ—¥æœŸä¸ºæœ€è¿‘äº¤æ˜“æ—¥ï¼ˆç¡®ä¿æ˜¯äº¤æ˜“æ—¥ï¼‰
        end_date = last_trading_day
        
        # è·å–å½“å‰åŒ—äº¬æ—¶é—´
        current_time = get_beijing_time()
        
        # å¦‚æœç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œè°ƒæ•´ä¸ºå½“å‰æ—¶é—´
        if end_date > current_time:
            end_date = current_time
        
        # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿ç»“æŸæ—¥æœŸæ˜¯äº¤æ˜“æ—¥
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
            if (last_trading_day - end_date).days > 30:
                logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥")
                return None, None
        
        # ä¸“ä¸šä¿®å¤ï¼šè®¾ç½®ç»“æŸæ—¶é—´ä¸ºå½“å¤©23:59:59
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        # æ„å»ºETFæ•°æ®æ–‡ä»¶è·¯å¾„
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        # æ£€æŸ¥å†å²æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                
                # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    # ä½¿ç”¨é»˜è®¤å›é€€ç­–ç•¥ï¼šè·å–ä¸€å¹´æ•°æ®
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                
                # è·å–æœ€æ–°æœ‰æ•ˆæ—¥æœŸ
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ä¸­æ—¥æœŸåˆ—å…¨ä¸ºNaN")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                
                # ç¡®ä¿latest_dateæ˜¯datetimeç±»å‹å¹¶å¸¦æœ‰æ—¶åŒº
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                # ä¸“ä¸šä¿®å¤ï¼šæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†ï¼ˆå¿½ç•¥æ—¶é—´éƒ¨åˆ†ï¼‰
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                logger.debug(f"ETF {etf_code} æ—¥æœŸæ¯”è¾ƒ: æœ€æ–°æ—¥æœŸ={latest_date_date}, ç»“æŸæ—¥æœŸ={end_date_date}")
                
                # ä¸“ä¸šä¿®å¤ï¼šå¦‚æœæœ€æ–°æ—¥æœŸå°äºç»“æŸæ—¥æœŸï¼Œåˆ™éœ€è¦çˆ¬å–
                if latest_date_date < end_date_date:
                    # ä¸“ä¸šä¿®å¤ï¼šä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¼€å§‹
                    start_date = latest_date + timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæ˜¯äº¤æ˜“æ—¥
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæœ‰æ—¶åŒºä¿¡æ¯
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    
                    # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿start_dateä¸è¶…è¿‡end_date
                    if start_date > end_date:
                        logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ={latest_date_date}ï¼Œç»“æŸæ—¥æœŸ={end_date_date}ï¼‰")
                        return None, None
                    
                    logger.info(f"ETF {etf_code} éœ€è¦æ›´æ–°æ•°æ®: æœ€æ–°æ—¥æœŸ {latest_date_date} < ç»“æŸæ—¥æœŸ {end_date_date}")
                    logger.info(f"ETF {etf_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                    return start_date, end_date
                else:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°: æœ€æ–°æ—¥æœŸ {latest_date_date} >= ç»“æŸæ—¥æœŸ {end_date_date}")
                    return None, None
            
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # å‡ºé”™æ—¶å°è¯•è·å–ä¸€å¹´æ•°æ®
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                return start_date, end_date
        else:
            logger.info(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œå°†è·å–ä¸€å¹´å†å²æ•°æ®")
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
            return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """
    ä¿å­˜ETFæ—¥çº¿æ•°æ® - ã€å…³é”®ä¿®å¤ã€‘åªä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸è¿›è¡Œä»»ä½•Gitæäº¤æ“ä½œ
    """
    if df.empty:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(DAILY_DIR, exist_ok=True)
    
    # ä¿å­˜åˆ°CSV - æ—¥æœŸåˆ—å·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸éœ€è¦è½¬æ¢
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
    temp_file = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            df.to_csv(temp_file.name, index=False)
        shutil.move(temp_file.name, save_path)
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
        
        # ã€å…³é”®ä¿®å¤ã€‘ç§»é™¤æ‰€æœ‰Gitæäº¤æ“ä½œï¼Œåªåœ¨æ‰¹é‡å¤„ç†æ—¶æäº¤
        logger.debug(f"ETF {etf_code} æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°ï¼Œç­‰å¾…æ‰¹é‡æäº¤")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

def crawl_all_etfs_daily_data() -> None:
    """
    çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ®
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        # è·å–æ‰€æœ‰ETFä»£ç 
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        # è·å–å½“å‰è¿›åº¦
        next_index = get_next_crawl_index()
        
        # ä¸“ä¸šä¿®å¤ï¼šå¾ªç¯æ‰¹å¤„ç†æœºåˆ¶
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        actual_end_idx = end_idx % total_count
        
        # å¤„ç†å¾ªç¯æƒ…å†µ
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")

        # è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹ETF
        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        # ã€å…³é”®ä¿®å¤ã€‘åˆå§‹åŒ–è®¡æ•°å™¨åœ¨å¾ªç¯å¤–éƒ¨
        processed_count = 0
        successful_count = 0
        skipped_count = 0
        failed_count = 0
        
        for i, etf_code in enumerate(batch_codes):
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(5, 11))
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                # ã€å…³é”®ä¿®å¤ã€‘å³ä½¿è·³è¿‡ä¹Ÿè¦è®¡æ•°
                processed_count += 1
                skipped_count += 1
                current_index = (start_idx + i) % total_count
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æäº¤
                if processed_count % COMMIT_BATCH_SIZE == 0:
                    logger.info(f"ã€æ‰¹é‡æäº¤ã€‘å·²å¤„ç† {processed_count} åªETFï¼ˆæˆåŠŸ {successful_count}ï¼Œè·³è¿‡ {skipped_count}ï¼Œå¤±è´¥ {failed_count}ï¼‰ï¼Œæäº¤æ‰¹é‡æ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“...")
                    commit_result = force_commit_remaining_files()
                    if commit_result:
                        logger.info(f"âœ… æ‰¹é‡æäº¤æˆåŠŸï¼å·²æäº¤ {processed_count} ä¸ªæ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“")
                    else:
                        logger.error("âŒ æ‰¹é‡æäº¤å¤±è´¥ï¼")
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                # è®°å½•å¤±è´¥æ—¥å¿—
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                
                # ã€å…³é”®ä¿®å¤ã€‘å³ä½¿å¤±è´¥ä¹Ÿè¦è®¡æ•°
                processed_count += 1
                failed_count += 1
                current_index = (start_idx + i) % total_count
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æäº¤
                if processed_count % COMMIT_BATCH_SIZE == 0:
                    logger.info(f"ã€æ‰¹é‡æäº¤ã€‘å·²å¤„ç† {processed_count} åªETFï¼ˆæˆåŠŸ {successful_count}ï¼Œè·³è¿‡ {skipped_count}ï¼Œå¤±è´¥ {failed_count}ï¼‰ï¼Œæäº¤æ‰¹é‡æ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“...")
                    commit_result = force_commit_remaining_files()
                    if commit_result:
                        logger.info(f"âœ… æ‰¹é‡æäº¤æˆåŠŸï¼å·²æäº¤ {processed_count} ä¸ªæ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“")
                    else:
                        logger.error("âŒ æ‰¹é‡æäº¤å¤±è´¥ï¼")
                continue
            
            # å¤„ç†å·²æœ‰æ•°æ®
            save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    if "æ—¥æœŸ" in existing_df.columns:
                        existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors='coerce')
                    
                    # ç¡®ä¿æ–°æ•°æ®çš„æ—¥æœŸåˆ—ä¹Ÿæ˜¯datetimeç±»å‹ä»¥ä¾¿åˆå¹¶
                    df_temp = df.copy()
                    df_temp["æ—¥æœŸ"] = pd.to_datetime(df_temp["æ—¥æœŸ"], errors='coerce')
                    
                    combined_df = pd.concat([existing_df, df_temp], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼ä¿å­˜
                    combined_df["æ—¥æœŸ"] = combined_df["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
                    
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                        combined_df.to_csv(temp_file.name, index=False)
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                except Exception as e:
                    logger.error(f"åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")
                    continue
                finally:
                    if 'temp_file' in locals() and os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            else:
                save_etf_daily_data(etf_code, df)
            
            # ã€å…³é”®ä¿®å¤ã€‘æˆåŠŸå¤„ç†è®¡æ•°
            processed_count += 1
            successful_count += 1
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            # ã€å…³é”®ä¿®å¤ã€‘æ¯å¤„ç†10åªETFå°±è°ƒç”¨git_utilsæäº¤
            if processed_count % COMMIT_BATCH_SIZE == 0:
                logger.info(f"ã€æ‰¹é‡æäº¤ã€‘å·²å¤„ç† {processed_count} åªETFï¼ˆæˆåŠŸ {successful_count}ï¼Œè·³è¿‡ {skipped_count}ï¼Œå¤±è´¥ {failed_count}ï¼‰ï¼Œæäº¤æ‰¹é‡æ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“...")
                commit_result = force_commit_remaining_files()
                if commit_result:
                    logger.info(f"âœ… æ‰¹é‡æäº¤æˆåŠŸï¼å·²æäº¤ {processed_count} ä¸ªæ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“")
                else:
                    logger.error("âŒ æ‰¹é‡æäº¤å¤±è´¥ï¼")
        
        # ä¸“ä¸šä¿®å¤ï¼šæ•´æ‰¹å¤„ç†å®Œæˆåæ‰æ›´æ–°è¿›åº¦
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„ETF
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count  # é‡ç½®å
        
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼ˆæˆåŠŸ {successful_count}ï¼Œè·³è¿‡ {skipped_count}ï¼Œå¤±è´¥ {failed_count}ï¼‰ï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‰©ä½™æ–‡ä»¶éƒ½è¢«æäº¤
        logger.info("ã€æœ€ç»ˆæäº¤ã€‘å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶åˆ°è¿œç¨‹ä»“åº“...")
        final_commit_result = force_commit_remaining_files()
        if final_commit_result:
            logger.info("âœ… æœ€ç»ˆæäº¤æˆåŠŸï¼æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸæäº¤åˆ°è¿œç¨‹ä»“åº“")
        else:
            logger.error("âŒ æœ€ç»ˆæäº¤å¤±è´¥ï¼å¯èƒ½æœ‰æ–‡ä»¶æœªæäº¤åˆ°è¿œç¨‹ä»“åº“")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                # å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶
                emergency_commit_result = force_commit_remaining_files()
                if emergency_commit_result:
                    logger.info("âœ… ç´§æ€¥æäº¤æˆåŠŸï¼å·²ä¿å­˜å½“å‰è¿›åº¦")
                else:
                    logger.error("âŒ ç´§æ€¥æäº¤å¤±è´¥ï¼")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """
    è·å–æ‰€æœ‰ETFä»£ç 
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.info("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            from data_crawler.all_etfs import update_all_etf_list
            update_all_etf_list()
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return []
        
        # ç¡®ä¿"ETFä»£ç "åˆ—å­˜åœ¨
        if "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘'ETFä»£ç 'åˆ—")
            return []
        
        # ç›´æ¥è·å–ETFä»£ç ï¼ˆå·²ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼‰
        etf_codes = basic_info_df["ETFä»£ç "].tolist()
        
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
        return etf_codes
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

if __name__ == "__main__":
    try:
        # é¦–æ¬¡è¿è¡Œæ—¶ç¡®ä¿å®‰è£…ä¾èµ–
        try:
            import yfinance
        except ImportError:
            logger.error("ç¼ºå°‘yfinanceä¾èµ–ï¼Œè¯·å…ˆå®‰è£…: pip install yfinance")
            raise SystemExit(1)
            
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²ä¿å­˜
        try:
            next_index = get_next_crawl_index()
            total_count = len(get_all_etf_codes())
            logger.info(f"å½“å‰è¿›åº¦: {next_index}/{total_count}")
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
