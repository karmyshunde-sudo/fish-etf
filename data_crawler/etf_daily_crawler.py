#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨æŒ‡å®šæ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
ã€ä¸“ä¸šçº§å®ç°ã€‘
- ä¸¥æ ¼åˆ†ç¦»æ•°æ®ä¸è¿›åº¦ç®¡ç†
- çœŸæ­£çš„å¢é‡çˆ¬å–ä¸è¿›åº¦æ›´æ–°
- ä¸“ä¸šé‡‘èç³»ç»Ÿå¯é æ€§ä¿éšœ
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import akshare as ak
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
import json
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content
from data_crawler.all_etfs import get_all_etf_codes, get_etf_name

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf_daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
PROGRESS_FILE = os.path.join(DATA_DIR, "all_etfs_progress.json")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

class CrawlProgressManager:
    """ä¸“ä¸šè®¾è®¡ï¼šåˆ†ç¦»æ•°æ®ä¸è¿›åº¦ç®¡ç†"""
    
    def __init__(self, etf_list_file: str):
        self.etf_list_file = etf_list_file
        self.progress_file = etf_list_file.replace(".csv", "_progress.json")
        self.etf_codes = self._load_etf_codes()
        self.progress = self._load_progress()
    
    def _load_etf_codes(self) -> list:
        """åŠ è½½ETFä»£ç åˆ—è¡¨"""
        if not os.path.exists(self.etf_list_file):
            logger.error(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.etf_list_file}")
            return []
        
        try:
            df = pd.read_csv(self.etf_list_file)
            if "ETFä»£ç " not in df.columns:
                logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘'ETFä»£ç 'åˆ—")
                return []
            
            # è§„èŒƒåŒ–ETFä»£ç 
            etf_codes = []
            for code in df["ETFä»£ç "].tolist():
                code_str = str(code).strip().zfill(6)
                if code_str.isdigit() and len(code_str) == 6:
                    etf_codes.append(code_str)
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(etf_codes)} åªETFä»£ç ")
            return etf_codes
        except Exception as e:
            logger.error(f"åŠ è½½ETFåˆ—è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
            return []
    
    def _load_progress(self) -> dict:
        """åŠ è½½è¿›åº¦ä¿¡æ¯ï¼ˆä¸“ä¸šè®¾è®¡ï¼šä½¿ç”¨JSONå­˜å‚¨è¿›åº¦ï¼‰"""
        if not os.path.exists(self.progress_file):
            # åˆå§‹åŒ–è¿›åº¦
            return {
                "total_etfs": len(self.etf_codes),
                "next_index": 0,
                "last_update": datetime.now().isoformat(),
                "completed_cycles": 0,
                "etf_statuses": {code: {"status": "pending", "last_crawled": None} 
                                for code in self.etf_codes}
            }
        
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
            
            # éªŒè¯è¿›åº¦æ•°æ®å®Œæ•´æ€§
            if "etf_statuses" not in progress or len(progress["etf_statuses"]) != len(self.etf_codes):
                logger.warning("è¿›åº¦æ•°æ®ä¸å®Œæ•´ï¼Œé‡æ–°åˆå§‹åŒ–")
                return self._reset_progress()
            
            return progress
        except Exception as e:
            logger.error(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œé‡æ–°åˆå§‹åŒ–", exc_info=True)
            return self._reset_progress()
    
    def _reset_progress(self) -> dict:
        """é‡ç½®è¿›åº¦æ•°æ®"""
        return {
            "total_etfs": len(self.etf_codes),
            "next_index": 0,
            "last_update": datetime.now().isoformat(),
            "completed_cycles": 0,
            "etf_statuses": {code: {"status": "pending", "last_crawled": None} 
                            for code in self.etf_codes}
        }
    
    def get_next_batch(self, batch_size: int = 100) -> list:
        """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„ETFä»£ç """
        if self.progress["next_index"] >= self.progress["total_etfs"]:
            logger.info("æ‰€æœ‰ETFå·²å¤„ç†å®Œæˆï¼Œé‡ç½®çˆ¬å–çŠ¶æ€")
            self.progress["next_index"] = 0
            self.progress["completed_cycles"] += 1
        
        start_idx = self.progress["next_index"]
        end_idx = min(start_idx + batch_size, self.progress["total_etfs"])
        batch = self.etf_codes[start_idx:end_idx]
        
        logger.info(f"è·å–æ‰¹æ¬¡: ç´¢å¼• {start_idx}-{end_idx-1}ï¼Œå…± {len(batch)} åªETF")
        return batch
    
    def update_progress(self, etf_code: str, status: str = "completed", last_crawled: str = None) -> bool:
        """æ›´æ–°å•ä¸ªETFçš„è¿›åº¦"""
        try:
            if etf_code not in self.etf_codes:
                logger.warning(f"ETF {etf_code} ä¸åœ¨ETFåˆ—è¡¨ä¸­ï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
                return False
            
            # æ›´æ–°å…·ä½“ETFçŠ¶æ€
            self.progress["etf_statuses"][etf_code] = {
                "status": status,
                "last_crawled": last_crawled or datetime.now().isoformat()
            }
            
            # æ›´æ–°å…¨å±€è¿›åº¦
            current_index = self.etf_codes.index(etf_code) + 1
            self.progress["next_index"] = current_index
            self.progress["last_update"] = datetime.now().isoformat()
            
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ETF {etf_code} è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)
            return False
    
    def save_progress(self) -> bool:
        """ä¿å­˜è¿›åº¦ï¼ˆä»…ä¿å­˜å˜åŒ–çš„éƒ¨åˆ†ï¼‰"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.progress_file), exist_ok=True)
            
            # ä¿å­˜è¿›åº¦
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)
            
            # ä¸“ä¸šéªŒè¯ï¼šç¡®ä¿æ–‡ä»¶å†…å®¹æ­£ç¡®
            if _verify_git_file_content(self.progress_file):
                logger.info(f"è¿›åº¦å·²æˆåŠŸä¿å­˜è‡³ {self.progress_file}")
                return True
            else:
                logger.error("è¿›åº¦æ–‡ä»¶ä¿å­˜åéªŒè¯å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)
            return False

def format_etf_code(code):
    """
    è§„èŒƒåŒ–ETFä»£ç ä¸º6ä½å­—ç¬¦ä¸²æ ¼å¼
    Args:
        code: ETFä»£ç ï¼ˆå¯èƒ½åŒ…å«å‰ç¼€æˆ–é6ä½ï¼‰
    Returns:
        str: è§„èŒƒåŒ–çš„6ä½ETFä»£ç 
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    code_str = str(code).strip().lower()
    
    # ç§»é™¤å¯èƒ½çš„å¸‚åœºå‰ç¼€
    if code_str.startswith(('sh', 'sz', 'hk', 'bj')):
        code_str = code_str[2:]
    
    # ç§»é™¤å¯èƒ½çš„ç‚¹å·ï¼ˆå¦‚"0.600022"ï¼‰
    if '.' in code_str:
        code_str = code_str.split('.')[1] if code_str.startswith('0.') else code_str
    
    # ç¡®ä¿æ˜¯6ä½æ•°å­—
    code_str = code_str.zfill(6)
    
    # éªŒè¯æ ¼å¼
    if not code_str.isdigit() or len(code_str) != 6:
        logger.warning(f"ETFä»£ç æ ¼å¼åŒ–å¤±è´¥: {code_str}")
        return None
    
    return code_str

def get_valid_trading_date_range(start_date, end_date):
    """
    è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´ï¼Œç¡®ä¿åªåŒ…å«å†å²äº¤æ˜“æ—¥
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
    
    Returns:
        tuple: (valid_start_date, valid_end_date) - æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´
    """
    # ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    
    if start_date is None or end_date is None:
        logger.error("æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
        return None, None
    
    # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
    now = get_beijing_time()
    # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡ç±»å‹ä¸€è‡´
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(now)
    
    if end_date > now:
        end_date = now
        logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    days_back = 0
    while days_back < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_end_date.date()):
            break
        valid_end_date -= timedelta(days=1)
        days_back += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼Œè¿”å›ç©ºèŒƒå›´
    if days_back >= 30:
        logger.warning(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼ˆä» {end_date.strftime('%Y-%m-%d')} å¼€å§‹ï¼‰")
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    days_forward = 0
    while days_forward < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_start_date.date()):
            break
        valid_start_date += timedelta(days=1)
        days_forward += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ç»“æŸäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹æ—¥
    if days_forward >= 30:
        valid_start_date = valid_end_date
    
    # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰ç±»å‹ä¸€è‡´
    start_naive = to_naive_datetime(valid_start_date)
    end_naive = to_naive_datetime(valid_end_date)
    
    if start_naive > end_naive:
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def to_naive_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºnaive datetimeï¼ˆæ— æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: naive datetime
    """
    if dt is None:
        return None
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

def to_aware_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºaware datetimeï¼ˆæœ‰æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: aware datetimeï¼ˆåŒ—äº¬æ—¶åŒºï¼‰
    """
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=Config.BEIJING_TIMEZONE)
    return dt

def to_datetime(date_input):
    """
    ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    Args:
        date_input: æ—¥æœŸè¾“å…¥ï¼Œå¯ä»¥æ˜¯strã€dateã€datetimeç­‰ç±»å‹
    Returns:
        datetime.datetime: ç»Ÿä¸€çš„datetimeç±»å‹
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try:
                return datetime.strptime(date_input, fmt)
            except:
                continue
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_input}")
        return None
    return None

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """
    åŠ è½½ETFæ—¥çº¿æ•°æ®
    """
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.warning(f"ETF {etf_code} æ—¥çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        # è¯»å–CSVæ–‡ä»¶ï¼Œæ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float
            }
        )
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        # æŒ‰æ—¥æœŸæ’åºå¹¶å»é‡
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        # ç§»é™¤æœªæ¥æ—¥æœŸçš„æ•°æ®
        today = datetime.now().strftime("%Y-%m-%d")
        df = df[df["æ—¥æœŸ"] <= today]
        return df
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # ç¡®ä¿æ—¥æœŸå‚æ•°æ˜¯datetimeç±»å‹
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºdatetimeç±»å‹")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸå¯¹è±¡æœ‰æ­£ç¡®çš„æ—¶åŒºä¿¡æ¯
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # ç›´æ¥è·å–åŸºç¡€ä»·æ ¼æ•°æ®
        df = ak.fund_etf_hist_em(
            symbol=etf_code,
            period="daily",
            start_date=start_date.strftime("%Y%m%d"),
            end_date=end_date.strftime("%Y%m%d")
        )
        
        # æ£€æŸ¥åŸºç¡€æ•°æ®
        if df is None or df.empty:
            logger.warning(f"ETF {etf_code} åŸºç¡€æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è·å–æŠ˜ä»·ç‡
        try:
            fund_df = ak.fund_etf_fund_daily_em()
            if not fund_df.empty and "åŸºé‡‘ä»£ç " in fund_df.columns and "æŠ˜ä»·ç‡" in fund_df.columns:
                etf_fund_data = fund_df[fund_df["åŸºé‡‘ä»£ç "] == etf_code]
                if not etf_fund_data.empty:
                    df["æŠ˜ä»·ç‡"] = etf_fund_data["æŠ˜ä»·ç‡"].values[0]
        except Exception as e:
            logger.warning(f"è·å–ETF {etf_code} æŠ˜ä»·ç‡æ•°æ®å¤±è´¥: {str(e)}")
        
        # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
        df["ETFä»£ç "] = etf_code
        df["ETFåç§°"] = get_etf_name(etf_code)
        df["çˆ¬å–æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ç¡®ä¿åˆ—é¡ºåº
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'ETFä»£ç ', 'ETFåç§°',
            'çˆ¬å–æ—¶é—´', 'æŠ˜ä»·ç‡'
        ]
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """
    è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´
    ä¸“ä¸šä¿®å¤ï¼šè§£å†³ETFå…¨éƒ¨è·³è¿‡é—®é¢˜
    """
    try:
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¸€è‡´
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è®¾ç½®ç»“æŸæ—¥æœŸä¸ºæœ€è¿‘äº¤æ˜“æ—¥ï¼ˆç¡®ä¿æ˜¯äº¤æ˜“æ—¥ï¼‰
        end_date = last_trading_day
        
        # è·å–å½“å‰åŒ—äº¬æ—¶é—´
        current_time = get_beijing_time()
        
        # å¦‚æœç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œè°ƒæ•´ä¸ºå½“å‰æ—¶é—´
        if end_date > current_time:
            end_date = current_time
        
        # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿ç»“æŸæ—¥æœŸæ˜¯äº¤æ˜“æ—¥
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
            if (last_trading_day - end_date).days > 30:
                logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥")
                return None, None
        
        # ä¸“ä¸šä¿®å¤ï¼šè®¾ç½®ç»“æŸæ—¶é—´ä¸ºå½“å¤©23:59:59
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        # æ„å»ºETFæ•°æ®æ–‡ä»¶è·¯å¾„
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        # æ£€æŸ¥å†å²æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                
                # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    # ä½¿ç”¨é»˜è®¤å›é€€ç­–ç•¥ï¼šè·å–ä¸€å¹´æ•°æ®
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                
                # è·å–æœ€æ–°æœ‰æ•ˆæ—¥æœŸ
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ä¸­æ—¥æœŸåˆ—å…¨ä¸ºNaN")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                
                # ç¡®ä¿latest_dateæ˜¯datetimeç±»å‹å¹¶å¸¦æœ‰æ—¶åŒº
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                # ä¸“ä¸šä¿®å¤ï¼šæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†ï¼ˆå¿½ç•¥æ—¶é—´éƒ¨åˆ†ï¼‰
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                logger.debug(f"ETF {etf_code} æ—¥æœŸæ¯”è¾ƒ: æœ€æ–°æ—¥æœŸ={latest_date_date}, ç»“æŸæ—¥æœŸ={end_date_date}")
                
                # ä¸“ä¸šä¿®å¤ï¼šå¦‚æœæœ€æ–°æ—¥æœŸå°äºç»“æŸæ—¥æœŸï¼Œåˆ™éœ€è¦çˆ¬å–
                if latest_date_date < end_date_date:
                    # ä¸“ä¸šä¿®å¤ï¼šä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¼€å§‹
                    start_date = latest_date + timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæ˜¯äº¤æ˜“æ—¥
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæœ‰æ—¶åŒºä¿¡æ¯
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    
                    # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿start_dateä¸è¶…è¿‡end_date
                    if start_date > end_date:
                        logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ={latest_date_date}ï¼Œç»“æŸæ—¥æœŸ={end_date_date}ï¼‰")
                        return None, None
                    
                    logger.info(f"ETF {etf_code} éœ€è¦æ›´æ–°æ•°æ®: æœ€æ–°æ—¥æœŸ {latest_date_date} < ç»“æŸæ—¥æœŸ {end_date_date}")
                    logger.info(f"ETF {etf_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                    return start_date, end_date
                else:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°: æœ€æ–°æ—¥æœŸ {latest_date_date} >= ç»“æŸæ—¥æœŸ {end_date_date}")
                    return None, None
            
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # å‡ºé”™æ—¶å°è¯•è·å–ä¸€å¹´æ•°æ®
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                return start_date, end_date
        else:
            logger.info(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œå°†è·å–ä¸€å¹´å†å²æ•°æ®")
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
            return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """
    ä¿å­˜ETFæ—¥çº¿æ•°æ®
    """
    if df.empty:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(DAILY_DIR, exist_ok=True)
    
    # ä¿å­˜å‰å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if "æ—¥æœŸ" in df.columns:
        df_save = df.copy()
        df_save["æ—¥æœŸ"] = df_save["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
    else:
        df_save = df
    
    # ä¿å­˜åˆ°CSV
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            df_save.to_csv(temp_file.name, index=False)
        shutil.move(temp_file.name, save_path)
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å
        if not _verify_git_file_content(save_path):
            logger.warning(f"ETF {etf_code} æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡è¯•æäº¤")
        commit_message = f"feat: æ›´æ–°ETF {etf_code} æ—¥çº¿æ•°æ® [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(save_path, commit_message)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def crawl_all_etfs_daily_data() -> None:
    """
    çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ®
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        # ä¸“ä¸šä¿®å¤ï¼šä½¿ç”¨åˆ†ç¦»çš„è¿›åº¦ç®¡ç†
        progress_manager = CrawlProgressManager(BASIC_INFO_FILE)
        etf_codes = progress_manager.etf_codes
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        # è·å–å½“å‰æ‰¹æ¬¡
        batch_size = 100
        batch_codes = progress_manager.get_next_batch(batch_size)
        
        # è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹ETF
        if batch_codes:
            first_stock = f"{batch_codes[0]} - {get_etf_name(batch_codes[0])}"
            last_stock = f"{batch_codes[-1]} - {get_etf_name(batch_codes[-1])}"
            logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock}")
            logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock}")
        
        # å¤„ç†è¿™æ‰¹ETF
        processed_count = 0
        for i, etf_code in enumerate(batch_codes):
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1.5, 2.5))
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                progress_manager.update_progress(etf_code, "skipped")
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                progress_manager.update_progress(etf_code, "failed")
                # è®°å½•å¤±è´¥æ—¥å¿—
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                continue
            
            # ä¿å­˜æ•°æ®
            save_etf_daily_data(etf_code, df)
            
            # æ›´æ–°è¿›åº¦
            processed_count += 1
            progress_manager.update_progress(etf_code, "completed")
            current_index = progress_manager.progress["next_index"]
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        if not progress_manager.save_progress():
            logger.error("è¿›åº¦ä¿å­˜å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´ä¸‹æ¬¡çˆ¬å–é‡å¤")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„ETF
        remaining_stocks = total_count - progress_manager.progress["next_index"]
        if remaining_stocks < 0:
            remaining_stocks = total_count  # é‡ç½®å
        
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
        # ç¡®ä¿æ‰€æœ‰å‰©ä½™æ–‡ä»¶éƒ½è¢«æäº¤
        logger.info("å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
        if not force_commit_remaining_files():
            logger.error("å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€
        try:
            if 'progress_manager' in locals():
                progress_manager.save_progress()
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²ä¿å­˜
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ è¿›åº¦æ–‡ä»¶æ£€æŸ¥é€»è¾‘
            pass
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
