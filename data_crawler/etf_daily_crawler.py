import os
import sys
import shutil
import tempfile
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from utils.logger import logger
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content
from utils.etf_utils import fetch_etf_daily_k_data, get_etf_name
from utils.constants import (
    DAILY_DIR, BASIC_INFO_FILE, BATCH_SIZE,
    RECENT_DATE, DAYS_PER_TASK, DATA_START_DATE
)

# ============================================================
# âœ… ETF æ—¥çº¿æ•°æ®ç»“æ„ & ç²¾åº¦è§„èŒƒåŒ–å‡½æ•°
# ============================================================
def normalize_etf_daily_data(df: pd.DataFrame, etf_code: str, etf_name: str) -> pd.DataFrame:
    """
    è§„èŒƒåŒ–ETFæ—¥çº¿æ•°æ®ç»“æ„å’Œæ•°æ®ç²¾åº¦ï¼Œä½¿å…¶ä¸data/etf/daily/159222.csvä¸€è‡´
    """
    expected_columns = [
        "æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜",
        "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢",
        "æ¢æ‰‹ç‡", "IOPV", "æŠ˜ä»·ç‡", "æº¢ä»·ç‡",
        "ETFä»£ç ", "ETFåç§°", "çˆ¬å–æ—¶é—´"
    ]

    # ç¡®ä¿å­˜åœ¨å¿…è¦åˆ—
    for col in expected_columns:
        if col not in df.columns:
            df[col] = np.nan

    # å¤„ç†æ•°å€¼ç²¾åº¦
    four_decimal_cols = [
        "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜",
        "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢",
        "æ¢æ‰‹ç‡", "IOPV", "æŠ˜ä»·ç‡", "æº¢ä»·ç‡"
    ]
    int_cols = ["æˆäº¤é‡"]

    for col in four_decimal_cols:
        df[col] = pd.to_numeric(df[col], errors="coerce").round(4)
    for col in int_cols:
        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

    # ETFåŸºæœ¬ä¿¡æ¯
    df["ETFä»£ç "] = etf_code
    df["ETFåç§°"] = etf_name
    df["çˆ¬å–æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ—¥æœŸæ ¼å¼åŒ–
    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce").dt.strftime("%Y-%m-%d")
    df = df.dropna(subset=["æ—¥æœŸ"])

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    df = df[expected_columns]
    df = df.sort_values("æ—¥æœŸ", ascending=True).reset_index(drop=True)

    return df


# ============================================================
# âœ… æ‰¹é‡ä¿å­˜å‡½æ•°ï¼ˆå¢åŠ æ ¼å¼åŒ–å¤„ç†ï¼‰
# ============================================================
def save_etf_daily_data_batch(etf_data_dict: dict) -> int:
    """
    æ‰¹é‡ä¿å­˜ETFæ—¥çº¿æ•°æ®ï¼Œæ¯ä¸ªETFä¸€ä¸ªCSVæ–‡ä»¶
    """
    if not etf_data_dict:
        return 0

    os.makedirs(DAILY_DIR, exist_ok=True)
    saved_count = 0

    for etf_code, df in etf_data_dict.items():
        if df.empty:
            continue

        etf_name = get_etf_name(etf_code)
        df = normalize_etf_daily_data(df, etf_code, etf_name)

        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")

        # å†™å…¥ä¸´æ—¶æ–‡ä»¶åå†ç§»åŠ¨ï¼Œé˜²æ­¢éƒ¨åˆ†å†™å…¥æŸå
        with tempfile.NamedTemporaryFile(delete=False, mode="w", encoding="utf-8-sig", newline="") as temp_file:
            df.to_csv(temp_file.name, index=False, encoding="utf-8-sig")
        shutil.move(temp_file.name, save_path)

        saved_count += 1
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path}")

    return saved_count


# ============================================================
# âœ… ä¸»çˆ¬å–é€»è¾‘
# ============================================================
def crawl_etf_daily_data(etf_codes: list[str], start_date: str, end_date: str):
    logger.info(f"å¼€å§‹çˆ¬å–ETFæ—¥çº¿æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{start_date} â†’ {end_date}")
    etf_data_dict = {}

    for etf_code in etf_codes:
        etf_name = get_etf_name(etf_code)
        logger.info(f"ğŸ“ˆ æ­£åœ¨è·å– {etf_name} ({etf_code}) æ—¥çº¿æ•°æ®...")

        try:
            df = fetch_etf_daily_k_data(etf_code, start_date, end_date)
            if df is not None and not df.empty:
                etf_data_dict[etf_code] = df
            else:
                logger.warning(f"âš ï¸ {etf_code} æœªè¿”å›æœ‰æ•ˆæ•°æ®")

        except Exception as e:
            logger.error(f"âŒ è·å– {etf_code} æ•°æ®å¤±è´¥: {e}")
            continue

    if not etf_data_dict:
        logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•ETFæ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
        return

    logger.info("å¼€å§‹æ‰¹é‡ä¿å­˜ETFæ•°æ®...")
    saved_count = save_etf_daily_data_batch(etf_data_dict)
    logger.info(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {saved_count} åªETFã€‚")

    # Gitæ“ä½œ
    try:
        os.system("git add data/etf/daily/*.csv")
        commit_success = force_commit_remaining_files()
        if commit_success:
            logger.info("âœ… æ•°æ®æ–‡ä»¶å·²æˆåŠŸæäº¤è‡³Gitä»“åº“ã€‚")
        else:
            logger.warning("âš ï¸ æ•°æ®æ–‡ä»¶æäº¤è‡³Gitä»“åº“å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"âŒ æäº¤æ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# ============================================================
# âœ… å…¥å£å‡½æ•°
# ============================================================
def run_etf_daily_crawler():
    logger.info("ğŸš€ å¯åŠ¨ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡")

    try:
        # ä»åŸºç¡€æ–‡ä»¶è¯»å–ETFä»£ç 
        if not os.path.exists(BASIC_INFO_FILE):
            logger.error(f"âŒ ETFåŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return

        all_etf_info = pd.read_csv(BASIC_INFO_FILE, dtype=str)
        if all_etf_info.empty or "ä»£ç " not in all_etf_info.columns:
            logger.error("âŒ ETFåŸºç¡€ä¿¡æ¯æ–‡ä»¶æ— æ•ˆæˆ–ç¼ºå°‘â€˜ä»£ç â€™åˆ—")
            return

        etf_codes = all_etf_info["ä»£ç "].dropna().unique().tolist()
        logger.info(f"å…±è¯»å– {len(etf_codes)} åªETFä»£ç ã€‚")

        # è®¾ç½®çˆ¬å–åŒºé—´
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=DAYS_PER_TASK)).strftime("%Y-%m-%d")

        crawl_etf_daily_data(etf_codes, start_date, end_date)

        logger.info("ğŸ ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡å®Œæˆã€‚")

    except Exception as e:
        logger.error(f"âŒ è¿è¡ŒETFæ—¥çº¿çˆ¬å–ä»»åŠ¡æ—¶å‡ºç°é”™è¯¯: {e}")


# ============================================================
# âœ… è„šæœ¬ç›´æ¥æ‰§è¡Œå…¥å£
# ============================================================
if __name__ == "__main__":
    run_etf_daily_crawler()
