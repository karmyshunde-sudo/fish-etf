#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–æœºåˆ¶
ã€å…³é”®ä¿®å¤ã€‘
- ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
- æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() ä½†ä¸ç«‹å³æäº¤
- æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘ä¸è‚¡ç¥¨çˆ¬å–ä»£ç å®Œå…¨ä¸€è‡´
BATCH_SIZE = 30  # ä¸€ä¸ªæ‰¹æ¬¡å¤„ç†çš„ETFæ•°é‡
COMMIT_BATCH_SIZE = 10  # æ¯COMMIT_BATCH_SIZEä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
BASE_DELAY = 2.5  # åŸºç¡€å»¶è¿Ÿå¢åŠ åˆ°2.5ç§’
MAX_RETRIES = 1  # å¢åŠ é‡è¯•æ¬¡æ•°
CONNECTION_TIMEOUT = 30  # å¢åŠ è¶…æ—¶æ—¶é—´
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

def get_etf_name(etf_code):
    """è·å–ETFåç§°ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            return etf_code
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        return etf_row["ETFåç§°"].values[0] if not etf_row.empty else etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code

def get_etf_fund_size(etf_code: str) -> float:
    """
    ä»ETFåˆ—è¡¨ä¸­è·å–åŸºé‡‘è§„æ¨¡ï¼ˆåªè¯»ï¼‰
    """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0.0
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "ETFä»£ç " not in basic_info_df.columns or "åŸºé‡‘è§„æ¨¡" not in basic_info_df.columns:
            logger.warning(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼ˆETFä»£ç /åŸºé‡‘è§„æ¨¡ï¼‰")
            return 0.0
        
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        if etf_row.empty:
            logger.warning(f"ETF {etf_code} åœ¨åˆ—è¡¨ä¸­ä¸å­˜åœ¨")
            return 0.0
        
        fund_size = float(etf_row["åŸºé‡‘è§„æ¨¡"].values[0])
        return fund_size * 100000000  # äº¿å…ƒè½¬è‚¡
    
    except Exception as e:
        logger.error(f"è·å–ETF {etf_code} åŸºé‡‘è§„æ¨¡å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_next_crawl_index() -> int:
    """è·å–è¿›åº¦ç´¢å¼•ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return 0
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        return int(basic_info_df["next_crawl_index"].iloc[0])
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """ä¿å­˜è¿›åº¦ï¼ˆä»…æ›´æ–°è¿›åº¦å­—æ®µï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        
        # é€šè¿‡ commit_files_in_batches æäº¤
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(BASIC_INFO_FILE, commit_message)
    except Exception as e:
        logger.error(f"ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def to_naive_datetime(dt):
    """è½¬æ¢ä¸ºæ— æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=None) if dt.tzinfo else dt

def to_aware_datetime(dt):
    """è½¬æ¢ä¸ºåŒ—äº¬æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=Config.BEIJING_TIMEZONE) if dt.tzinfo is None else dt

def to_datetime(date_input):
    """ç»Ÿä¸€æ—¥æœŸè½¬æ¢"""
    if isinstance(date_input, datetime): return date_input
    if isinstance(date_input, str):
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try: return datetime.strptime(date_input, fmt)
            except: continue
    return None

def get_valid_trading_date_range(start_date, end_date):
    """è·å–æœ‰æ•ˆäº¤æ˜“æ—¥èŒƒå›´"""
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    if not start_date or not end_date: return None, None
    
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(get_beijing_time())
    if end_date > now: end_date = now
    
    # æŸ¥æ‰¾æœ‰æ•ˆç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    for _ in range(30):
        if is_trading_day(valid_end_date.date()): break
        valid_end_date -= timedelta(days=1)
    else:
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆèµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    for _ in range(30):
        if is_trading_day(valid_start_date.date()): break
        valid_start_date += timedelta(days=1)
    else:
        valid_start_date = valid_end_date
    
    if to_naive_datetime(valid_start_date) > to_naive_datetime(valid_end_date):
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """åŠ è½½æœ¬åœ°æ•°æ®"""
    try:
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if not os.path.exists(file_path):
            return pd.DataFrame()
        
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float,
                "æŒ¯å¹…": float,
                "æ¶¨è·Œå¹…": float,
                "æ¶¨è·Œé¢": float,
                "æ¢æ‰‹ç‡": float,
                "IOPV": float,
                "æŠ˜ä»·ç‡": float,
                "æº¢ä»·ç‡": float
            }
        )
        
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns):
            return pd.DataFrame()
        
        df = df[[col for col in standard_columns if col in df.columns]]
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        today = datetime.now().strftime("%Y-%m-%d")
        return df[df["æ—¥æœŸ"] <= today]
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
# 1. æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() 
# 2. æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
class RequestThrottler:
    """è¯·æ±‚é™æµå™¨ - ä¿®å¤è¿æ¥é—®é¢˜"""
    def __init__(self, base_delay=2.5, max_delay=15.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.current_delay = base_delay
        self.last_request_time = None
        self.failure_count = 0
    
    def wait(self):
        """ç­‰å¾…é€‚å½“æ—¶é—´å†è¯·æ±‚"""
        if self.last_request_time:
            elapsed = time.time() - self.last_request_time
            if elapsed < self.current_delay:
                time.sleep(self.current_delay - elapsed)
        
        self.last_request_time = time.time()
    
    def record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚ - å®ç°æŒ‡æ•°é€€é¿ç­–ç•¥"""
        self.failure_count += 1
        # æŒ‡æ•°é€€é¿ï¼š2^failure_count * base_delay
        self.current_delay = min(
            self.max_delay, 
            self.base_delay * (2 ** self.failure_count) + random.uniform(0.5, 1.5)
        )
        logger.warning(f"è¯·æ±‚å¤±è´¥ï¼ŒæŒ‡æ•°é€€é¿å»¶æ—¶: {self.current_delay:.2f}ç§’")
    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚ - é‡ç½®å»¶æ—¶"""
        self.failure_count = 0
        self.current_delay = self.base_delay

throttler = RequestThrottler(base_delay=BASE_DELAY)

def apply_precision_control(df: pd.DataFrame) -> pd.DataFrame:
    """
    åº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
    """
    # éœ€è¦ä¿ç•™4ä½å°æ•°çš„å­—æ®µ
    precision_fields = [
        'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é¢',
        'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
        'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡'
    ]
    
    for field in precision_fields:
        if field in df.columns:
            # ä¿ç•™4ä½å°æ•°
            df[field] = df[field].round(4)
    
    # æˆäº¤é‡é€šå¸¸ä¸ºæ•´æ•°
    if 'æˆäº¤é‡' in df.columns:
        df['æˆäº¤é‡'] = df['æˆäº¤é‡'].round(0).astype(int)
    
    return df

def process_yfinance_data(df: pd.DataFrame, etf_code: str) -> pd.DataFrame:
    """
    å¤„ç†Yahoo Financeè¿”å›çš„DataFrame
    """
    # 1. ç¡®ä¿DataFrameæ˜¯æ‰å¹³ç»“æ„
    if isinstance(df.columns, pd.MultiIndex):
        # æå–ç¬¬ä¸€çº§åˆ—å
        columns = []
        for col in df.columns:
            if isinstance(col, tuple) and len(col) > 0:
                columns.append(col[0])
            else:
                columns.append(col)
        df.columns = columns
    
    # 2. ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
    if 'Date' in df.columns:
        df = df.reset_index(drop=True)
    elif df.index.name == 'Date':
        df = df.reset_index()
    elif 'date' in df.columns:
        df = df.rename(columns={'date': 'Date'})
    else:
        return pd.DataFrame()  # æ— æœ‰æ•ˆæ—¥æœŸåˆ—ï¼Œè¿”å›ç©ºDataFrame
    
    # 3. æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    for col in required_columns:
        if col not in df.columns:
            logger.error(f"ETF {etf_code} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return pd.DataFrame()  # å…³é”®ä¿®å¤ï¼šç¼ºå¤±å¿…è¦åˆ—ï¼Œç›´æ¥è¿”å›ç©ºDataFrame
    
    # 4. åˆ›å»ºä¸´æ—¶å•åˆ—DataFrame
    result_df = pd.DataFrame()
    result_df['æ—¥æœŸ'] = df['Date'].dt.strftime('%Y-%m-%d')
    result_df['å¼€ç›˜'] = df['Open'].astype(float)
    result_df['æœ€é«˜'] = df['High'].astype(float)
    result_df['æœ€ä½'] = df['Low'].astype(float)
    result_df['æ”¶ç›˜'] = df['Close'].astype(float)
    result_df['æˆäº¤é‡'] = df['Volume'].astype(float)
    
    # 5. è®¡ç®—è¡ç”Ÿå­—æ®µ
    # æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / æœ€ä½ * 100%
    result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['æœ€ä½'] * 100).round(2)
    
    # æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰ä¸€æ—¥æ”¶ç›˜
    result_df['æ¶¨è·Œé¢'] = result_df['æ”¶ç›˜'].diff().fillna(0)
    
    # æ¶¨è·Œå¹… = æ¶¨è·Œé¢ / å‰ä¸€æ—¥æ”¶ç›˜ * 100%
    prev_close = result_df['æ”¶ç›˜'].shift(1)
    # é¿å…é™¤ä»¥0
    valid_prev_close = prev_close.replace(0, float('nan'))
    result_df['æ¶¨è·Œå¹…'] = (result_df['æ¶¨è·Œé¢'] / valid_prev_close * 100).round(2)
    result_df['æ¶¨è·Œå¹…'] = result_df['æ¶¨è·Œå¹…'].fillna(0)
    
    # æ¢æ‰‹ç‡ = æˆäº¤é‡ / åŸºé‡‘è§„æ¨¡
    fund_size = get_etf_fund_size(etf_code)
    if fund_size > 0:
        result_df['æ¢æ‰‹ç‡'] = (result_df['æˆäº¤é‡'] / fund_size * 100).round(2)
    else:
        result_df['æ¢æ‰‹ç‡'] = 0.0
    
    # 6. IOPV/æŠ˜ä»·ç‡/æº¢ä»·ç‡ï¼ˆYahoo Financeä¸æä¾›ï¼‰
    result_df['IOPV'] = 0.0
    result_df['æŠ˜ä»·ç‡'] = 0.0
    result_df['æº¢ä»·ç‡'] = 0.0
    
    # 7. æˆäº¤é¢ = æ”¶ç›˜ * æˆäº¤é‡
    result_df['æˆäº¤é¢'] = (result_df['æ”¶ç›˜'] * result_df['æˆäº¤é‡']).round(2)
    
    # å…³é”®ä¿®å¤ï¼šåº”ç”¨æ•°æ®ç²¾åº¦æ§åˆ¶
    result_df = apply_precision_control(result_df)
    
    return result_df

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨Yahoo Financeçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯")
            return pd.DataFrame()
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç†
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è½¬æ¢ä¸ºYahoo Financeæ‰€éœ€çš„æ ¼å¼
        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")
        
        # 1. æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦å¢å¼ºé‡è¯•æœºåˆ¶ï¼‰
        max_retries = MAX_RETRIES
        for retry in range(max_retries):
            try:
                throttler.wait()
                
                # Yahoo Finance API
                symbol = etf_code
                if etf_code.startswith(('51', '56', '57', '58')):
                    symbol = f"{etf_code}.SS"
                elif etf_code.startswith('15'):
                    symbol = f"{etf_code}.SZ"
                
                # è·å–æ•°æ®
                df = yf.download(
                    symbol,
                    start=start_str,
                    end=end_str,
                    progress=False,
                    auto_adjust=True,
                    timeout=CONNECTION_TIMEOUT
                )
                
                # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ•°æ®
                if df is None or df.empty:
                    raise ValueError("No data returned")
                
                throttler.record_success()
                break
                
            except Exception as e:
                throttler.record_failure()
                if retry == max_retries - 1:
                    logger.error(f"ETF {etf_code} æ¥å£è¯·æ±‚å¤±è´¥ (é‡è¯• {max_retries} æ¬¡): {str(e)}")
                    return pd.DataFrame()
                
                wait_time = BASE_DELAY * (2 ** retry) + random.uniform(1.5, 3.0)
                logger.warning(f"ETF {etf_code} è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯•: {str(e)}")
                time.sleep(wait_time)
        
        # 2. å¤„ç†æ•°æ®
        df = process_yfinance_data(df, etf_code)
        
        # 3. ä¸¥æ ¼æ•°æ®éªŒè¯
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns) or df.empty:
            logger.error(f"ETF {etf_code} æ•°æ®éªŒè¯å¤±è´¥ - æ— æ³•ä¿å­˜")
            return pd.DataFrame()
        
        # 4. è¡¥å……å¿…è¦å­—æ®µ
        df['ETFä»£ç '] = etf_code
        df['ETFåç§°'] = get_etf_name(etf_code)
        df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 5. ç¡®ä¿å­—æ®µé¡ºåº
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """å¢é‡æ—¥æœŸèŒƒå›´ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        current_time = get_beijing_time()
        if end_date > current_time:
            end_date = current_time
        
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
        
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                if "æ—¥æœŸ" not in df.columns:
                    return last_trading_day - timedelta(days=365), end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    return last_trading_day - timedelta(days=365), end_date
                
                latest_date = valid_dates.max()
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    if start_date > end_date:
                        return None, None
                    return start_date, end_date
                return None, None
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                return last_trading_day - timedelta(days=365), end_date
        else:
            return last_trading_day - timedelta(days=365), end_date
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        return last_trading_day - timedelta(days=365), last_trading_day

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼åŒ¹é…è‚¡ç¥¨çˆ¬å–ä»£ç çš„æäº¤æœºåˆ¶
# 1. æ¯å¤„ç†ä¸€ä¸ªETFè°ƒç”¨ commit_files_in_batches() ä½†ä¸ç«‹å³æäº¤
# 2. æ¯10ä¸ªETFè°ƒç”¨ force_commit_remaining_files() ç¡®ä¿æäº¤
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """ä¿å­˜æ•°æ®ï¼ˆå…³é”®ä¿®å¤ï¼šä¸è‚¡ç¥¨çˆ¬å–ä»£ç ç›¸åŒï¼‰"""
    if df.empty: 
        logger.error(f"ETF {etf_code} æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        return
    
    os.makedirs(DAILY_DIR, exist_ok=True)
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            # ä¿å­˜æ•°æ®
            df.to_csv(temp_file.name, index=False)
        
        # ç§»åŠ¨æ–‡ä»¶
        shutil.move(temp_file.name, save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
        
        # å…³é”®ä¿®å¤ï¼šè°ƒç”¨ commit_files_in_batchesï¼Œä½†ä¸æ£€æŸ¥è¿”å›å€¼
        commit_message = f"è‡ªåŠ¨æ›´æ–°ETF {etf_code} æ—¥çº¿æ•°æ®"
        commit_files_in_batches(save_path, commit_message)
        logger.debug(f"å·²æ·»åŠ ETF {etf_code} æ—¥çº¿æ•°æ®åˆ°æäº¤é˜Ÿåˆ—")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

def crawl_all_etfs_daily_data() -> None:
    """ä¸»çˆ¬å–é€»è¾‘"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        next_index = get_next_crawl_index()
        
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        actual_end_idx = end_idx % total_count
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
        
        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æœ¬åœ°è®¡æ•°å™¨ï¼Œä¸å†ä¾èµ–å…¨å±€å˜é‡
        processed_count = 0
        for i, etf_code in enumerate(batch_codes):
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            if df.empty:
                logger.error(f"âŒ ETF {etf_code} æ•°æ®è·å–å¤±è´¥ - æ— æ³•ä¿å­˜")
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æ•°æ®éªŒè¯å¤±è´¥\n")
                continue
            
            # ä¿å­˜æ•°æ®ï¼ˆä½†ä¸æäº¤ï¼‰
            save_etf_daily_data(etf_code, df)
            
            processed_count += 1
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            # å…³é”®ä¿®å¤ï¼šæ¯å¤„ç†10ä¸ªETFï¼Œæ‰‹åŠ¨è§¦å‘æäº¤
            if processed_count % COMMIT_BATCH_SIZE == 0:
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæ‰§è¡Œæ‰¹é‡æäº¤...")
                if not force_commit_remaining_files():
                    logger.error("æäº¤æ‰¹é‡æ–‡ä»¶å¤±è´¥")
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶
        logger.info("å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
        if not force_commit_remaining_files():
            logger.error("å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±")
        
        # æ›´æ–°è¿›åº¦
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                if not force_commit_remaining_files():
                    logger.error("å¼·åˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """è·å–ETFä»£ç åˆ—è¡¨ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return []
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶æ ¼å¼é”™è¯¯")
            return []
        
        return basic_info_df["ETFä»£ç "].tolist()
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

if __name__ == "__main__":
    try:
        # é¦–æ¬¡è¿è¡Œæ—¶ç¡®ä¿å®‰è£…ä¾èµ–
        try:
            import yfinance
        except ImportError:
            logger.error("ç¼ºå°‘yfinanceä¾èµ–ï¼Œè¯·å…ˆå®‰è£…: pip install yfinance")
            raise SystemExit(1)
        
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
