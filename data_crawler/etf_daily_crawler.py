#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - ä¸¥æ ¼è®¡æ•°å™¨ç‰ˆæœ¬
yFinanceæ•°æ®DS-etf_daily_crawler-6.py
ã€ä¸¥æ ¼æŒ‰ç…§æˆåŠŸè®¡æ•°å™¨=10æ—¶æäº¤ï¼Œç¡®ä¿æ¯10ä¸ªæˆåŠŸæ–‡ä»¶æäº¤ä¸€æ¬¡ã€‘
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
import subprocess
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ã€å…³é”®å‚æ•°ã€‘ä¸¥æ ¼éµå¾ªæ¯10ä¸ªæˆåŠŸæ–‡ä»¶æäº¤ä¸€æ¬¡
BATCH_SIZE = 40  # æ¯æ‰¹å¤„ç†40ä¸ªETF
COMMIT_BATCH_SIZE = 10  # æ¯10ä¸ªæˆåŠŸæ–‡ä»¶æäº¤ä¸€æ¬¡
BASE_DELAY = 1.0
MAX_RETRIES = 2

class StrictCounterETFCrawler:
    def __init__(self):
        # ã€å…³é”®ã€‘ä¸¥æ ¼çš„è®¡æ•°å™¨ç³»ç»Ÿ
        self.success_count = 0  # æˆåŠŸè·å–æ•°æ®çš„è®¡æ•°å™¨
        self.fail_count = 0     # å¤±è´¥è·å–æ•°æ®çš„è®¡æ•°å™¨
        self.skip_count = 0     # è·³è¿‡è®¡æ•°ï¼ˆæ•°æ®å·²æœ€æ–°ï¼‰
        self.staged_files = []  # æš‚å­˜åŒºï¼šæˆåŠŸæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
        self.batch_commit_number = 1  # æ‰¹æ¬¡æäº¤ç¼–å·
        
    def get_etf_name(self, etf_code):
        """è·å–ETFåç§°"""
        try:
            if not os.path.exists(BASIC_INFO_FILE):
                return etf_code
            
            basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
            if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
                return etf_code
            
            etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
            if not etf_row.empty:
                return etf_row["ETFåç§°"].values[0]
            
            return etf_code
        except Exception as e:
            logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
            return etf_code

    def get_yfinance_symbol(self, etf_code):
        """è·å–yfinanceå¯¹åº”çš„symbol"""
        if etf_code.startswith(('51', '56', '57', '58')):
            return f"{etf_code}.SS"
        elif etf_code.startswith('15'):
            return f"{etf_code}.SZ"
        else:
            return etf_code

    def crawl_with_yfinance(self, etf_code, start_date, end_date):
        """ä½¿ç”¨yfinanceçˆ¬å–æ•°æ®"""
        try:
            symbol = self.get_yfinance_symbol(etf_code)
            
            df = yf.download(
                symbol,
                start=start_date.strftime("%Y-%m-%d"),
                end=end_date.strftime("%Y-%m-%d"),
                progress=False,
                auto_adjust=True,
                timeout=30
            )
            
            if df is None or df.empty:
                return None
            
            # å¤„ç†æ•°æ®
            result_df = pd.DataFrame()
            result_df['æ—¥æœŸ'] = df.index.strftime('%Y-%m-%d')
            result_df['å¼€ç›˜'] = df['Open'].astype(float)
            result_df['æœ€é«˜'] = df['High'].astype(float)
            result_df['æœ€ä½'] = df['Low'].astype(float)
            result_df['æ”¶ç›˜'] = df['Close'].astype(float)
            result_df['æˆäº¤é‡'] = df['Volume'].astype(float)
            
            # è®¡ç®—è¡ç”Ÿå­—æ®µ
            result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['æœ€ä½'] * 100).round(2)
            result_df['æ¶¨è·Œé¢'] = result_df['æ”¶ç›˜'].diff().fillna(0)
            
            prev_close = result_df['æ”¶ç›˜'].shift(1)
            valid_prev_close = prev_close.replace(0, float('nan'))
            result_df['æ¶¨è·Œå¹…'] = (result_df['æ¶¨è·Œé¢'] / valid_prev_close * 100).round(2)
            result_df['æ¶¨è·Œå¹…'] = result_df['æ¶¨è·Œå¹…'].fillna(0)
            
            result_df['æˆäº¤é¢'] = (result_df['æ”¶ç›˜'] * result_df['æˆäº¤é‡']).round(2)
            result_df['æ¢æ‰‹ç‡'] = 0.0
            result_df['IOPV'] = 0.0
            result_df['æŠ˜ä»·ç‡'] = 0.0
            result_df['æº¢ä»·ç‡'] = 0.0
            
            result_df['ETFä»£ç '] = etf_code
            result_df['ETFåç§°'] = self.get_etf_name(etf_code)
            result_df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            return result_df
            
        except Exception as e:
            logger.error(f"yfinanceçˆ¬å– {etf_code} å¤±è´¥: {str(e)}")
            return None

    def crawl_etf_data(self, etf_code, start_date, end_date):
        """çˆ¬å–ETFæ•°æ®"""
        max_retries = MAX_RETRIES
        
        for retry in range(max_retries):
            try:
                df = self.crawl_with_yfinance(etf_code, start_date, end_date)
                
                if df is not None and not df.empty:
                    return df
                
                # ç­‰å¾…åé‡è¯•
                if retry < max_retries - 1:
                    wait_time = (retry + 1) * 3
                    logger.info(f"æ•°æ®è·å–å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                logger.error(f"çˆ¬å– {etf_code} å¤±è´¥ (å°è¯• {retry + 1}): {str(e)}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 3)
        
        return None

    def save_etf_data(self, etf_code, df):
        """ä¿å­˜ETFæ•°æ®åˆ°æœ¬åœ°"""
        if df is None or df.empty:
            return False
        
        try:
            save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    if "æ—¥æœŸ" in existing_df.columns:
                        # åˆå¹¶æ•°æ®å¹¶å»é‡
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                        combined_df = combined_df.sort_values("æ—¥æœŸ")
                        df = combined_df
                except Exception as e:
                    logger.warning(f"åˆå¹¶æ•°æ®å¤±è´¥ï¼Œå°†è¦†ç›–æ–‡ä»¶: {str(e)}")
            
            # ä¿å­˜æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                df.to_csv(temp_file.name, index=False)
            shutil.move(temp_file.name, save_path)
            
            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°: {save_path} ({len(df)}æ¡)")
            return save_path  # è¿”å›æ–‡ä»¶è·¯å¾„
            
        except Exception as e:
            logger.error(f"ä¿å­˜ETF {etf_code} æ•°æ®å¤±è´¥: {str(e)}")
            return None

    def git_add_file(self, file_path):
        """Gitæ·»åŠ å•ä¸ªæ–‡ä»¶"""
        try:
            if not os.path.exists(file_path):
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ·»åŠ : {file_path}")
                return False
            
            result = subprocess.run(
                ['git', 'add', file_path],
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode == 0:
                logger.info(f"âœ… Gitæ·»åŠ æˆåŠŸ: {file_path}")
                return True
            else:
                logger.error(f"âŒ Gitæ·»åŠ å¤±è´¥ {file_path}: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"Gitæ·»åŠ å¼‚å¸¸ {file_path}: {str(e)}")
            return False

    def git_commit_batch(self, batch_files, batch_number):
        """æäº¤æ‰¹æ¬¡æ–‡ä»¶åˆ°Git"""
        try:
            if not batch_files:
                logger.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦æäº¤")
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
            status_result = subprocess.run(
                ['git', 'status', '--porcelain'],
                capture_output=True, text=True, timeout=30
            )
            
            if not status_result.stdout.strip():
                logger.info("æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡æäº¤")
                return True
            
            # æ‰§è¡Œæäº¤
            commit_message = f"è‡ªåŠ¨æ›´æ–°ETFæ—¥çº¿æ•°æ® æ‰¹æ¬¡{batch_number} [skip ci]"
            commit_result = subprocess.run(
                ['git', 'commit', '-m', commit_message],
                capture_output=True, text=True, timeout=30
            )
            
            if commit_result.returncode == 0:
                logger.info(f"âœ… Gitæäº¤æˆåŠŸ: æ‰¹æ¬¡{batch_number}")
                
                # æ‰§è¡Œæ¨é€
                push_result = subprocess.run(
                    ['git', 'push'],
                    capture_output=True, text=True, timeout=60
                )
                
                if push_result.returncode == 0:
                    logger.info(f"âœ… Gitæ¨é€æˆåŠŸ: æ‰¹æ¬¡{batch_number}")
                    return True
                else:
                    logger.error(f"âŒ Gitæ¨é€å¤±è´¥: {push_result.stderr}")
                    return False
            else:
                logger.error(f"âŒ Gitæäº¤å¤±è´¥: {commit_result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"Gitæäº¤å¼‚å¸¸: {str(e)}")
            return False

    def process_successful_etf(self, etf_code, file_path):
        """
        ã€æ ¸å¿ƒé€»è¾‘ã€‘å¤„ç†æˆåŠŸçš„ETF
        - æ·»åŠ åˆ°æš‚å­˜åŒº
        - æˆåŠŸè®¡æ•°å™¨+1
        - æ£€æŸ¥æ˜¯å¦éœ€è¦æäº¤
        """
        # 1. Gitæ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº
        if self.git_add_file(file_path):
            # 2. æ·»åŠ åˆ°æš‚å­˜åŒºåˆ—è¡¨
            self.staged_files.append(file_path)
            
            # 3. æˆåŠŸè®¡æ•°å™¨+1
            self.success_count += 1
            logger.info(f"ğŸ¯ æˆåŠŸè®¡æ•°å™¨: {self.success_count}/{COMMIT_BATCH_SIZE}")
            
            # 4. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æäº¤æ¡ä»¶
            if self.success_count >= COMMIT_BATCH_SIZE:
                logger.info(f"ğŸš€ è¾¾åˆ°æäº¤æ¡ä»¶! æˆåŠŸè®¡æ•°å™¨={self.success_count}ï¼Œå¼€å§‹æäº¤æ‰¹æ¬¡{self.batch_commit_number}")
                
                # æ‰§è¡Œæ‰¹æ¬¡æäº¤
                if self.git_commit_batch(self.staged_files, self.batch_commit_number):
                    logger.info(f"âœ… æ‰¹æ¬¡{self.batch_commit_number}æäº¤æˆåŠŸ!")
                    
                    # 5. é‡ç½®è®¡æ•°å™¨å’Œæš‚å­˜åŒº
                    self.success_count = 0
                    self.staged_files = []
                    self.batch_commit_number += 1
                    
                    return True
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡{self.batch_commit_number}æäº¤å¤±è´¥!")
                    return False
            return True
        else:
            logger.error(f"âŒ ETF {etf_code} Gitæ·»åŠ å¤±è´¥")
            return False

    def get_incremental_date_range(self, etf_code):
        """è·å–å¢é‡æ—¥æœŸèŒƒå›´"""
        try:
            last_trading_day = get_last_trading_day()
            if not isinstance(last_trading_day, datetime):
                last_trading_day = datetime.now()
            
            # è®¾ç½®ç»“æŸæ—¥æœŸ
            end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
            
            # æ£€æŸ¥å†å²æ–‡ä»¶
            save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
            
            if os.path.exists(save_path):
                try:
                    df = pd.read_csv(save_path)
                    if "æ—¥æœŸ" not in df.columns or df.empty:
                        # è·å–ä¸€å¹´æ•°æ®
                        start_date = last_trading_day - timedelta(days=365)
                        return start_date, end_date
                    
                    # è·å–æœ€æ–°æ—¥æœŸ
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                    valid_dates = df["æ—¥æœŸ"].dropna()
                    
                    if valid_dates.empty:
                        start_date = last_trading_day - timedelta(days=365)
                        return start_date, end_date
                    
                    latest_date = valid_dates.max()
                    latest_date_date = latest_date.date()
                    end_date_date = end_date.date()
                    
                    if latest_date_date < end_date_date:
                        # éœ€è¦æ›´æ–°æ•°æ®
                        start_date = latest_date + timedelta(days=1)
                        # ç¡®ä¿æ˜¯äº¤æ˜“æ—¥
                        while not is_trading_day(start_date.date()):
                            start_date += timedelta(days=1)
                        
                        if start_date > end_date:
                            return None, None
                        
                        return start_date, end_date
                    else:
                        return None, None
                        
                except Exception as e:
                    logger.error(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {str(e)}")
                    start_date = last_trading_day - timedelta(days=365)
                    return start_date, end_date
            else:
                # æ–°ETFï¼Œè·å–ä¸€å¹´æ•°æ®
                start_date = last_trading_day - timedelta(days=365)
                return start_date, end_date
                
        except Exception as e:
            logger.error(f"è·å–æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}")
            last_trading_day = get_last_trading_day()
            start_date = last_trading_day - timedelta(days=365)
            end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
            return start_date, end_date

    def get_all_etf_codes(self):
        """è·å–æ‰€æœ‰ETFä»£ç """
        try:
            if not os.path.exists(BASIC_INFO_FILE):
                logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨")
                return []
            
            basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
            if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
                return []
            
            etf_codes = basic_info_df["ETFä»£ç "].tolist()
            logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
            return etf_codes
            
        except Exception as e:
            logger.error(f"è·å–ETFä»£ç å¤±è´¥: {str(e)}")
            return []

    def get_next_crawl_index(self):
        """è·å–ä¸‹ä¸€ä¸ªçˆ¬å–ç´¢å¼•"""
        try:
            if not os.path.exists(BASIC_INFO_FILE):
                return 0
            
            basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
            if basic_info_df.empty:
                return 0
            
            if "next_crawl_index" not in basic_info_df.columns:
                basic_info_df["next_crawl_index"] = 0
                basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
                return 0
            
            next_index = int(basic_info_df["next_crawl_index"].iloc[0])
            return next_index
            
        except Exception as e:
            logger.error(f"è·å–è¿›åº¦å¤±è´¥: {str(e)}")
            return 0

    def save_crawl_progress(self, next_index):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        try:
            if not os.path.exists(BASIC_INFO_FILE):
                return
            
            basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
            if basic_info_df.empty:
                return
            
            if "next_crawl_index" not in basic_info_df.columns:
                basic_info_df["next_crawl_index"] = 0
            
            basic_info_df["next_crawl_index"] = next_index
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            
            logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜: {next_index}/{len(basic_info_df)}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}")

    def commit_remaining_files(self):
        """æäº¤å‰©ä½™æœªæäº¤çš„æ–‡ä»¶"""
        if self.staged_files and self.success_count > 0:
            logger.info(f"ğŸš€ æäº¤å‰©ä½™ {len(self.staged_files)} ä¸ªæ–‡ä»¶ (æˆåŠŸè®¡æ•°å™¨={self.success_count})")
            
            if self.git_commit_batch(self.staged_files, f"æœ€ç»ˆæ‰¹æ¬¡_{self.batch_commit_number}"):
                logger.info("âœ… å‰©ä½™æ–‡ä»¶æäº¤æˆåŠŸ!")
                self.success_count = 0
                self.staged_files = []
                return True
            else:
                logger.error("âŒ å‰©ä½™æ–‡ä»¶æäº¤å¤±è´¥!")
                return False
        else:
            logger.info("æ²¡æœ‰å‰©ä½™æ–‡ä»¶éœ€è¦æäº¤")
            return True

    def crawl_all_etfs_daily_data(self):
        """ä¸»çˆ¬å–å‡½æ•° - ä¸¥æ ¼è®¡æ•°å™¨ç‰ˆæœ¬"""
        try:
            logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å–ï¼ˆä¸¥æ ¼è®¡æ•°å™¨ç‰ˆæœ¬ï¼‰===")
            logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(DAILY_DIR, exist_ok=True)
            
            # è·å–ETFä»£ç åˆ—è¡¨
            etf_codes = self.get_all_etf_codes()
            total_count = len(etf_codes)
            
            if total_count == 0:
                logger.error("ETFåˆ—è¡¨ä¸ºç©º")
                return
            
            logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åª")
            
            # è·å–è¿›åº¦
            next_index = self.get_next_crawl_index()
            logger.info(f"å½“å‰è¿›åº¦ï¼š{next_index}/{total_count}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            start_idx = next_index % total_count
            end_idx = start_idx + BATCH_SIZE
            
            if end_idx <= total_count:
                batch_codes = etf_codes[start_idx:end_idx]
            else:
                batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({len(batch_codes)}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
            
            # ã€æ ¸å¿ƒå¤„ç†å¾ªç¯ã€‘
            processed_count = 0
            for i, etf_code in enumerate(batch_codes):
                # éšæœºå»¶æ—¶
                time.sleep(random.uniform(3, 8))
                
                etf_name = self.get_etf_name(etf_code)
                current_index = (start_idx + i) % total_count
                logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                
                # è·å–æ—¥æœŸèŒƒå›´
                date_range = self.get_incremental_date_range(etf_code)
                if date_range[0] is None:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡")
                    self.skip_count += 1
                    processed_count += 1
                    logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                    continue
                
                start_date, end_date = date_range
                logger.info(f"ğŸ“… çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                
                # çˆ¬å–æ•°æ®
                df = self.crawl_etf_data(etf_code, start_date, end_date)
                
                if df is None or df.empty:
                    logger.warning(f"âš ï¸ ETF {etf_code} æœªè·å–åˆ°æ•°æ®")
                    # è®°å½•å¤±è´¥
                    with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                        f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                    self.fail_count += 1
                    processed_count += 1
                    logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                    continue
                
                # ä¿å­˜æ•°æ®åˆ°æœ¬åœ°
                file_path = self.save_etf_data(etf_code, df)
                if file_path:
                    # ã€å…³é”®æ­¥éª¤ã€‘å¤„ç†æˆåŠŸçš„ETFï¼ˆæ·»åŠ åˆ°æš‚å­˜åŒºå¹¶æ£€æŸ¥æäº¤æ¡ä»¶ï¼‰
                    if self.process_successful_etf(etf_code, file_path):
                        processed_count += 1
                    else:
                        self.fail_count += 1  # Gitæ“ä½œå¤±è´¥ä¹Ÿç®—å¤±è´¥
                        processed_count += 1
                else:
                    self.fail_count += 1
                    processed_count += 1
                
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            # ã€å…³é”®æ­¥éª¤ã€‘æäº¤å‰©ä½™æœªæäº¤çš„æ–‡ä»¶
            logger.info("å¼€å§‹æäº¤å‰©ä½™æœªæäº¤çš„æ–‡ä»¶...")
            self.commit_remaining_files()
            
            # æ›´æ–°è¿›åº¦
            new_index = end_idx % total_count
            self.save_crawl_progress(new_index)
            
            # ã€ä¸¥æ ¼çš„è®¡æ•°å™¨éªŒè¯ã€‘
            total_processed = self.success_count + self.fail_count + self.skip_count
            logger.info("=" * 60)
            logger.info("ğŸ“Š ä¸¥æ ¼çš„è®¡æ•°å™¨ç»Ÿè®¡ç»“æœ:")
            logger.info(f"âœ… æˆåŠŸè®¡æ•°: {self.success_count}")
            logger.info(f"âŒ å¤±è´¥è®¡æ•°: {self.fail_count}") 
            logger.info(f"â­ï¸  è·³è¿‡è®¡æ•°: {self.skip_count}")
            logger.info(f"ğŸ“¦ æ€»å¤„ç†: {total_processed}/{len(batch_codes)}")
            logger.info(f"ğŸ’¾ æäº¤æ‰¹æ¬¡: {self.batch_commit_number - 1}")
            logger.info("=" * 60)
            
            # éªŒè¯è®¡æ•°å™¨æ˜¯å¦æ­£ç¡®
            if total_processed == len(batch_codes):
                logger.info("âœ… è®¡æ•°å™¨éªŒè¯é€šè¿‡!")
            else:
                logger.error(f"âŒ è®¡æ•°å™¨éªŒè¯å¤±è´¥! æœŸæœ›: {len(batch_codes)}, å®é™…: {total_processed}")
            
        except Exception as e:
            logger.error(f"ETFçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            # å°è¯•æäº¤å‰©ä½™æ–‡ä»¶
            try:
                self.commit_remaining_files()
            except:
                pass
            raise

def crawl_all_etfs_daily_data():
    """ä¸»å…¥å£å‡½æ•°"""
    crawler = StrictCounterETFCrawler()
    crawler.crawl_all_etfs_daily_data()

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
