#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - ç”Ÿäº§çº§ä¿®å¤ç‰ˆæœ¬

yFinanceæ•°æ®-etf_daily_crawler-DS13.py

ä¿®å¤é—®é¢˜ï¼š
1. æäº¤é€»è¾‘é”™è¯¯ï¼ˆåªæäº¤ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
2. æ•°æ®è·å–é—®é¢˜ï¼ˆyfinanceè¿”å›ç©ºæ•°æ®ï¼‰
3. æ—¶é—´å¤„ç†é—®é¢˜ï¼ˆæ—¶åŒºé”™è¯¯ï¼‰
4. æ–‡ä»¶ä¿å­˜éªŒè¯
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
import subprocess
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)

# å…³é”®å‚æ•°
BATCH_SIZE = 40
COMMIT_BATCH_SIZE = 10

def get_etf_name(etf_code):
    """è·å–ETFåç§°"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return etf_code
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            return etf_code
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        return etf_row["ETFåç§°"].values[0] if not etf_row.empty else etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
        return etf_code

def get_yfinance_symbol(etf_code):
    """è·å–yfinanceå¯¹åº”çš„symbol"""
    etf_code = str(etf_code).strip()
    if etf_code.startswith(('51', '56', '57', '58')):
        return f"{etf_code}.SS"
    elif etf_code.startswith('15'):
        return f"{etf_code}.SZ"
    else:
        return etf_code

def crawl_etf_data(etf_code, start_date, end_date):
    """çˆ¬å–ETFæ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
    try:
        symbol = get_yfinance_symbol(etf_code)
        logger.info(f"å°è¯•è·å– {etf_code} æ•°æ®ï¼Œsymbol: {symbol}")
        
        # ä½¿ç”¨Tickerå¯¹è±¡è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
        ticker = yf.Ticker(symbol)
        
        # è·å–å†å²æ•°æ®
        df = ticker.history(
            start=start_date.strftime("%Y-%m-%d"),
            end=end_date.strftime("%Y-%m-%d"),
            auto_adjust=False  # ä¸è‡ªåŠ¨è°ƒæ•´ä»·æ ¼
        )
        
        if df is None or df.empty:
            logger.warning(f"yfinanceè¿”å›ç©ºæ•°æ®: {etf_code}")
            return None
        
        logger.info(f"è·å–åˆ° {len(df)} æ¡æ•°æ®ï¼Œåˆ—å: {df.columns.tolist()}")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if df['Close'].isna().all() or (df['Close'] == 0).all():
            logger.warning(f"æ•°æ®å…¨ä¸º0æˆ–ç©º: {etf_code}")
            return None
        
        # åˆ›å»ºç»“æœDataFrame
        result_df = pd.DataFrame()
        result_df['æ—¥æœŸ'] = df.index.strftime('%Y-%m-%d')
        result_df['å¼€ç›˜'] = df['Open'].round(4)
        result_df['æœ€é«˜'] = df['High'].round(4)
        result_df['æœ€ä½'] = df['Low'].round(4)
        result_df['æ”¶ç›˜'] = df['Close'].round(4)
        result_df['æˆäº¤é‡'] = df['Volume'].astype(int)
        
        # è®¡ç®—å…¶ä»–å­—æ®µ
        result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['æœ€ä½'].replace(0, 1) * 100).round(2)
        result_df['æ¶¨è·Œé¢'] = result_df['æ”¶ç›˜'].diff().fillna(0).round(4)
        
        # è®¡ç®—æ¶¨è·Œå¹…
        prev_close = result_df['æ”¶ç›˜'].shift(1)
        result_df['æ¶¨è·Œå¹…'] = (result_df['æ¶¨è·Œé¢'] / prev_close.replace(0, 1) * 100).round(2)
        result_df['æ¶¨è·Œå¹…'] = result_df['æ¶¨è·Œå¹…'].fillna(0)
        
        # è®¡ç®—æˆäº¤é¢
        if 'Dividends' in df.columns:
            result_df['æˆäº¤é¢'] = (df['Close'] * df['Volume']).round(2)
        else:
            result_df['æˆäº¤é¢'] = (result_df['æ”¶ç›˜'] * result_df['æˆäº¤é‡']).round(2)
        
        # å¡«å……å…¶ä»–å­—æ®µ
        result_df['æ¢æ‰‹ç‡'] = 0.0
        result_df['IOPV'] = 0.0
        result_df['æŠ˜ä»·ç‡'] = 0.0
        result_df['æº¢ä»·ç‡'] = 0.0
        
        result_df['ETFä»£ç '] = etf_code
        result_df['ETFåç§°'] = get_etf_name(etf_code)
        result_df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # éªŒè¯æ•°æ®
        sample_close = result_df['æ”¶ç›˜'].iloc[0] if len(result_df) > 0 else 'N/A'
        logger.info(f"æ•°æ®éªŒè¯ - é¦–æ¡æ”¶ç›˜ä»·: {sample_close}, æ•°æ®é‡: {len(result_df)}")
        return result_df
        
    except Exception as e:
        logger.error(f"yfinanceçˆ¬å– {etf_code} å¤±è´¥: {str(e)}", exc_info=True)
        return None

def save_etf_data(etf_code, df):
    """ä¿å­˜ETFæ•°æ®åˆ°æœ¬åœ° - ä¿®å¤ç‰ˆæœ¬"""
    if df is None or df.empty:
        logger.warning(f"ETF {etf_code} æ— æ•°æ®å¯ä¿å­˜")
        return None
    
    try:
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if os.path.exists(save_path):
            try:
                existing_df = pd.read_csv(save_path)
                if not existing_df.empty and "æ—¥æœŸ" in existing_df.columns:
                    # åˆå¹¶æ•°æ®ï¼Œå»é‡
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ")
                    original_count = len(existing_df)
                    new_count = len(combined_df)
                    df = combined_df
                    logger.info(f"æ•°æ®åˆå¹¶: åŸæœ‰ {original_count} æ¡ï¼Œåˆå¹¶å {new_count} æ¡ï¼Œæ–°å¢ {new_count - original_count} æ¡")
                else:
                    logger.info(f"ç°æœ‰æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œè¦†ç›–å†™å…¥")
            except Exception as e:
                logger.warning(f"åˆå¹¶æ•°æ®å¤±è´¥ï¼Œè¦†ç›–å†™å…¥: {str(e)}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿æ•°æ®å®Œæ•´æ€§
        temp_file = None
        try:
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                df.to_csv(temp_file.name, index=False)
            
            # ç§»åŠ¨ä¸´æ—¶æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            shutil.move(temp_file.name, save_path)
            temp_file = None  # é˜²æ­¢åˆ é™¤
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                # è¯»å–éªŒè¯æ–‡ä»¶å†…å®¹
                verify_df = pd.read_csv(save_path)
                actual_count = len(verify_df)
                
                logger.info(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ: {save_path} ({actual_count}æ¡, {file_size} bytes)")
                
                # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                if actual_count != len(df):
                    logger.warning(f"æ•°æ®å®Œæ•´æ€§è­¦å‘Š: é¢„æœŸ {len(df)} æ¡ï¼Œå®é™…ä¿å­˜ {actual_count} æ¡")
                
                return save_path
            else:
                logger.error(f"âŒ æ–‡ä»¶ä¿å­˜éªŒè¯å¤±è´¥: {save_path}")
                return None
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file.name):
                os.unlink(temp_file.name)
            
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return None

def get_incremental_date_range(etf_code):
    """è·å–å¢é‡æ—¥æœŸèŒƒå›´ - ä¿®å¤ç‰ˆæœ¬"""
    try:
        # ä½¿ç”¨naive datetimeé¿å…æ—¶åŒºé—®é¢˜
        end_date = datetime.now()
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                if "æ—¥æœŸ" not in df.columns or df.empty:
                    start_date = end_date - timedelta(days=365)
                    logger.info(f"æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œé‡æ–°çˆ¬å–å…¨å¹´æ•°æ®")
                    return start_date, end_date
                
                # è½¬æ¢æ—¥æœŸåˆ—
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                
                if valid_dates.empty:
                    start_date = end_date - timedelta(days=365)
                    logger.info(f"æ— æœ‰æ•ˆæ—¥æœŸæ•°æ®ï¼Œé‡æ–°çˆ¬å–å…¨å¹´æ•°æ®")
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                if hasattr(latest_date, 'tzinfo') and latest_date.tzinfo is not None:
                    latest_date = latest_date.replace(tzinfo=None)
                
                start_date = latest_date + timedelta(days=1)
                
                if start_date > end_date:
                    logger.info(f"æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
                    return None, None
                
                logger.info(f"å¢é‡æ›´æ–°: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                return start_date, end_date
                
            except Exception as e:
                logger.warning(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥ï¼Œé‡æ–°çˆ¬å–: {str(e)}")
                start_date = end_date - timedelta(days=365)
                return start_date, end_date
        else:
            start_date = end_date - timedelta(days=365)
            logger.info(f"æ–°ETFï¼Œçˆ¬å–å…¨å¹´æ•°æ®")
            return start_date, end_date
            
    except Exception as e:
        logger.error(f"è·å–æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}")
        start_date = datetime.now() - timedelta(days=365)
        end_date = datetime.now()
        return start_date, end_date

def get_all_etf_codes():
    """è·å–æ‰€æœ‰ETFä»£ç """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.error("ETFåŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")
            return []
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåŸºç¡€ä¿¡æ¯æ–‡ä»¶æ ¼å¼é”™è¯¯")
            return []
        etf_codes = basic_info_df["ETFä»£ç "].tolist()
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
        return etf_codes
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç å¤±è´¥: {str(e)}")
        return []

def get_next_crawl_index():
    """è·å–ä¸‹ä¸€ä¸ªçˆ¬å–ç´¢å¼•"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return 0
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty:
            return 0
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            return 0
        return int(basic_info_df["next_crawl_index"].iloc[0])
    except Exception as e:
        logger.error(f"è·å–è¿›åº¦å¤±è´¥: {str(e)}")
        return 0

def save_crawl_progress(next_index):
    """ä¿å­˜çˆ¬å–è¿›åº¦"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty:
            return
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜: {next_index}/{len(basic_info_df)}")
    except Exception as e:
        logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}")

def commit_files_in_batches(file_paths, commit_message):
    """æ‰¹é‡æäº¤æ–‡ä»¶åˆ°Git - ä¿®å¤ç‰ˆæœ¬"""
    if not file_paths:
        logger.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦æäº¤")
        return False
    
    # ç¡®ä¿file_pathsæ˜¯åˆ—è¡¨
    if isinstance(file_paths, str):
        file_paths = [file_paths]
    
    try:
        logger.info(f"å¼€å§‹æäº¤ {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        existing_files = []
        for file_path in file_paths:
            if os.path.exists(file_path):
                existing_files.append(file_path)
            else:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not existing_files:
            logger.error("æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶å¯æäº¤")
            return False
        
        # æ·»åŠ æ–‡ä»¶åˆ°git
        for file_path in existing_files:
            try:
                subprocess.run(['git', 'add', file_path], check=True, capture_output=True, text=True)
                logger.debug(f"å·²æ·»åŠ : {file_path}")
            except subprocess.CalledProcessError as e:
                logger.error(f"æ·»åŠ æ–‡ä»¶å¤±è´¥ {file_path}: {e.stderr}")
                return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
        try:
            result = subprocess.run(['git', 'status', '--porcelain'], check=True, capture_output=True, text=True)
            if not result.stdout.strip():
                logger.info("æ²¡æœ‰å˜æ›´éœ€è¦æäº¤")
                return True
        except subprocess.CalledProcessError as e:
            logger.error(f"æ£€æŸ¥gitçŠ¶æ€å¤±è´¥: {e.stderr}")
            return False
        
        # æäº¤å˜æ›´
        try:
            subprocess.run(['git', 'commit', '-m', commit_message], check=True, capture_output=True, text=True)
            logger.info(f"æäº¤æˆåŠŸ: {commit_message}")
        except subprocess.CalledProcessError as e:
            logger.error(f"æäº¤å¤±è´¥: {e.stderr}")
            return False
        
        # æ¨é€å˜æ›´
        try:
            # å…ˆæ‹‰å–æœ€æ–°æ›´æ”¹
            try:
                subprocess.run(['git', 'pull', '--rebase'], check=True, capture_output=True, text=True)
            except subprocess.CalledProcessError as e:
                logger.warning(f"æ‹‰å–å¤±è´¥ï¼Œç»§ç»­æ¨é€: {e.stderr}")
            
            # æ¨é€æ›´æ”¹
            subprocess.run(['git', 'push'], check=True, capture_output=True, text=True)
            logger.info("æ¨é€æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            logger.error(f"æ¨é€å¤±è´¥: {e.stderr}")
            return False
        
        logger.info(f"âœ… æˆåŠŸæäº¤ {len(existing_files)} ä¸ªæ–‡ä»¶")
        return True
        
    except Exception as e:
        logger.error(f"æäº¤è¿‡ç¨‹å¤±è´¥: {str(e)}")
        return False

def crawl_all_etfs_daily_data():
    """ä¸»çˆ¬å–å‡½æ•° - ç”Ÿäº§çº§ä¿®å¤ç‰ˆæœ¬"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å–ï¼ˆç”Ÿäº§çº§ä¿®å¤ç‰ˆæœ¬ï¼‰===")
        
        # è·å–ETFä»£ç åˆ—è¡¨
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œç»ˆæ­¢æ‰§è¡Œ")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åª")
        
        # è·å–è¿›åº¦
        next_index = get_next_crawl_index()
        logger.info(f"å½“å‰è¿›åº¦ï¼š{next_index}/{total_count}")
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
        
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({len(batch_codes)}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        
        success_count = 0
        fail_count = 0
        skip_count = 0
        batch_files = []  # ç´¯ç§¯æˆåŠŸæ–‡ä»¶çš„åˆ—è¡¨
        batch_number = 1  # æ‰¹æ¬¡ç¼–å·
        
        for i, etf_code in enumerate(batch_codes):
            # éšæœºå»¶è¿Ÿé¿å…è¢«å°
            time.sleep(random.uniform(3, 8))
            
            etf_name = get_etf_name(etf_code)
            current_index = (start_idx + i) % total_count
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–æ—¥æœŸèŒƒå›´
            date_range = get_incremental_date_range(etf_code)
            if date_range[0] is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡")
                skip_count += 1
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                continue
            
            start_date, end_date = date_range
            logger.info(f"ğŸ“… çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            
            # çˆ¬å–æ•°æ®
            df = crawl_etf_data(etf_code, start_date, end_date)
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ ETF {etf_code} æœªè·å–åˆ°æ•°æ®")
                fail_count += 1
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                continue
            
            # ä¿å­˜æ•°æ®åˆ°æœ¬åœ°
            file_path = save_etf_data(etf_code, df)
            if file_path:
                batch_files.append(file_path)
                success_count += 1
                logger.info(f"ğŸ¯ æˆåŠŸè®¡æ•°å™¨: {len(batch_files)}/{COMMIT_BATCH_SIZE}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æäº¤æ¡ä»¶
                if len(batch_files) >= COMMIT_BATCH_SIZE:
                    logger.info(f"ğŸš€ è¾¾åˆ°æäº¤æ¡ä»¶! å¼€å§‹æäº¤æ‰¹æ¬¡{batch_number}ï¼ŒåŒ…å« {len(batch_files)} ä¸ªæ–‡ä»¶")
                    
                    # ä½¿ç”¨æ­£ç¡®çš„æäº¤é€»è¾‘
                    commit_message = f"feat: æ‰¹é‡æäº¤ETFæ•°æ® æ‰¹æ¬¡{batch_number} [åŒ…å« {len(batch_files)} ä¸ªæ–‡ä»¶]"
                    commit_result = commit_files_in_batches(batch_files, commit_message)
                    
                    if commit_result:
                        logger.info(f"âœ… æ‰¹æ¬¡{batch_number}æäº¤æˆåŠŸ! æäº¤äº† {len(batch_files)} ä¸ªæ–‡ä»¶")
                        batch_files = []  # æ¸…ç©ºåˆ—è¡¨
                        batch_number += 1
                    else:
                        logger.error(f"âŒ æ‰¹æ¬¡{batch_number}æäº¤å¤±è´¥!")
            else:
                fail_count += 1
            
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
        
        # æäº¤å‰©ä½™æ–‡ä»¶
        if batch_files:
            logger.info(f"ğŸš€ æäº¤å‰©ä½™ {len(batch_files)} ä¸ªæ–‡ä»¶")
            commit_message = f"feat: æ‰¹é‡æäº¤ETFæ•°æ® æœ€ç»ˆæ‰¹æ¬¡ [åŒ…å« {len(batch_files)} ä¸ªæ–‡ä»¶]"
            commit_result = commit_files_in_batches(batch_files, commit_message)
            if commit_result:
                logger.info("âœ… å‰©ä½™æ–‡ä»¶æäº¤æˆåŠŸ!")
            else:
                logger.error("âŒ å‰©ä½™æ–‡ä»¶æäº¤å¤±è´¥!")
        
        # æ›´æ–°è¿›åº¦
        new_index = end_idx % total_count
        save_crawl_progress(new_index)
        
        # ç»Ÿè®¡ç»“æœ
        total_processed = success_count + fail_count + skip_count
        logger.info("=" * 60)
        logger.info("ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        logger.info(f"âœ… æˆåŠŸ: {success_count}")
        logger.info(f"âŒ å¤±è´¥: {fail_count}") 
        logger.info(f"â­ï¸  è·³è¿‡: {skip_count}")
        logger.info(f"ğŸ“¦ æ€»è®¡: {total_processed}/{len(batch_codes)}")
        logger.info(f"ğŸ’¾ æäº¤æ‰¹æ¬¡: {batch_number - 1}")
        logger.info(f"ğŸ“ˆ è¿›åº¦æ›´æ–°: {next_index} -> {new_index}")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ETFçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        # å°è¯•æäº¤å‰©ä½™æ–‡ä»¶
        try:
            if 'batch_files' in locals() and batch_files:
                logger.info("å°è¯•ç´§æ€¥æäº¤å‰©ä½™æ–‡ä»¶...")
                commit_files_in_batches(batch_files, "feat: ç´§æ€¥æäº¤ETFæ•°æ® [å¼‚å¸¸æ¢å¤]")
        except Exception as commit_error:
            logger.error(f"ç´§æ€¥æäº¤å¤±è´¥: {commit_error}")
        raise

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
        logger.info("ğŸ‰ ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡å®Œæˆ!")
    except Exception as e:
        logger.error(f"âŒ ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except ImportError:
            logger.warning("å¾®ä¿¡æ¨é€æ¨¡å—æœªå®‰è£…")
        except Exception as notify_error:
            logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {notify_error}")
        exit(1)
