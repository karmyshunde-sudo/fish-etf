#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨æŒ‡å®šæ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
"""

import akshare as ak
import pandas as pd
import logging
import os
import time
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day
from utils.file_utils import ensure_dir_exists, get_last_crawl_date
from data_crawler.all_etfs import get_all_etf_codes, get_etf_name
from wechat_push.push import send_wechat_message

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def crawl_etf_daily_data(etf_code: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    ä½¿ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # 1. è·å–åŸºç¡€ä»·æ ¼æ•°æ®
        df = ak.fund_etf_hist_em(
            symbol=etf_code,
            period="daily",
            start_date=start_date,
            end_date=end_date
        )
        
        # 2. æ£€æŸ¥åŸºç¡€æ•°æ®
        if df.empty:
            logger.warning(f"ETF {etf_code} åŸºç¡€æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # 3. è·å–æŠ˜ä»·ç‡
        fund_df = ak.fund_etf_fund_daily_em()
        if not fund_df.empty and "åŸºé‡‘ä»£ç " in fund_df.columns and "æŠ˜ä»·ç‡" in fund_df.columns:
            etf_fund_data = fund_df[fund_df["åŸºé‡‘ä»£ç "] == etf_code]
            if not etf_fund_data.empty:
                # ä»fund_dfæå–æŠ˜ä»·ç‡
                df["æŠ˜ä»·ç‡"] = etf_fund_data["æŠ˜ä»·ç‡"].values[0]
        
        # 4. è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
        df["ETFä»£ç "] = etf_code
        df["ETFåç§°"] = get_etf_name(etf_code)
        df["çˆ¬å–æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 5. ç¡®ä¿åˆ—é¡ºåºä¸ç›®æ ‡ç»“æ„ä¸€è‡´
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'ETFä»£ç ', 'ETFåç§°',
            'çˆ¬å–æ—¶é—´', 'æŠ˜ä»·ç‡'
        ]
        
        # åªä¿ç•™ç›®æ ‡åˆ—
        df = df[[col for col in standard_columns if col in df.columns]]
        
        return df
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """
    ä¿å­˜ETFæ—¥çº¿æ•°æ®
    """
    if df.empty:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    etf_daily_dir = Config.ETFS_DAILY_DIR
    ensure_dir_exists(etf_daily_dir)
    
    # ä¿å­˜åˆ°CSV
    save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
    try:
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
        df.to_csv(temp_file.name, index=False)
        # åŸå­æ›¿æ¢
        shutil.move(temp_file.name, save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
    finally:
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

def crawl_all_etfs_daily_data() -> None:
    """
    çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ®
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        Config.init_dirs()
        etf_daily_dir = Config.ETFS_DAILY_DIR
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {etf_daily_dir}")
        
        # è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
        last_trading_day = get_last_trading_day()
        end_date = last_trading_day.strftime("%Y%m%d")
        
        # è·å–æ‰€æœ‰ETFä»£ç 
        etf_codes = get_all_etf_codes()
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{len(etf_codes)}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
        completed_file = os.path.join(etf_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"è¿›åº¦è®°å½•ä¸­å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–è¿›åº¦è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                completed_codes = set()
        
        # åˆ†æ‰¹çˆ¬å–
        batch_size = Config.CRAWL_BATCH_SIZE
        num_batches = (len(etf_codes) + batch_size - 1) // batch_size
        
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥è·Ÿè¸ªéœ€è¦æäº¤çš„æ–‡ä»¶
        files_to_commit = []
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(etf_codes))
            batch_codes = etf_codes[start_idx:end_idx]
            
            logger.info(f"å¤„ç†ç¬¬ {batch_idx+1}/{num_batches} æ‰¹ ETF ({len(batch_codes)}åª)")
            
            for etf_code in batch_codes:
                etf_name = get_etf_name(etf_code)
                
                # ç¡®å®šçˆ¬å–æ—¶é—´èŒƒå›´ï¼ˆä¸€å¹´ï¼‰
                start_date = (last_trading_day - timedelta(days=365)).strftime("%Y%m%d")
                
                # çˆ¬å–æ•°æ®
                logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                logger.info(f"ğŸ“… çˆ¬å–ä¸€å¹´å†å²æ•°æ®ï¼š{start_date} è‡³ {end_date}")
                
                df = crawl_etf_daily_data(etf_code, start_date, end_date)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
                if df.empty:
                    logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                    # è®°å½•å¤±è´¥æ—¥å¿—
                    with open(os.path.join(etf_daily_dir, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                        f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                    continue
                
                # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
                save_path = os.path.join(etf_daily_dir, f"{etf_code}.csv")
                if os.path.exists(save_path):
                    try:
                        existing_df = pd.read_csv(save_path)
                        
                        # åˆå¹¶æ•°æ®å¹¶å»é‡
                        combined_df = pd.concat([existing_df, df], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                        combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                        
                        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                        combined_df.to_csv(temp_file.name, index=False)
                        # åŸå­æ›¿æ¢
                        shutil.move(temp_file.name, save_path)
                        logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                    finally:
                        if os.path.exists(temp_file.name):
                            os.unlink(temp_file.name)
                else:
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                    try:
                        df.to_csv(temp_file.name, index=False)
                        # åŸå­æ›¿æ¢
                        shutil.move(temp_file.name, save_path)
                        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                    finally:
                        if os.path.exists(temp_file.name):
                            os.unlink(temp_file.name)
                
                # æ ‡è®°ä¸ºå·²å®Œæˆ
                with open(completed_file, "a", encoding="utf-8") as f:
                    f.write(f"{etf_code}\n")
                
                # é™åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(1)
            
            # æ‰¹æ¬¡é—´æš‚åœ
            if batch_idx < num_batches - 1:
                batch_pause_seconds = 2
                logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæš‚åœ {batch_pause_seconds} ç§’...")
                time.sleep(batch_pause_seconds)
    
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """
    è·å–æ‰€æœ‰ETFä»£ç 
    """
    # ä»all_etfsæ¨¡å—è·å–
    from all_etfs import get_all_etf_codes
    return get_all_etf_codes()
