#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - æ‰¹é‡æäº¤ä¿®å¤ç‰ˆ
yFinanceæ•°æ®-etf_daily_crawler-DS10.py
ã€ç¡®ä¿æ¯10ä¸ªæˆåŠŸæ–‡ä»¶ä¸€èµ·æäº¤ï¼Œè€Œä¸æ˜¯åªæäº¤æœ€åä¸€ä¸ªã€‘
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)

# å…³é”®å‚æ•°
BATCH_SIZE = 40
COMMIT_BATCH_SIZE = 10

def get_etf_name(etf_code):
    """è·å–ETFåç§°"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return etf_code
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            return etf_code
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        return etf_row["ETFåç§°"].values[0] if not etf_row.empty else etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
        return etf_code

def get_yfinance_symbol(etf_code):
    """è·å–yfinanceå¯¹åº”çš„symbol"""
    if etf_code.startswith(('51', '56', '57', '58')):
        return f"{etf_code}.SS"
    elif etf_code.startswith('15'):
        return f"{etf_code}.SZ"
    else:
        return etf_code

def crawl_etf_data(etf_code, start_date, end_date):
    """çˆ¬å–ETFæ•°æ®"""
    try:
        symbol = get_yfinance_symbol(etf_code)
        df = yf.download(
            symbol,
            start=start_date.strftime("%Y-%m-%d"),
            end=end_date.strftime("%Y-%m-%d"),
            progress=False,
            auto_adjust=True,
            timeout=30
        )
        
        if df is None or df.empty:
            return None
        
        # å¤„ç†æ•°æ®
        result_df = pd.DataFrame()
        result_df['æ—¥æœŸ'] = df.index.strftime('%Y-%m-%d')
        result_df['å¼€ç›˜'] = df['Open'].astype(float)
        result_df['æœ€é«˜'] = df['High'].astype(float)
        result_df['æœ€ä½'] = df['Low'].astype(float)
        result_df['æ”¶ç›˜'] = df['Close'].astype(float)
        result_df['æˆäº¤é‡'] = df['Volume'].astype(float)
        
        # è®¡ç®—è¡ç”Ÿå­—æ®µ
        result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['æœ€ä½'] * 100).round(2)
        result_df['æ¶¨è·Œé¢'] = result_df['æ”¶ç›˜'].diff().fillna(0)
        
        prev_close = result_df['æ”¶ç›˜'].shift(1)
        valid_prev_close = prev_close.replace(0, float('nan'))
        result_df['æ¶¨è·Œå¹…'] = (result_df['æ¶¨è·Œé¢'] / valid_prev_close * 100).round(2)
        result_df['æ¶¨è·Œå¹…'] = result_df['æ¶¨è·Œå¹…'].fillna(0)
        
        result_df['æˆäº¤é¢'] = (result_df['æ”¶ç›˜'] * result_df['æˆäº¤é‡']).round(2)
        result_df['æ¢æ‰‹ç‡'] = 0.0
        result_df['IOPV'] = 0.0
        result_df['æŠ˜ä»·ç‡'] = 0.0
        result_df['æº¢ä»·ç‡'] = 0.0
        
        result_df['ETFä»£ç '] = etf_code
        result_df['ETFåç§°'] = get_etf_name(etf_code)
        result_df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        return result_df
    except Exception as e:
        logger.error(f"yfinanceçˆ¬å– {etf_code} å¤±è´¥: {str(e)}")
        return None

def save_etf_data(etf_code, df):
    """ä¿å­˜ETFæ•°æ®åˆ°æœ¬åœ°"""
    if df is None or df.empty:
        return None
    
    try:
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if os.path.exists(save_path):
            try:
                existing_df = pd.read_csv(save_path)
                if "æ—¥æœŸ" in existing_df.columns:
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ")
                    df = combined_df
            except Exception as e:
                logger.warning(f"åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")
        
        # ä¿å­˜æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            df.to_csv(temp_file.name, index=False)
        shutil.move(temp_file.name, save_path)
        
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°: {save_path} ({len(df)}æ¡)")
        return save_path
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ•°æ®å¤±è´¥: {str(e)}")
        return None

def get_incremental_date_range(etf_code):
    """è·å–å¢é‡æ—¥æœŸèŒƒå›´"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                if "æ—¥æœŸ" not in df.columns or df.empty:
                    start_date = last_trading_day - timedelta(days=365)
                    return start_date, end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                
                if valid_dates.empty:
                    start_date = last_trading_day - timedelta(days=365)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    if start_date > end_date:
                        return None, None
                    
                    return start_date, end_date
                else:
                    return None, None
            except Exception as e:
                logger.error(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {str(e)}")
                start_date = last_trading_day - timedelta(days=365)
                return start_date, end_date
        else:
            start_date = last_trading_day - timedelta(days=365)
            return start_date, end_date
    except Exception as e:
        logger.error(f"è·å–æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}")
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date

def get_all_etf_codes():
    """è·å–æ‰€æœ‰ETFä»£ç """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return []
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            return []
        etf_codes = basic_info_df["ETFä»£ç "].tolist()
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
        return etf_codes
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç å¤±è´¥: {str(e)}")
        return []

def get_next_crawl_index():
    """è·å–ä¸‹ä¸€ä¸ªçˆ¬å–ç´¢å¼•"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return 0
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty:
            return 0
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            return 0
        return int(basic_info_df["next_crawl_index"].iloc[0])
    except Exception as e:
        logger.error(f"è·å–è¿›åº¦å¤±è´¥: {str(e)}")
        return 0

def save_crawl_progress(next_index):
    """ä¿å­˜çˆ¬å–è¿›åº¦"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty:
            return
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜: {next_index}/{len(basic_info_df)}")
    except Exception as e:
        logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}")

def crawl_all_etfs_daily_data():
    """ä¸»çˆ¬å–å‡½æ•° - æ‰¹é‡æäº¤ä¿®å¤ç‰ˆ"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å–ï¼ˆæ‰¹é‡æäº¤ä¿®å¤ç‰ˆï¼‰===")
        
        # è·å–ETFä»£ç åˆ—è¡¨
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©º")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åª")
        
        # è·å–è¿›åº¦
        next_index = get_next_crawl_index()
        logger.info(f"å½“å‰è¿›åº¦ï¼š{next_index}/{total_count}")
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
        
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({len(batch_codes)}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨åˆ—è¡¨ç´¯ç§¯æˆåŠŸæ–‡ä»¶ï¼Œç¡®ä¿æ‰¹é‡æäº¤
        success_count = 0
        fail_count = 0
        skip_count = 0
        batch_files = []  # ç´¯ç§¯æˆåŠŸæ–‡ä»¶çš„åˆ—è¡¨
        batch_number = 1  # æ‰¹æ¬¡ç¼–å·
        
        for i, etf_code in enumerate(batch_codes):
            time.sleep(random.uniform(3, 8))
            etf_name = get_etf_name(etf_code)
            current_index = (start_idx + i) % total_count
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–æ—¥æœŸèŒƒå›´
            date_range = get_incremental_date_range(etf_code)
            if date_range[0] is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡")
                skip_count += 1
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                continue
            
            start_date, end_date = date_range
            logger.info(f"ğŸ“… çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            
            # çˆ¬å–æ•°æ®
            df = crawl_etf_data(etf_code, start_date, end_date)
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ ETF {etf_code} æœªè·å–åˆ°æ•°æ®")
                fail_count += 1
                logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
                continue
            
            # ä¿å­˜æ•°æ®åˆ°æœ¬åœ°
            file_path = save_etf_data(etf_code, df)
            if file_path:
                # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ åˆ°æ‰¹é‡æ–‡ä»¶åˆ—è¡¨ï¼Œè€Œä¸æ˜¯ç«‹å³æäº¤
                batch_files.append(file_path)
                success_count += 1
                logger.info(f"ğŸ¯ æˆåŠŸè®¡æ•°å™¨: {success_count}/{COMMIT_BATCH_SIZE}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æäº¤æ¡ä»¶
                if success_count >= COMMIT_BATCH_SIZE:
                    logger.info(f"ğŸš€ è¾¾åˆ°æäº¤æ¡ä»¶! å¼€å§‹æäº¤æ‰¹æ¬¡{batch_number}ï¼ŒåŒ…å« {len(batch_files)} ä¸ªæ–‡ä»¶")
                    
                    # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨LAST_FILEå‚æ•°ç¡®ä¿æ‰¹é‡æäº¤
                    commit_message = f"è‡ªåŠ¨æ›´æ–°ETFæ—¥çº¿æ•°æ® æ‰¹æ¬¡{batch_number} [skip ci]"
                    commit_result = commit_files_in_batches("LAST_FILE", commit_message)
                    
                    if commit_result:
                        logger.info(f"âœ… æ‰¹æ¬¡{batch_number}æäº¤æˆåŠŸ! æäº¤äº† {len(batch_files)} ä¸ªæ–‡ä»¶")
                        # é‡ç½®è®¡æ•°å™¨å’Œæ–‡ä»¶åˆ—è¡¨
                        success_count = 0
                        batch_files = []
                        batch_number += 1
                    else:
                        logger.error(f"âŒ æ‰¹æ¬¡{batch_number}æäº¤å¤±è´¥!")
            else:
                fail_count += 1
            
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
        
        # ã€å…³é”®ä¿®å¤ã€‘æäº¤å‰©ä½™æ–‡ä»¶
        if batch_files:
            logger.info(f"ğŸš€ æäº¤å‰©ä½™ {len(batch_files)} ä¸ªæ–‡ä»¶")
            commit_message = f"è‡ªåŠ¨æ›´æ–°ETFæ—¥çº¿æ•°æ® æœ€ç»ˆæ‰¹æ¬¡ [skip ci]"
            commit_result = commit_files_in_batches("LAST_FILE", commit_message)
            if commit_result:
                logger.info("âœ… å‰©ä½™æ–‡ä»¶æäº¤æˆåŠŸ!")
            else:
                logger.error("âŒ å‰©ä½™æ–‡ä»¶æäº¤å¤±è´¥!")
        
        # æ›´æ–°è¿›åº¦
        new_index = end_idx % total_count
        save_crawl_progress(new_index)
        
        # ç»Ÿè®¡ç»“æœ
        total_processed = success_count + fail_count + skip_count
        logger.info("=" * 60)
        logger.info("ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        logger.info(f"âœ… æˆåŠŸ: {success_count}")
        logger.info(f"âŒ å¤±è´¥: {fail_count}") 
        logger.info(f"â­ï¸  è·³è¿‡: {skip_count}")
        logger.info(f"ğŸ“¦ æ€»è®¡: {total_processed}/{len(batch_codes)}")
        logger.info(f"ğŸ’¾ æäº¤æ‰¹æ¬¡: {batch_number - 1}")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ETFçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        # å°è¯•æäº¤å‰©ä½™æ–‡ä»¶
        try:
            if 'batch_files' in locals() and batch_files:
                commit_files_in_batches("LAST_FILE", "ç´§æ€¥æäº¤å‰©ä½™æ–‡ä»¶ [skip ci]")
        except:
            pass
        raise

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
