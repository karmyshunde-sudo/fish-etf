#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å—
ä½¿ç”¨æŒ‡å®šæ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
ã€ç”Ÿäº§çº§å®ç°ã€‘
- ä¸¥æ ¼éµå¾ª"å„å¸å…¶èŒ"åŸåˆ™
- ä¸è‚¡ç¥¨çˆ¬å–ç³»ç»Ÿå®Œå…¨ä¸€è‡´çš„è¿›åº¦ç®¡ç†é€»è¾‘
- ä¸“ä¸šé‡‘èç³»ç»Ÿå¯é æ€§ä¿éšœ
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import akshare as ak
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf_daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘å¯åœ¨æ­¤å¤„ä¿®æ”¹æ¯æ¬¡å¤„ç†çš„ETFæ•°é‡
# ä¸“ä¸šä¿®å¤ï¼šæ‰¹æ¬¡å¤§å°ä½œä¸ºå¯é…ç½®å‚æ•°
BATCH_SIZE = 100  # å¯æ ¹æ®éœ€è¦è°ƒæ•´ä¸º100ã€150ã€200ç­‰
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

def get_etf_name(etf_code):
    """
    è·å–ETFåç§°
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return etf_code
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        if "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            return etf_code
        
        # ç¡®ä¿æ¯”è¾ƒæ—¶æ•°æ®ç±»å‹ä¸€è‡´ï¼ˆéƒ½è½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
        etf_code_str = str(etf_code).strip()
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == etf_code_str]
        
        if not etf_row.empty:
            return etf_row["ETFåç§°"].values[0]
        
        logger.warning(f"ETF {etf_code_str} ä¸åœ¨åˆ—è¡¨ä¸­")
        return etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code

def get_next_crawl_index() -> int:
    """
    è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    Returns:
        int: ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0
        
        # ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°åï¼ˆæ·»åŠ ä¸‹åˆ’çº¿ï¼‰
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½")
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•è·å–è¿›åº¦")
            return 0
        
        # ç¡®ä¿"next_crawl_index"åˆ—å­˜åœ¨
        if "next_crawl_index" not in basic_info_df.columns:
            # æ·»åŠ åˆ—å¹¶åˆå§‹åŒ–
            basic_info_df["next_crawl_index"] = 0
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            if not _verify_git_file_content(BASIC_INFO_FILE):
                logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°æäº¤")
            logger.info("å·²æ·»åŠ next_crawl_indexåˆ—å¹¶åˆå§‹åŒ–ä¸º0")
        
        # è·å–ç¬¬ä¸€ä¸ªETFçš„next_crawl_indexå€¼
        next_index = int(basic_info_df["next_crawl_index"].iloc[0])
        logger.info(f"å½“å‰è¿›åº¦ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
        return next_index
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """
    ä¿å­˜ETFçˆ¬å–è¿›åº¦
    Args:
        next_index: ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
            return
        
        # ç¡®ä¿"next_crawl_index"åˆ—å­˜åœ¨
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        
        # æ›´æ–°æ‰€æœ‰è¡Œçš„next_crawl_indexå€¼
        basic_info_df["next_crawl_index"] = next_index
        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡è¯•æäº¤")
        # æäº¤æ›´æ–°
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(BASIC_INFO_FILE, commit_message)
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜å¹¶æäº¤ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def to_naive_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºnaive datetimeï¼ˆæ— æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: naive datetime
    """
    if dt is None:
        return None
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

def to_aware_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºaware datetimeï¼ˆæœ‰æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: aware datetimeï¼ˆåŒ—äº¬æ—¶åŒºï¼‰
    """
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=Config.BEIJING_TIMEZONE)
    return dt

def to_datetime(date_input):
    """
    ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    Args:
        date_input: æ—¥æœŸè¾“å…¥ï¼Œå¯ä»¥æ˜¯strã€dateã€datetimeç­‰ç±»å‹
    Returns:
        datetime.datetime: ç»Ÿä¸€çš„datetimeç±»å‹
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try:
                return datetime.strptime(date_input, fmt)
            except:
                continue
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_input}")
        return None
    return None

def get_valid_trading_date_range(start_date, end_date):
    """
    è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´ï¼Œç¡®ä¿åªåŒ…å«å†å²äº¤æ˜“æ—¥
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
    
    Returns:
        tuple: (valid_start_date, valid_end_date) - æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´
    """
    # ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    
    if start_date is None or end_date is None:
        logger.error("æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
        return None, None
    
    # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
    now = get_beijing_time()
    # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡ç±»å‹ä¸€è‡´
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(now)
    
    if end_date > now:
        end_date = now
        logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    days_back = 0
    while days_back < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_end_date.date()):
            break
        valid_end_date -= timedelta(days=1)
        days_back += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼Œè¿”å›ç©ºèŒƒå›´
    if days_back >= 30:
        logger.warning(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼ˆä» {end_date.strftime('%Y-%m-%d')} å¼€å§‹ï¼‰")
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    days_forward = 0
    while days_forward < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_start_date.date()):
            break
        valid_start_date += timedelta(days=1)
        days_forward += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ç»“æŸäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹æ—¥
    if days_forward >= 30:
        valid_start_date = valid_end_date
    
    # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰ç±»å‹ä¸€è‡´
    start_naive = to_naive_datetime(valid_start_date)
    end_naive = to_naive_datetime(valid_end_date)
    
    if start_naive > end_naive:
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """
    åŠ è½½ETFæ—¥çº¿æ•°æ®
    """
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.warning(f"ETF {etf_code} æ—¥çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        # è¯»å–CSVæ–‡ä»¶ï¼Œæ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float
            }
        )
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        # æŒ‰æ—¥æœŸæ’åºå¹¶å»é‡
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        # ç§»é™¤æœªæ¥æ—¥æœŸçš„æ•°æ®
        today = datetime.now().strftime("%Y-%m-%d")
        df = df[df["æ—¥æœŸ"] <= today]
        return df
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨AkShareçˆ¬å–ETFæ—¥çº¿æ•°æ®
    """
    try:
        # ç¡®ä¿æ—¥æœŸå‚æ•°æ˜¯datetimeç±»å‹
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºdatetimeç±»å‹")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸå¯¹è±¡æœ‰æ­£ç¡®çš„æ—¶åŒºä¿¡æ¯
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # ç›´æ¥è·å–åŸºç¡€ä»·æ ¼æ•°æ®
        df = ak.fund_etf_hist_em(
            symbol=etf_code,
            period="daily",
            start_date=start_date.strftime("%Y%m%d"),
            end_date=end_date.strftime("%Y%m%d")
        )
        
        # æ£€æŸ¥åŸºç¡€æ•°æ®
        if df is None or df.empty:
            logger.warning(f"ETF {etf_code} åŸºç¡€æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
        
        # è·å–æŠ˜ä»·ç‡
        try:
            fund_df = ak.fund_etf_fund_daily_em()
            if not fund_df.empty and "åŸºé‡‘ä»£ç " in fund_df.columns and "æŠ˜ä»·ç‡" in fund_df.columns:
                etf_fund_data = fund_df[fund_df["åŸºé‡‘ä»£ç "] == etf_code]
                if not etf_fund_data.empty:
                    df["æŠ˜ä»·ç‡"] = etf_fund_data["æŠ˜ä»·ç‡"].values[0]
        except Exception as e:
            logger.warning(f"è·å–ETF {etf_code} æŠ˜ä»·ç‡æ•°æ®å¤±è´¥: {str(e)}")
        
        # è¡¥å……ETFåŸºæœ¬ä¿¡æ¯
        df["ETFä»£ç "] = etf_code
        df["ETFåç§°"] = get_etf_name(etf_code)
        df["çˆ¬å–æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ç¡®ä¿åˆ—é¡ºåº
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'ETFä»£ç ', 'ETFåç§°',
            'çˆ¬å–æ—¶é—´', 'æŠ˜ä»·ç‡'
        ]
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """
    è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´
    ä¸“ä¸šä¿®å¤ï¼šè§£å†³ETFå…¨éƒ¨è·³è¿‡é—®é¢˜
    """
    try:
        # è·å–æœ€è¿‘äº¤æ˜“æ—¥
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        # ç¡®ä¿æ—¶åŒºä¸€è‡´
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # è®¾ç½®ç»“æŸæ—¥æœŸä¸ºæœ€è¿‘äº¤æ˜“æ—¥ï¼ˆç¡®ä¿æ˜¯äº¤æ˜“æ—¥ï¼‰
        end_date = last_trading_day
        
        # è·å–å½“å‰åŒ—äº¬æ—¶é—´
        current_time = get_beijing_time()
        
        # å¦‚æœç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œè°ƒæ•´ä¸ºå½“å‰æ—¶é—´
        if end_date > current_time:
            end_date = current_time
        
        # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿ç»“æŸæ—¥æœŸæ˜¯äº¤æ˜“æ—¥
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
            if (last_trading_day - end_date).days > 30:
                logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥")
                return None, None
        
        # ä¸“ä¸šä¿®å¤ï¼šè®¾ç½®ç»“æŸæ—¶é—´ä¸ºå½“å¤©23:59:59
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        # æ„å»ºETFæ•°æ®æ–‡ä»¶è·¯å¾„
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        
        # æ£€æŸ¥å†å²æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                
                # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    # ä½¿ç”¨é»˜è®¤å›é€€ç­–ç•¥ï¼šè·å–ä¸€å¹´æ•°æ®
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                
                # è·å–æœ€æ–°æœ‰æ•ˆæ—¥æœŸ
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ä¸­æ—¥æœŸåˆ—å…¨ä¸ºNaN")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                
                # ç¡®ä¿latest_dateæ˜¯datetimeç±»å‹å¹¶å¸¦æœ‰æ—¶åŒº
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                # ä¸“ä¸šä¿®å¤ï¼šæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†ï¼ˆå¿½ç•¥æ—¶é—´éƒ¨åˆ†ï¼‰
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                logger.debug(f"ETF {etf_code} æ—¥æœŸæ¯”è¾ƒ: æœ€æ–°æ—¥æœŸ={latest_date_date}, ç»“æŸæ—¥æœŸ={end_date_date}")
                
                # ä¸“ä¸šä¿®å¤ï¼šå¦‚æœæœ€æ–°æ—¥æœŸå°äºç»“æŸæ—¥æœŸï¼Œåˆ™éœ€è¦çˆ¬å–
                if latest_date_date < end_date_date:
                    # ä¸“ä¸šä¿®å¤ï¼šä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥å¼€å§‹
                    start_date = latest_date + timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæ˜¯äº¤æ˜“æ—¥
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    # ç¡®ä¿start_dateæœ‰æ—¶åŒºä¿¡æ¯
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    
                    # ä¸“ä¸šä¿®å¤ï¼šç¡®ä¿start_dateä¸è¶…è¿‡end_date
                    if start_date > end_date:
                        logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ={latest_date_date}ï¼Œç»“æŸæ—¥æœŸ={end_date_date}ï¼‰")
                        return None, None
                    
                    logger.info(f"ETF {etf_code} éœ€è¦æ›´æ–°æ•°æ®: æœ€æ–°æ—¥æœŸ {latest_date_date} < ç»“æŸæ—¥æœŸ {end_date_date}")
                    logger.info(f"ETF {etf_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                    return start_date, end_date
                else:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°: æœ€æ–°æ—¥æœŸ {latest_date_date} >= ç»“æŸæ—¥æœŸ {end_date_date}")
                    return None, None
            
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                # å‡ºé”™æ—¶å°è¯•è·å–ä¸€å¹´æ•°æ®
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                return start_date, end_date
        else:
            logger.info(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œå°†è·å–ä¸€å¹´å†å²æ•°æ®")
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
            return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """
    ä¿å­˜ETFæ—¥çº¿æ•°æ® - ä»…è´Ÿè´£æœ¬åœ°ä¿å­˜ï¼Œä¸å¤„ç†Gitæäº¤
    """
    if df.empty:
        return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(DAILY_DIR, exist_ok=True)
    
    # ä¿å­˜å‰å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if "æ—¥æœŸ" in df.columns:
        df_save = df.copy()
        df_save["æ—¥æœŸ"] = df_save["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
    else:
        df_save = df
    
    # ä¿å­˜åˆ°CSV
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            df_save.to_csv(temp_file.name, index=False)
        shutil.move(temp_file.name, save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def crawl_all_etfs_daily_data() -> None:
    """
    çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ®
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        # è·å–æ‰€æœ‰ETFä»£ç 
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        # è·å–å½“å‰è¿›åº¦
        next_index = get_next_crawl_index()
        
        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
        # ä¸“ä¸šä¿®å¤ï¼šå¾ªç¯æ‰¹å¤„ç†æœºåˆ¶
        # 1. ç¡®å®šå¤„ç†èŒƒå›´ï¼ˆä½¿ç”¨å¾ªç¯å¤„ç†ï¼‰
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        
        # 2. è®¡ç®—å®é™…çš„end_idxï¼ˆç”¨äºè¿›åº¦æ›´æ–°ï¼‰
        actual_end_idx = end_idx % total_count
        
        # 3. è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹ETFï¼ˆä½¿ç”¨å®é™…ç´¢å¼•ï¼‰
        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        
        # 4. å¤„ç†å¾ªç¯æƒ…å†µ
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            # å¾ªç¯å¤„ç†ï¼šç¬¬ä¸€éƒ¨åˆ†ï¼ˆstart_idxåˆ°total_countï¼‰+ ç¬¬äºŒéƒ¨åˆ†ï¼ˆ0åˆ°end_idx-total_countï¼‰
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        # è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹ETF
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        # å¤„ç†è¿™æ‰¹ETF
        processed_count = 0
        for i, etf_code in enumerate(batch_codes):
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1.5, 2.5))
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                # è®°å½•å¤±è´¥æ—¥å¿—
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                continue
            
            # å¤„ç†å·²æœ‰æ•°æ®
            save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    if "æ—¥æœŸ" in existing_df.columns:
                        existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors='coerce')
                    
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                        combined_df.to_csv(temp_file.name, index=False)
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            else:
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    df.to_csv(temp_file.name, index=False)
                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
            
            # ä¸“ä¸šä¿®å¤ï¼šä¸å†æ¯ä¸ªETFéƒ½æ›´æ–°è¿›åº¦
            processed_count += 1
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            # ã€å…³é”®ä¿®å¤ã€‘æ¯å¤„ç†10åªETFå°±è°ƒç”¨git_utilsæäº¤
            if processed_count % 10 == 0:
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæäº¤æ‰¹é‡æ–‡ä»¶...")
                if not force_commit_remaining_files():
                    logger.error("æäº¤æ‰¹é‡æ–‡ä»¶å¤±è´¥")
        
        # ä¸“ä¸šä¿®å¤ï¼šæ•´æ‰¹å¤„ç†å®Œæˆåæ‰æ›´æ–°è¿›åº¦
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„ETF
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count  # é‡ç½®å
        
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‰©ä½™æ–‡ä»¶éƒ½è¢«æäº¤
        logger.info("å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
        if not force_commit_remaining_files():
            logger.error("å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                # å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶
                if not force_commit_remaining_files():
                    logger.error("å¼·åˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """
    è·å–æ‰€æœ‰ETFä»£ç 
    """
    try:
        # ç¡®ä¿ETFåˆ—è¡¨æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(BASIC_INFO_FILE):
            logger.info("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            from data_crawler.all_etfs import update_all_etf_list
            update_all_etf_list()
        
        # è¯»å–æ—¶æŒ‡å®šETFä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return []
        
        # ç¡®ä¿"ETFä»£ç "åˆ—å­˜åœ¨
        if "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘'ETFä»£ç 'åˆ—")
            return []
        
        # ç›´æ¥è·å–ETFä»£ç ï¼ˆå·²ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ï¼‰
        etf_codes = basic_info_df["ETFä»£ç "].tolist()
        
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
        return etf_codes
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²ä¿å­˜
        try:
            next_index = get_next_crawl_index()
            total_count = len(get_all_etf_codes())
            logger.info(f"å½“å‰è¿›åº¦: {next_index}/{total_count}")
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
