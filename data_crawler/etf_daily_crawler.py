#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - ä¼˜åŒ–ç‰ˆ
ã€å…³é”®ä¼˜åŒ–ã€‘
- ç§‘å­¦å»¶æ—¶ç­–ç•¥ï¼šå¢é‡æ›´æ–°0.8-1.5ç§’ï¼Œå…¨é‡2-3ç§’
- åŠ¨æ€å»¶æ—¶ï¼šæ ¹æ®è¯·æ±‚ç»“æœè‡ªåŠ¨è°ƒæ•´
- å¤±è´¥é‡è¯•æœºåˆ¶ï¼šé¿å…ä¸´æ—¶é™æµå¯¼è‡´å¤±è´¥
- ä¸¥æ ¼ä¿æŒæ•°æ®ç»“æ„ä¸å˜
"""

import requests
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
import io
import math
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘ä¼˜åŒ–ç‰ˆ
BATCH_SIZE = 80  # ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜
BASE_DELAY = 0.8  # åŸºç¡€å»¶æ—¶ï¼ˆç§’ï¼‰
DYNAMIC_DELAY = True  # å¯ç”¨åŠ¨æ€å»¶æ—¶
MAX_RETRIES = 3      # æœ€å¤§é‡è¯•æ¬¡æ•°
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

def get_etf_name(etf_code):
    """è·å–ETFåç§°ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            return etf_code
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        return etf_row["ETFåç§°"].values[0] if not etf_row.empty else etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code

def get_etf_fund_size(etf_code: str) -> float:
    """
    ä»ETFåˆ—è¡¨ä¸­è·å–åŸºé‡‘è§„æ¨¡ï¼ˆåªè¯»ï¼‰
    è¿”å›å•ä½ï¼šè‚¡ï¼ˆåŸºé‡‘è§„æ¨¡(äº¿å…ƒ) Ã— 100,000,000ï¼‰
    """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0.0
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        
        # æ£€æŸ¥å¿…è¦åˆ—
        if "ETFä»£ç " not in basic_info_df.columns or "åŸºé‡‘è§„æ¨¡" not in basic_info_df.columns:
            logger.warning(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼ˆETFä»£ç /åŸºé‡‘è§„æ¨¡ï¼‰")
            return 0.0
        
        # è·å–å¯¹åº”ETFçš„åŸºé‡‘è§„æ¨¡
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == str(etf_code).strip()]
        if etf_row.empty:
            logger.warning(f"ETF {etf_code} åœ¨åˆ—è¡¨ä¸­ä¸å­˜åœ¨")
            return 0.0
        
        fund_size = float(etf_row["åŸºé‡‘è§„æ¨¡"].values[0])
        
        # åŸºé‡‘è§„æ¨¡å•ä½è½¬æ¢ï¼šäº¿å…ƒ â†’ è‚¡
        return fund_size * 100000000
    
    except Exception as e:
        logger.error(f"è·å–ETF {etf_code} åŸºé‡‘è§„æ¨¡å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_next_crawl_index() -> int:
    """è·å–è¿›åº¦ç´¢å¼•ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return 0
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        return int(basic_info_df["next_crawl_index"].iloc[0])
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """ä¿å­˜è¿›åº¦ï¼ˆä»…æ›´æ–°è¿›åº¦å­—æ®µï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            return
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_files_in_batches(BASIC_INFO_FILE, commit_message)
    except Exception as e:
        logger.error(f"ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def to_naive_datetime(dt):
    """è½¬æ¢ä¸ºæ— æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=None) if dt.tzinfo else dt

def to_aware_datetime(dt):
    """è½¬æ¢ä¸ºåŒ—äº¬æ—¶åŒºæ—¶é—´"""
    if dt is None: return None
    return dt.replace(tzinfo=Config.BEIJING_TIMEZONE) if dt.tzinfo is None else dt

def to_datetime(date_input):
    """ç»Ÿä¸€æ—¥æœŸè½¬æ¢"""
    if isinstance(date_input, datetime): return date_input
    if isinstance(date_input, str):
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try: return datetime.strptime(date_input, fmt)
            except: continue
    return None

def get_valid_trading_date_range(start_date, end_date):
    """è·å–æœ‰æ•ˆäº¤æ˜“æ—¥èŒƒå›´"""
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    if not start_date or not end_date: return None, None
    
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(get_beijing_time())
    if end_date > now: end_date = now
    
    # æŸ¥æ‰¾æœ‰æ•ˆç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    for _ in range(30):
        if is_trading_day(valid_end_date.date()): break
        valid_end_date -= timedelta(days=1)
    else:
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆèµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    for _ in range(30):
        if is_trading_day(valid_start_date.date()): break
        valid_start_date += timedelta(days=1)
    else:
        valid_start_date = valid_end_date
    
    if to_naive_datetime(valid_start_date) > to_naive_datetime(valid_end_date):
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def load_etf_daily_data(etf_code: str) -> pd.DataFrame:
    """åŠ è½½æœ¬åœ°æ•°æ®"""
    try:
        file_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if not os.path.exists(file_path):
            return pd.DataFrame()
        
        # å®šä¹‰æ ‡å‡†åˆ—å
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        df = pd.read_csv(
            file_path,
            encoding="utf-8",
            dtype={
                "æ—¥æœŸ": str,
                "å¼€ç›˜": float,
                "æœ€é«˜": float,
                "æœ€ä½": float,
                "æ”¶ç›˜": float,
                "æˆäº¤é‡": float,
                "æˆäº¤é¢": float,
                "æŒ¯å¹…": float,
                "æ¶¨è·Œå¹…": float,
                "æ¶¨è·Œé¢": float,
                "æ¢æ‰‹ç‡": float,
                "IOPV": float,
                "æŠ˜ä»·ç‡": float,
                "æº¢ä»·ç‡": float
            }
        )
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦åˆ—å­˜åœ¨
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns):
            return pd.DataFrame()
        
        # ä»…ä¿ç•™æ ‡å‡†åˆ—
        df = df[[col for col in standard_columns if col in df.columns]]
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼
        df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
        df = df.sort_values("æ—¥æœŸ").drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
        today = datetime.now().strftime("%Y-%m-%d")
        return df[df["æ—¥æœŸ"] <= today]
    except Exception as e:
        logger.error(f"åŠ è½½ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€æ ¸å¿ƒä¼˜åŒ–ã€‘åŠ¨æ€å»¶æ—¶ç­–ç•¥
# 1. åŸºç¡€å»¶æ—¶ï¼š0.8-1.5ç§’ï¼ˆå¢é‡æ›´æ–°ï¼‰
# 2. åŠ¨æ€è°ƒæ•´ï¼šæ ¹æ®è¯·æ±‚ç»“æœè‡ªåŠ¨ä¼˜åŒ–
# 3. å¤±è´¥é‡è¯•ï¼šé¿å…ä¸´æ—¶é™æµ
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
class RequestThrottler:
    """
    è¯·æ±‚é™æµå™¨ - åŠ¨æ€è°ƒæ•´è¯·æ±‚é—´éš”
    """
    def __init__(self, base_delay=0.8, max_delay=3.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.current_delay = base_delay
        self.success_count = 0
        self.failure_count = 0
        self.last_request_time = None
    
    def wait(self):
        """ç­‰å¾…é€‚å½“æ—¶é—´å†è¯·æ±‚"""
        if self.last_request_time:
            elapsed = time.time() - self.last_request_time
            if elapsed < self.current_delay:
                time.sleep(self.current_delay - elapsed)
        
        self.last_request_time = time.time()
    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.success_count += 1
        self.failure_count = 0
        
        # æ¯10æ¬¡æˆåŠŸè¯·æ±‚å°è¯•å‡å°‘å»¶æ—¶
        if self.success_count % 10 == 0 and self.current_delay > self.base_delay:
            self.current_delay = max(self.base_delay, self.current_delay - 0.1)
    
    def record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self.failure_count += 1
        self.success_count = 0
        
        # å¤±è´¥3æ¬¡åå¢åŠ å»¶æ—¶
        if self.failure_count >= 3:
            self.current_delay = min(self.max_delay, self.current_delay + 0.5)
            self.failure_count = 0

# åˆå§‹åŒ–å…¨å±€é™æµå™¨
throttler = RequestThrottler(base_delay=BASE_DELAY)

def get_etf_iopv(etf_code: str, date: datetime) -> float:
    """
    è·å–ETFçš„IOPVï¼ˆå®æ—¶å‚è€ƒå‡€å€¼ï¼‰
    """
    try:
        date_str = date.strftime("%Y%m%d")
        logger.debug(f"è·å– {etf_code} åœ¨ {date_str} çš„IOPV")
        
        # æ·±äº¤æ‰€ETF
        if etf_code.startswith('15'):
            url = f"http://www.szse.cn/api/marketdata/v1/etf/realtime?etfCode={etf_code}"
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Referer": "http://www.szse.cn/market/etf/index.html",
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            data = response.json()
            if isinstance(data, list) and len(data) > 0:
                iopv_str = data[0].get('iopv', '')
                return float(iopv_str) if iopv_str else None
        
        # ä¸Šäº¤æ‰€ETF (51/58å¼€å¤´)
        elif etf_code.startswith(('51', '58')):
            url = f"http://www.sse.com.cn/market/etfdata/iopvdata/{etf_code}.csv"
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Referer": "https://www.sse.com.cn/market/etf/iopv/",
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            df = pd.read_csv(io.StringIO(response.text), encoding='gbk')
            if not df.empty:
                iopv_str = df.iloc[-1]['å‚è€ƒå‡€å€¼']
                return float(iopv_str) if iopv_str else None
        
        logger.warning(f"ETF {etf_code} ä¸æ”¯æŒè·å–IOPV")
        return None
    
    except Exception as e:
        logger.warning(f"ETF {etf_code} IOPVè·å–å¤±è´¥: {str(e)}")
        return None

def calculate_additional_fields(df: pd.DataFrame, etf_code: str) -> pd.DataFrame:
    """
    è®¡ç®—æ‰€æœ‰å¿…è¦è¡ç”Ÿå­—æ®µ
    """
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values("æ—¥æœŸ").reset_index(drop=True)
    
    # 1. æŒ¯å¹… = (æœ€é«˜ - æœ€ä½) / æœ€ä½ * 100%
    df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æœ€ä½'] * 100).round(2)
    
    # 2. æ¶¨è·Œé¢ = æ”¶ç›˜ - å‰ä¸€æ—¥æ”¶ç›˜
    df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'].diff().fillna(0)
    
    # 3. æ¶¨è·Œå¹… = æ¶¨è·Œé¢ / å‰ä¸€æ—¥æ”¶ç›˜ * 100%
    df['æ¶¨è·Œå¹…'] = (df['æ¶¨è·Œé¢'] / df['æ”¶ç›˜'].shift(1) * 100).round(2)
    
    # 4. æ¢æ‰‹ç‡ = æˆäº¤é‡ / åŸºé‡‘è§„æ¨¡ï¼ˆä»ETFåˆ—è¡¨è·å–ï¼‰
    fund_size = get_etf_fund_size(etf_code)
    if fund_size > 0:
        df['æ¢æ‰‹ç‡'] = (df['æˆäº¤é‡'] / fund_size * 100).round(2)
    else:
        df['æ¢æ‰‹ç‡'] = 0.0
    
    # 5. æŠ˜ä»·ç‡/æº¢ä»·ç‡ï¼ˆä¸¥æ ¼åŒºåˆ†ï¼‰
    df['æŠ˜ä»·ç‡'] = df.apply(lambda row: 
        round(((row['IOPV'] - row['æ”¶ç›˜']) / row['IOPV'] * 100), 2) 
        if row['IOPV'] > row['æ”¶ç›˜'] else 0, axis=1)
    
    df['æº¢ä»·ç‡'] = df.apply(lambda row: 
        round(((row['æ”¶ç›˜'] - row['IOPV']) / row['IOPV'] * 100), 2) 
        if row['æ”¶ç›˜'] > row['IOPV'] else 0, axis=1)
    
    return df

def crawl_etf_daily_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨äº¤æ˜“æ‰€å®˜æ–¹æ¥å£çˆ¬å–ETFæ—¥çº¿æ•°æ®
    ä¼˜åŒ–ç‚¹ï¼š
      - åŠ¨æ€å»¶æ—¶ç­–ç•¥
      - å¤±è´¥é‡è¯•æœºåˆ¶
      - ä¸¥æ ¼ä¿æŒæ•°æ®ç»“æ„
    """
    try:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯")
            return pd.DataFrame()
        
        # ç»Ÿä¸€æ—¶åŒºå¤„ç†
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        # 1. åˆ¤æ–­äº¤æ˜“æ‰€
        if etf_code.startswith(('51', '58')):
            exchange = 'sse'  # ä¸Šäº¤æ‰€
            url = f"http://www.sse.com.cn/market/etfdata/dailydata/{etf_code}.csv"
            logger.info(f"ETF {etf_code} ä½¿ç”¨ä¸Šäº¤æ‰€æ¥å£: {url}")
        elif etf_code.startswith('15'):
            exchange = 'szse'  # æ·±äº¤æ‰€
            url = f"http://www.szse.cn/api/marketdata/v1/etf?etfCode={etf_code}"
            logger.info(f"ETF {etf_code} ä½¿ç”¨æ·±äº¤æ‰€æ¥å£: {url}")
        else:
            logger.error(f"ETF {etf_code} ä»£ç æ ¼å¼ä¸æ”¯æŒ (é51/58/15å¼€å¤´)")
            return pd.DataFrame()
        
        # 2. æ·»åŠ å¿…è¦è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.sse.com.cn/" if exchange == 'sse' else "http://www.szse.cn/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
        
        # 3. æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        max_retries = MAX_RETRIES
        for retry in range(max_retries):
            try:
                # åŠ¨æ€å»¶æ—¶
                throttler.wait()
                
                response = requests.get(
                    url,
                    headers=headers,
                    timeout=20,
                    verify=True
                )
                response.raise_for_status()
                
                # è®°å½•æˆåŠŸè¯·æ±‚
                throttler.record_success()
                break
                
            except requests.exceptions.RequestException as e:
                # è®°å½•å¤±è´¥è¯·æ±‚
                throttler.record_failure()
                
                # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥
                if retry == max_retries - 1:
                    logger.error(f"ETF {etf_code} æ¥å£è¯·æ±‚å¤±è´¥ (é‡è¯• {max_retries} æ¬¡): {str(e)}")
                    return pd.DataFrame()
                
                # ç­‰å¾…å¹¶é‡è¯•
                wait_time = BASE_DELAY * (2 ** retry) + random.uniform(0.1, 0.5)
                logger.warning(f"ETF {etf_code} è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯•: {str(e)}")
                time.sleep(wait_time)
        
        # 4. å¤„ç†ä¸åŒäº¤æ˜“æ‰€è¿”å›æ ¼å¼
        if exchange == 'sse':
            try:
                df = pd.read_csv(
                    io.StringIO(response.text),
                    encoding='gbk',
                    parse_dates=['æ—¥æœŸ'],
                    date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d')
                )
                df.rename(columns={
                    'å¼€ç›˜ä»·': 'å¼€ç›˜',
                    'æœ€é«˜ä»·': 'æœ€é«˜',
                    'æœ€ä½ä»·': 'æœ€ä½',
                    'æ”¶ç›˜ä»·': 'æ”¶ç›˜',
                    'æˆäº¤é‡(è‚¡)': 'æˆäº¤é‡',
                    'æˆäº¤é‡‘é¢(å…ƒ)': 'æˆäº¤é¢',
                    'å‚è€ƒå‡€å€¼': 'IOPV'
                }, inplace=True)
            except Exception as e:
                logger.error(f"ä¸Šäº¤æ‰€CSVè§£æå¤±è´¥: {str(e)}")
                return pd.DataFrame()
        
        else:  # æ·±äº¤æ‰€
            try:
                data = response.json()
                if not isinstance(data, list) or not data:
                    return pd.DataFrame()
                df = pd.DataFrame(data)
                df.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜',
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜',
                    'volume': 'æˆäº¤é‡',
                    'turnover': 'æˆäº¤é¢',
                    'iopv': 'IOPV'
                }, inplace=True)
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='%Y-%m-%d')
            except Exception as e:
                logger.error(f"æ·±äº¤æ‰€JSONè§£æå¤±è´¥: {str(e)}")
                return pd.DataFrame()
        
        # 5. åŸºç¡€æ•°æ®éªŒè¯
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        if any(col not in df.columns for col in required_columns):
            logger.error(f"ETF {etf_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {df.columns.tolist()}")
            return pd.DataFrame()
        
        # 6. ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´
        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")
        df = df[(df['æ—¥æœŸ'] >= start_str) & (df['æ—¥æœŸ'] <= end_str)]
        
        # 7. è·å–IOPVå¹¶è®¡ç®—æ‰€æœ‰è¡ç”Ÿå­—æ®µ
        if 'IOPV' not in df.columns:
            df['IOPV'] = None
        
        # å¯¹äºæ²¡æœ‰IOPVçš„æ•°æ®è¡Œï¼Œå°è¯•è·å–
        for idx, row in df[df['IOPV'].isna()].iterrows():
            iopv = get_etf_iopv(etf_code, row['æ—¥æœŸ'])
            if iopv is not None:
                df.at[idx, 'IOPV'] = iopv
        
        # è®¡ç®—æ‰€æœ‰è¡ç”Ÿå­—æ®µ
        df = calculate_additional_fields(df, etf_code)
        
        # 8. è¡¥å……å¿…è¦å­—æ®µ
        df['ETFä»£ç '] = etf_code
        df['ETFåç§°'] = get_etf_name(etf_code)
        df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 9. ã€ä¸¥æ ¼ä¿è¯ã€‘å­—æ®µé¡ºåºå®Œå…¨åŒ¹é…æ‚¨çš„è¦æ±‚
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
            'IOPV', 'æŠ˜ä»·ç‡', 'æº¢ä»·ç‡',
            'ETFä»£ç ', 'ETFåç§°', 'çˆ¬å–æ—¶é—´'
        ]
        
        # ä»…ä¿ç•™éœ€è¦çš„åˆ—ï¼ˆç¡®ä¿é¡ºåºä¸€è‡´ï¼‰
        return df[[col for col in standard_columns if col in df.columns]]
    
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """å¢é‡æ—¥æœŸèŒƒå›´ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        current_time = get_beijing_time()
        if end_date > current_time:
            end_date = current_time
        
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
        
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                if "æ—¥æœŸ" not in df.columns:
                    return last_trading_day - timedelta(days=365), end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    return last_trading_day - timedelta(days=365), end_date
                
                latest_date = valid_dates.max()
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    if start_date > end_date:
                        return None, None
                    return start_date, end_date
                return None, None
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                return last_trading_day - timedelta(days=365), end_date
        else:
            return last_trading_day - timedelta(days=365), end_date
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        return last_trading_day - timedelta(days=365), last_trading_day

def save_etf_daily_data(etf_code: str, df: pd.DataFrame) -> None:
    """ä¿å­˜æ•°æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    if df.empty: return
    
    os.makedirs(DAILY_DIR, exist_ok=True)
    save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
    
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            if "æ—¥æœŸ" in df.columns:
                df_save = df.copy()
                df_save["æ—¥æœŸ"] = df_save["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
            else:
                df_save = df
            df_save.to_csv(temp_file.name, index=False)
        shutil.move(temp_file.name, save_path)
        logger.info(f"ETF {etf_code} æ—¥çº¿æ•°æ®å·²ä¿å­˜è‡³ {save_path}ï¼Œå…±{len(df)}æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def crawl_all_etfs_daily_data() -> None:
    """ä¸»çˆ¬å–é€»è¾‘"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        next_index = get_next_crawl_index()
        
        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
        # ä¿æŒåŸæœ‰åˆ†æ‰¹å¤„ç†é€»è¾‘
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        actual_end_idx = end_idx % total_count
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        processed_count = 0
        for i, etf_code in enumerate(batch_codes):
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_daily_data(etf_code, start_date, end_date)
            
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                with open(os.path.join(DAILY_DIR, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®\n")
                continue
            
            save_path = os.path.join(DAILY_DIR, f"{etf_code}.csv")
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    if "æ—¥æœŸ" in existing_df.columns:
                        existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors='coerce')
                    
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                        combined_df.to_csv(temp_file.name, index=False)
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            else:
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    df.to_csv(temp_file.name, index=False)
                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
            
            processed_count += 1
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)")
            
            if processed_count % 10 == 0:
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæäº¤æ‰¹é‡æ–‡ä»¶...")
                if not force_commit_remaining_files():
                    logger.error("æäº¤æ‰¹é‡æ–‡ä»¶å¤±è´¥")
        
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        logger.info(f"è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼Œè¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
        logger.info("å¤„ç†å®Œæˆåï¼Œç¡®ä¿æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
        if not force_commit_remaining_files():
            logger.error("å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                if not force_commit_remaining_files():
                    logger.error("å¼·åˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def get_all_etf_codes() -> list:
    """è·å–ETFä»£ç åˆ—è¡¨ï¼ˆåªè¯»ï¼‰"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return []
        
        basic_info_df = pd.read_csv(BASIC_INFO_FILE, dtype={"ETFä»£ç ": str})
        
        # ä¸¥æ ¼åªè¯» - ç¡®ä¿ä¸ä¿®æ”¹ETFåˆ—è¡¨
        if basic_info_df.empty or "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶æ ¼å¼é”™è¯¯")
            return []
        
        return basic_info_df["ETFä»£ç "].tolist()
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
