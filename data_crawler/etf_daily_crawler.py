#!/usr/bin/env python3 
# -*- coding: utf-8 -*-
"""
ETFæ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - çœŸæ­£æ‰¹é‡ä¿å­˜ç‰ˆæœ¬
yFinanceæ•°æ®-etf_daily_crawler-GPT2.py
"""

import yfinance as yf
import pandas as pd
import logging
import os
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time, get_last_trading_day, is_trading_day
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
# DAILY_DIR = os.path.join(DATA_DIR, "etf", "daily")
DAILY_DIR = os.path.join(DATA_DIR, "etf_daily")  # âœ… æ”¹å›æ—§çš„è·¯å¾„
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_etfs.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 13

def get_etf_name(etf_code):
    """è·å–ETFåç§°"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return etf_code
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return etf_code
        
        if "ETFä»£ç " not in basic_info_df.columns or "ETFåç§°" not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            return etf_code
        
        etf_code_str = str(etf_code).strip()
        etf_row = basic_info_df[basic_info_df["ETFä»£ç "] == etf_code_str]
        
        if not etf_row.empty:
            return etf_row["ETFåç§°"].values[0]
        
        logger.warning(f"ETF {etf_code_str} ä¸åœ¨åˆ—è¡¨ä¸­")
        return etf_code
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}", exc_info=True)
        return etf_code


def get_next_crawl_index() -> int:
    """è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„ETFç´¢å¼•"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0
        
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½")
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•è·å–è¿›åº¦")
            return 0
        
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            if not _verify_git_file_content(BASIC_INFO_FILE):
                logger.warning("ETFåˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°æäº¤")
            logger.info("å·²æ·»åŠ next_crawl_indexåˆ—å¹¶åˆå§‹åŒ–ä¸º0")
        
        next_index = int(basic_info_df["next_crawl_index"].iloc[0])
        logger.info(f"å½“å‰è¿›åº¦ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
        return next_index
    except Exception as e:
        logger.error(f"è·å–ETFè¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0


def save_crawl_progress(next_index: int):
    """ä¿å­˜ETFçˆ¬å–è¿›åº¦ - ä»…ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸æäº¤"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
            return
        
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ETFè¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)


def commit_crawl_progress():
    """æäº¤è¿›åº¦æ–‡ä»¶åˆ°Gitä»“åº“"""
    try:
        commit_message = f"feat: æ›´æ–°ETFçˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        success = commit_files_in_batches(BASIC_INFO_FILE, commit_message)
        if success:
            logger.info("âœ… è¿›åº¦æ–‡ä»¶å·²æäº¤åˆ°Gitä»“åº“")
        else:
            logger.error("âŒ è¿›åº¦æ–‡ä»¶æäº¤å¤±è´¥")
        return success
    except Exception as e:
        logger.error(f"âŒ æäº¤è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return False


def get_all_etf_codes() -> list:
    """è·å–æ‰€æœ‰ETFä»£ç """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.info("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            from data_crawler.all_etfs import update_all_etf_list
            update_all_etf_list()
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ETFä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return []
        
        if "ETFä»£ç " not in basic_info_df.columns:
            logger.error("ETFåˆ—è¡¨æ–‡ä»¶ç¼ºå°‘'ETFä»£ç 'åˆ—")
            return []
        
        etf_codes = basic_info_df["ETFä»£ç "].tolist()
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªETFä»£ç ")
        return etf_codes
    except Exception as e:
        logger.error(f"è·å–ETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []


# âœ… æ–°å¢ï¼šç»Ÿä¸€è§„èŒƒETFæ—¥çº¿æ•°æ®ç»“æ„ä¸ç²¾åº¦
def normalize_etf_df(df: pd.DataFrame, etf_code: str, etf_name: str) -> pd.DataFrame:
    """
    è§„èŒƒETFæ—¥çº¿æ•°æ®ç»“æ„ä¸ç²¾åº¦ï¼Œä½¿å…¶ä¸data/etf//159222.csvä¸€è‡´
    """
    import datetime

    expected_columns = [
        "æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢",
        "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡", "IOPV", "æŠ˜ä»·ç‡", "æº¢ä»·ç‡",
        "ETFä»£ç ", "ETFåç§°", "çˆ¬å–æ—¶é—´"
    ]

    # ç¼ºå°‘åˆ—è‡ªåŠ¨è¡¥0
    for col in expected_columns:
        if col not in df.columns:
            df[col] = 0

    # ç²¾åº¦å¤„ç†
    four_decimals = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡", "IOPV", "æŠ˜ä»·ç‡", "æº¢ä»·ç‡"]
    for col in four_decimals:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").round(4)

    if "æˆäº¤é‡" in df.columns:
        df["æˆäº¤é‡"] = pd.to_numeric(df["æˆäº¤é‡"], errors="coerce").fillna(0).astype(int)

    df["ETFä»£ç "] = etf_code
    df["ETFåç§°"] = etf_name
    df["çˆ¬å–æ—¶é—´"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    df = df[expected_columns]
    df = df.sort_values(by="æ—¥æœŸ", ascending=True)
    return df


def crawl_etf_data(etf_code: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """ä½¿ç”¨yfinanceçˆ¬å–ETFæ—¥çº¿æ•°æ®"""
    try:
        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):
            logger.error(f"ETF {etf_code} æ—¥æœŸå‚æ•°ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºdatetimeç±»å‹")
            return pd.DataFrame()
        
        symbol = etf_code
        if etf_code.startswith(('51', '56', '57', '58')):
            symbol = f"{etf_code}.SS"
        elif etf_code.startswith('15'):
            symbol = f"{etf_code}.SZ"
        else:
            symbol = f"{etf_code}.SZ"
        
        logger.info(f"å°è¯•è·å–ETF {etf_code} æ•°æ®ï¼Œç¬¦å·: {symbol}")
        
        etf_ticker = yf.Ticker(symbol)
        df = etf_ticker.history(
            start=start_date.strftime("%Y-%m-%d"),
            end=end_date.strftime("%Y-%m-%d"),
            auto_adjust=True
        )
        
        if df is None:
            logger.warning(f"ETF {etf_code} è¿”å›æ•°æ®ä¸ºNone")
            return pd.DataFrame()
        
        if df.empty:
            logger.warning(f"ETF {etf_code} è¿”å›æ•°æ®ä¸ºç©º")
            alternative_symbols = []
            if symbol.endswith('.SS'):
                alternative_symbols.append(symbol.replace('.SS', '.SZ'))
            elif symbol.endswith('.SZ'):
                alternative_symbols.append(symbol.replace('.SZ', '.SS'))
            
            for alt_symbol in alternative_symbols:
                logger.info(f"å°è¯•æ›¿ä»£ç¬¦å·: {alt_symbol}")
                try:
                    alt_ticker = yf.Ticker(alt_symbol)
                    df = alt_ticker.history(
                        start=start_date.strftime("%Y-%m-%d"),
                        end=end_date.strftime("%Y-%m-%d"),
                        auto_adjust=True
                    )
                    if not df.empty:
                        symbol = alt_symbol
                        logger.info(f"ä½¿ç”¨æ›¿ä»£ç¬¦å· {alt_symbol} æˆåŠŸè·å–æ•°æ®")
                        break
                except Exception as alt_e:
                    logger.warning(f"æ›¿ä»£ç¬¦å· {alt_symbol} ä¹Ÿå¤±è´¥: {str(alt_e)}")
            
            if df.empty:
                logger.warning(f"ETF {etf_code} æ‰€æœ‰ç¬¦å·å°è¯•å‡å¤±è´¥")
                return pd.DataFrame()
        
        df = df.reset_index()
        
        logger.info(f"ETF {etf_code} å®é™…åˆ—å: {df.columns.tolist()}")
        logger.info(f"ETF {etf_code} æ•°æ®å½¢çŠ¶: {df.shape}")
        
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"ETF {etf_code} ç¼ºå°‘åŸºç¡€åˆ—: {', '.join(missing_columns)}")
            logger.error(f"ç°æœ‰åˆ—: {', '.join(df.columns.tolist())}")
            return pd.DataFrame()
        
        column_mapping = {
            'Date': 'æ—¥æœŸ',
            'Open': 'å¼€ç›˜',
            'High': 'æœ€é«˜', 
            'Low': 'æœ€ä½',
            'Close': 'æ”¶ç›˜',
            'Volume': 'æˆäº¤é‡'
        }
        
        actual_mapping = {k: v for k, v in column_mapping.items() if k in df.columns}
        df = df.rename(columns=actual_mapping)
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.strftime('%Y-%m-%d')
            if df['æ—¥æœŸ'].isnull().any():
                logger.warning(f"ETF {etf_code} æ—¥æœŸåˆ—åŒ…å«æ— æ•ˆæ—¥æœŸï¼Œå·²è¿‡æ»¤")
                df = df.dropna(subset=['æ—¥æœŸ'])
        else:
            logger.error(f"ETF {etf_code} é‡å‘½ååç¼ºå°‘æ—¥æœŸåˆ—")
            return pd.DataFrame()
        
        chinese_required = ['æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']
        chinese_missing = [col for col in chinese_required if col not in df.columns]
        
        if chinese_missing:
            logger.error(f"ETF {etf_code} é‡å‘½ååç¼ºå°‘åˆ—: {', '.join(chinese_missing)}")
            return pd.DataFrame()
        
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'].diff()
        
        prev_close = df['æ”¶ç›˜'].shift(1)
        df['æ¶¨è·Œå¹…'] = (df['æ¶¨è·Œé¢'] / prev_close.replace(0, float('nan')) * 100).round(2)
        df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'].fillna(0)
        
        df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / prev_close.replace(0, float('nan')) * 100).round(2)
        df['æŒ¯å¹…'] = df['æŒ¯å¹…'].fillna(0)
        
        if 'æˆäº¤é¢' not in df.columns:
            df['æˆäº¤é¢'] = (df['æ”¶ç›˜'] * df['æˆäº¤é‡']).round(2)
        
        df['æ¢æ‰‹ç‡'] = 0.0
        df['ETFä»£ç '] = etf_code
        df['ETFåç§°'] = get_etf_name(etf_code)
        df['çˆ¬å–æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        df['æŠ˜ä»·ç‡'] = 0.0
        
        standard_columns = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡', 'æˆäº¤é¢',
            'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'ETFä»£ç ', 'ETFåç§°',
            'çˆ¬å–æ—¶é—´', 'æŠ˜ä»·ç‡'
        ]
        
        final_columns = [col for col in standard_columns if col in df.columns]
        df = df[final_columns]
        
        logger.info(f"ETF {etf_code} æˆåŠŸå¤„ç† {len(df)} æ¡æ•°æ®")
        return df
        
    except Exception as e:
        logger.error(f"ETF {etf_code} æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_incremental_date_range(etf_code: str) -> (datetime, datetime):
    """è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        current_time = get_beijing_time()
        
        if end_date > current_time:
            end_date = current_time
        
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
            if (last_trading_day - end_date).days > 30:
                logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥")
                return None, None
        
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        save_path = os.path.join(_DIR, f"{etf_code}.csv")
        
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    logger.warning(f"ETF {etf_code} æ•°æ®æ–‡ä»¶ä¸­æ—¥æœŸåˆ—å…¨ä¸ºNaN")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                logger.debug(f"ETF {etf_code} æ—¥æœŸæ¯”è¾ƒ: æœ€æ–°æ—¥æœŸ={latest_date_date}, ç»“æŸæ—¥æœŸ={end_date_date}")
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    
                    if start_date > end_date:
                        logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ={latest_date_date}ï¼Œç»“æŸæ—¥æœŸ={end_date_date}ï¼‰")
                        return None, None
                    
                    logger.info(f"ETF {etf_code} éœ€è¦æ›´æ–°æ•°æ®: æœ€æ–°æ—¥æœŸ {latest_date_date} < ç»“æŸæ—¥æœŸ {end_date_date}")
                    logger.info(f"ETF {etf_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                    return start_date, end_date
                else:
                    logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°: æœ€æ–°æ—¥æœŸ {latest_date_date} >= ç»“æŸæ—¥æœŸ {end_date_date}")
                    return None, None
            
            except Exception as e:
                logger.error(f"è¯»å–ETF {etf_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                return start_date, end_date
        else:
            logger.info(f"ETF {etf_code} æ— å†å²æ•°æ®ï¼Œå°†è·å–ä¸€å¹´å†å²æ•°æ®")
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
            return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date


def save_etf_data_batch(etf_data_dict: dict) -> int:
    """
    æ‰¹é‡ä¿å­˜ETFæ—¥çº¿æ•°æ® - çœŸæ­£æ‰¹é‡ä¿å­˜ç‰ˆæœ¬
    """
    if not etf_data_dict:
        return 0

    os.makedirs(DAILY_DIR, exist_ok=True)
    saved_count = 0

    for etf_code, df in etf_data_dict.items():
        if df.empty:
            continue

        save_path = os.path.join(_DIR, f"{etf_code}.csv")

        # âœ… æ–°å¢ï¼šä¿å­˜å‰è§„èŒƒåŒ–æ•°æ®ç»“æ„ä¸ç²¾åº¦
        etf_name = df["ETFåç§°"].iloc[0] if "ETFåç§°" in df.columns else get_etf_name(etf_code)
        df = normalize_etf_df(df, etf_code, etf_name)

        try:
            if os.path.exists(save_path):
                existing_df = pd.read_csv(save_path)
                if "æ—¥æœŸ" in existing_df.columns:
                    existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors="coerce")
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=True)

                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    combined_df.to_csv(temp_file.name, index=False)
                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²åˆå¹¶è‡³: {save_path} (å…±{len(combined_df)}æ¡)")
            else:
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    df.to_csv(temp_file.name, index=False)
                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")

            saved_count += 1
        except Exception as e:
            logger.error(f"ä¿å­˜ETF {etf_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

    return saved_count


def crawl_all_etfs_daily_data() -> None:
    """çˆ¬å–æ‰€æœ‰ETFæ—¥çº¿æ•°æ® - çœŸæ­£æ‰¹é‡ä¿å­˜ç‰ˆæœ¬"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡ŒETFæ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        etf_codes = get_all_etf_codes()
        total_count = len(etf_codes)
        
        if total_count == 0:
            logger.error("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–ETFæ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºETFï¼‰")
        
        next_index = get_next_crawl_index()
        
        # è®¡ç®—å¤„ç†èŒƒå›´
        start_idx = next_index % total_count
        end_idx = start_idx + BATCH_SIZE
        actual_end_idx = end_idx % total_count
        
        first_stock_idx = start_idx % total_count
        last_stock_idx = (end_idx - 1) % total_count
        
        if end_idx <= total_count:
            batch_codes = etf_codes[start_idx:end_idx]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        else:
            batch_codes = etf_codes[start_idx:total_count] + etf_codes[0:end_idx-total_count]
            logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
        
        first_stock = f"{etf_codes[first_stock_idx]} - {get_etf_name(etf_codes[first_stock_idx])}" if first_stock_idx < len(etf_codes) else "N/A"
        last_stock = f"{etf_codes[last_stock_idx]} - {get_etf_name(etf_codes[last_stock_idx])}" if last_stock_idx < len(etf_codes) else "N/A"
        logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªETF: {first_stock} (ç´¢å¼• {first_stock_idx})")
        logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªETF: {last_stock} (ç´¢å¼• {last_stock_idx})")
        
        # ã€å…³é”®ä¿®æ”¹ã€‘å…ˆæ”¶é›†æ‰€æœ‰æ•°æ®ï¼Œæœ€åæ‰¹é‡ä¿å­˜
        etf_data_dict = {}  # ç”¨äºå­˜å‚¨æ‰€æœ‰ETFæ•°æ®çš„å­—å…¸
        processed_count = 0
        successful_count = 0
        failed_etfs = []
        
        for i, etf_code in enumerate(batch_codes):
            time.sleep(random.uniform(2, 5))
            etf_name = get_etf_name(etf_code)
            logger.info(f"ETFä»£ç ï¼š{etf_code}| åç§°ï¼š{etf_name}")
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(etf_code)
            if start_date is None or end_date is None:
                logger.info(f"ETF {etf_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                processed_count += 1
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = crawl_etf_data(etf_code, start_date, end_date)
            
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                failed_etfs.append(f"{etf_code},{etf_name},æœªè·å–åˆ°æ•°æ®")
                processed_count += 1
                continue
            
            # ã€å…³é”®ä¿®æ”¹ã€‘å°†æ•°æ®å­˜å…¥å­—å…¸ï¼Œè€Œä¸æ˜¯ç«‹å³ä¿å­˜
            etf_data_dict[etf_code] = df
            successful_count += 1
            processed_count += 1
            
            current_index = (start_idx + i) % total_count
            logger.info(f"è¿›åº¦: {current_index}/{total_count} ({(current_index)/total_count*100:.1f}%) - æ•°æ®å·²ç¼“å­˜")
        
        # ã€å…³é”®ä¿®æ”¹ã€‘æ‰€æœ‰ETFå¤„ç†å®Œæˆåï¼Œä¸€æ¬¡æ€§æ‰¹é‡ä¿å­˜æ‰€æœ‰æ•°æ®
        logger.info(f"å¼€å§‹æ‰¹é‡ä¿å­˜ {len(etf_data_dict)} ä¸ªETFçš„æ•°æ®æ–‡ä»¶...")
        saved_count = save_etf_data_batch(etf_data_dict)
        logger.info(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {saved_count} ä¸ªETFæ•°æ®æ–‡ä»¶")

        # âœ… æ–°å¢ï¼šç¡®ä¿æ‰€æœ‰æ•°æ®æ–‡ä»¶è¢«æš‚å­˜
        # os.system("git add data/etf/daily/*.csv")
        os.system("git add data/etf_daily/*.csv")
        
        # ç„¶åæäº¤æ‰€æœ‰æ•°æ®æ–‡ä»¶åˆ°Git
        logger.info("å¼€å§‹æäº¤æ•°æ®æ–‡ä»¶åˆ°Gitä»“åº“...")
        commit_success = force_commit_remaining_files()
        if commit_success:
            logger.info(f"âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶æäº¤æˆåŠŸï¼Œå…± {saved_count} ä¸ªæ–‡ä»¶")
        else:
            logger.error("âŒ æ•°æ®æ–‡ä»¶æäº¤å¤±è´¥")
        
        # ç„¶åæ›´æ–°å¹¶æäº¤è¿›åº¦æ–‡ä»¶
        new_index = actual_end_idx
        save_crawl_progress(new_index)
        progress_commit_success = commit_crawl_progress()
        
        if progress_commit_success:
            logger.info(f"âœ… è¿›åº¦æ–‡ä»¶æäº¤æˆåŠŸï¼Œè¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
        else:
            logger.error("âŒ è¿›åº¦æ–‡ä»¶æäº¤å¤±è´¥")
        
        # è®°å½•å¤±è´¥çš„ETF
        if failed_etfs:
            failed_file = os.path.join(DAILY_DIR, "failed_etfs.txt")
            with open(failed_file, "w", encoding="utf-8") as f:
                f.write("\n".join(failed_etfs))
            logger.info(f"è®°å½•äº† {len(failed_etfs)} åªå¤±è´¥çš„ETF")
        
        remaining_stocks = total_count - new_index
        if remaining_stocks < 0:
            remaining_stocks = total_count
        
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETFï¼ŒæˆåŠŸ {successful_count} åªï¼Œå¤±è´¥ {len(failed_etfs)} åª")
        logger.info(f"è¿˜æœ‰ {remaining_stocks} åªETFå¾…çˆ¬å–")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # å¼‚å¸¸æƒ…å†µä¸‹å°è¯•ä¿å­˜è¿›åº¦
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(next_index)
                commit_crawl_progress()
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

if __name__ == "__main__":
    try:
        crawl_all_etfs_daily_data()
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"ETFæ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
        # ç¡®ä¿è¿›åº¦æ–‡ä»¶å·²ä¿å­˜
        try:
            next_index = get_next_crawl_index()
            total_count = len(get_all_etf_codes())
            logger.info(f"å½“å‰è¿›åº¦: {next_index}/{total_count}")
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
