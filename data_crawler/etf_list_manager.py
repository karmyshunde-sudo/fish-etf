import akshare as ak
import pandas as pd
import os
import logging
import requests
from datetime import datetime, timezone, timedelta
from utils.date_utils import get_beijing_time
from utils.file_utils import init_dirs
from retrying import retry
from config import Config

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# åˆ—è¡¨æ›´æ–°é¢‘ç‡ï¼ˆå¤©ï¼‰
LIST_UPDATE_INTERVAL = 7

def load_all_etf_list():
    """åŠ è½½å…¨å¸‚åœºETFåˆ—è¡¨"""
    return update_all_etf_list()

def is_list_need_update():
    """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°å…¨å¸‚åœºETFåˆ—è¡¨ï¼ˆä¿®å¤æ—¶åŒºè®¡ç®—ï¼‰"""
    if not os.path.exists(Config.ALL_ETFS_PATH):
        return True
    # è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´ï¼ˆè½¬æ¢ä¸ºä¸œå…«åŒºæ—¶åŒºï¼‰
    last_modify_time = datetime.fromtimestamp(os.path.getmtime(Config.ALL_ETFS_PATH))
    last_modify_time = last_modify_time.replace(tzinfo=timezone.utc).astimezone(timezone(timedelta(hours=8)))
    # è®¡ç®—è·ç¦»ä¸Šæ¬¡æ›´æ–°çš„å¤©æ•°
    days_since_update = (get_beijing_time() - last_modify_time).days
    return days_since_update >= LIST_UPDATE_INTERVAL

@retry(stop_max_attempt_number=3, wait_fixed=2000)
def fetch_all_etfs_akshare():
    """ä½¿ç”¨AkShareæ¥å£è·å–ETFåˆ—è¡¨"""
    try:
        # è°ƒç”¨fund_etf_spot_emæ¥å£
        etf_info = ak.fund_etf_spot_em()
        
        # æ ‡å‡†åŒ–åˆ—å
        etf_list = etf_info.rename(columns={
            "ä»£ç ": "ETFä»£ç ",
            "åç§°": "ETFåç§°",
            "ä¸Šå¸‚æ—¥æœŸ": "ä¸Šå¸‚æ—¥æœŸ"  # æ·»åŠ ä¸Šå¸‚æ—¥æœŸåˆ—
        })
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰æ ‡å‡†åˆ—å’Œä¸Šå¸‚æ—¥æœŸåˆ—
        all_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
        etf_list = etf_list[all_columns]
        
        # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿ä»£ç ä¸º6ä½æ•°å­—
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
        etf_list = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
        
        return etf_list.drop_duplicates(subset="ETFä»£ç ")
    except Exception as e:
        error_msg = f"AkShareæ¥å£é”™è¯¯: {str(e)}"
        logger.warning(f"âš ï¸ {error_msg}")
        raise Exception(error_msg)

@retry(stop_max_attempt_number=3, wait_fixed=2000)
def fetch_all_etfs_sina():
    """æ–°æµªæ¥å£å…œåº•è·å–ETFåˆ—è¡¨"""
    try:
        url = "https://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getETFList"
        params = {"page": 1, "num": 1000, "sort": "symbol", "asc": 1}
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        etf_data = response.json() if response.text.startswith("[") else eval(response.text)
        etf_list = pd.DataFrame(etf_data)[["symbol", "name"]]
        
        etf_list = etf_list.rename(columns={
            "symbol": "ETFä»£ç ",
            "name": "ETFåç§°"
        })
        
        # æ·»åŠ ç©ºç™½çš„ä¸Šå¸‚æ—¥æœŸåˆ—ï¼ˆæ–°æµªæ¥å£ä¸æä¾›æ­¤ä¿¡æ¯ï¼‰
        etf_list["ä¸Šå¸‚æ—¥æœŸ"] = ""
        etf_list = etf_list[Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]]
        
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].str[-6:].str.strip()
        
        return etf_list.drop_duplicates(subset="ETFä»£ç ")
    except Exception as e:
        error_msg = f"æ–°æµªæ¥å£é”™è¯¯: {str(e)}"
        logger.warning(f"âš ï¸ {error_msg}")
        raise Exception(error_msg)

def read_csv_with_encoding(file_path):
    """è¯»å–CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å…¼å®¹UTF-8å’ŒGBKç¼–ç """
    encodings = ["utf-8", "gbk", "latin-1"]
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
            for col in required_columns:
                if col not in df.columns:
                    df[col] = ""
            return df[required_columns].copy()
        except (UnicodeDecodeError, LookupError, KeyError) as e:
            continue
    raise Exception(f"æ— æ³•è§£ææ–‡ä»¶ {file_path}ï¼Œå°è¯•äº†ç¼–ç : {encodings}")

def update_all_etf_list():
    """æ›´æ–°å…¨å¸‚åœºETFåˆ—è¡¨ï¼ˆä¸‰çº§é™çº§ç­–ç•¥ï¼‰"""
    Config.init_dirs()
    primary_etf_list = None
    
    if is_list_need_update():
        logger.info("ğŸ” å°è¯•æ›´æ–°å…¨å¸‚åœºETFåˆ—è¡¨...")
        
        # 1. å°è¯•AkShareæ¥å£
        try:
            etf_list = fetch_all_etfs_akshare()
            # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
            for col in required_columns:
                if col not in etf_list.columns:
                    etf_list[col] = ""
            etf_list = etf_list[required_columns]
            
            etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
            logger.info(f"âœ… AkShareæ›´æ–°æˆåŠŸï¼ˆ{len(etf_list)}åªETFï¼‰")
            primary_etf_list = etf_list
        except Exception as e:
            logger.error(f"âŒ AkShareæ›´æ–°å¤±è´¥: {str(e)}")
        
        # 2. å°è¯•æ–°æµªæ¥å£ï¼ˆä»…å½“AkShareå¤±è´¥æ—¶ï¼‰
        if primary_etf_list is None:
            try:
                etf_list = fetch_all_etfs_sina()
                # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                required_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
                for col in required_columns:
                    if col not in etf_list.columns:
                        etf_list[col] = ""
                etf_list = etf_list[required_columns]
                
                etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
                logger.info(f"âœ… æ–°æµªæ¥å£æ›´æ–°æˆåŠŸï¼ˆ{len(etf_list)}åªETFï¼‰")
                primary_etf_list = etf_list
            except Exception as e:
                logger.error(f"âŒ æ–°æµªæ¥å£æ›´æ–°å¤±è´¥: {str(e)}")
        
        # æ–°å¢é€»è¾‘ï¼šç¬¬ä¸€æ¬¡åˆå§‹åŒ–æ—¶åŒæ­¥è¡¥å……å…œåº•æ–‡ä»¶
        backup_file_exists = os.path.exists(Config.BACKUP_ETFS_PATH)
        backup_file_empty = False
        if backup_file_exists:
            backup_file_empty = os.path.getsize(Config.BACKUP_ETFS_PATH) == 0
        
        if not backup_file_exists or backup_file_empty:
            logger.info("ğŸ”„ æ£€æµ‹åˆ°å…œåº•æ–‡ä»¶æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åŒæ­¥æ•°æ®...")
            
            if primary_etf_list is not None and not primary_etf_list.empty:
                backup_df = primary_etf_list.copy()
                backup_df.to_csv(Config.BACKUP_ETFS_PATH, index=False, encoding="utf-8")
                logger.info(f"âœ… å·²ä»æ–°è·å–æ•°æ®åŒæ­¥å…œåº•æ–‡ä»¶ï¼ˆ{len(backup_df)}æ¡è®°å½•ï¼‰")
            
            elif os.path.exists(Config.ALL_ETFS_PATH) and os.path.getsize(Config.ALL_ETFS_PATH) > 0:
                try:
                    all_etfs_df = read_csv_with_encoding(Config.ALL_ETFS_PATH)
                    all_etfs_df.to_csv(Config.BACKUP_ETFS_PATH, index=False, encoding="utf-8")
                    logger.info(f"âœ… å·²ä»æœ¬åœ°all_etfs.csvåŒæ­¥å…œåº•æ–‡ä»¶ï¼ˆ{len(all_etfs_df)}æ¡è®°å½•ï¼‰")
                except Exception as e:
                    logger.error(f"âŒ ä»all_etfs.csvåŒæ­¥å…œåº•æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # 3. å°è¯•å…œåº•æ–‡ä»¶ï¼ˆå¦‚æœä¸»æ•°æ®æºéƒ½å¤±è´¥ï¼‰
        if primary_etf_list is None:
            if os.path.exists(Config.BACKUP_ETFS_PATH):
                try:
                    backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                    
                    # éªŒè¯å¿…è¦åˆ—
                    required_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
                    for col in required_columns:
                        if col not in backup_df.columns:
                            backup_df[col] = ""
                    
                    # æ•°æ®æ¸…æ´—
                    backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                    backup_df = backup_df[required_columns].drop_duplicates()
                    
                    logger.info(f"âœ… å…œåº•æ–‡ä»¶åŠ è½½æˆåŠŸï¼ˆ{len(backup_df)}åªETFï¼‰")
                    return backup_df
                except Exception as e:
                    logger.error(f"âŒ å…œåº•æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                    # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
                    empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"])
                    return empty_df
            else:
                logger.error(f"âŒ å…œåº•æ–‡ä»¶ä¸å­˜åœ¨: {Config.BACKUP_ETFS_PATH}")
                # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
                empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"])
                return empty_df
        
        return primary_etf_list
    
    else:
        logger.info("â„¹ï¸ æ— éœ€æ›´æ–°ï¼ŒåŠ è½½æœ¬åœ°ETFåˆ—è¡¨")
        try:
            etf_list = read_csv_with_encoding(Config.ALL_ETFS_PATH)
            # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"]
            for col in required_columns:
                if col not in etf_list.columns:
                    etf_list[col] = ""
            return etf_list[required_columns]
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
            empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS + ["ä¸Šå¸‚æ—¥æœŸ"])
            return empty_df

def get_filtered_etf_codes():
    """è·å–è¿‡æ»¤åçš„æœ‰æ•ˆETFä»£ç åˆ—è¡¨"""
    etf_list = update_all_etf_list()
    if etf_list.empty:
        logger.warning("âš ï¸ æ— æœ‰æ•ˆETFä»£ç åˆ—è¡¨")
        return []
    
    # ç¡®ä¿ETFä»£ç ä¸ºå­—ç¬¦ä¸²ç±»å‹
    etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip()
    valid_codes = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]["ETFä»£ç "].tolist()
    logger.info(f"ğŸ“Š æœ‰æ•ˆETFä»£ç æ•°é‡: {len(valid_codes)}")
    return valid_codes
