import os
import akshare as ak
import pandas as pd
import logging
import requests
import time
import json
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any
from retrying import retry
from config import Config
from utils.date_utils import is_file_outdated, get_beijing_time  # ç¡®ä¿å¯¼å…¥get_beijing_time

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# ç¼“å­˜å˜é‡ï¼Œé¿å…é‡å¤åŠ è½½
_etf_list_cache = None
_last_load_time = None

def load_all_etf_list() -> pd.DataFrame:
    """åŠ è½½å…¨å¸‚åœºETFåˆ—è¡¨ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åŠ è½½
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame
    """
    global _etf_list_cache, _last_load_time
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ5åˆ†é’Ÿå†…ï¼‰
    if (_etf_list_cache is not None and 
        _last_load_time is not None and 
        (datetime.now() - _last_load_time).total_seconds() < 300):
        logger.debug("ä½¿ç”¨ç¼“å­˜çš„ETFåˆ—è¡¨")
        return _etf_list_cache.copy()
    
    # æ›´æ–°ETFåˆ—è¡¨
    _etf_list_cache = update_all_etf_list()
    _last_load_time = datetime.now()
    return _etf_list_cache.copy() if _etf_list_cache is not None else pd.DataFrame()

def update_all_etf_list() -> pd.DataFrame:
    """æ›´æ–°ETFåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œè‹¥éœ€æ›´æ–°åˆ™ä»ç½‘ç»œè·å–ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame
    """
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not os.path.exists(Config.ALL_ETFS_PATH) or is_file_outdated(Config.ALL_ETFS_PATH, Config.ETF_LIST_UPDATE_INTERVAL):
        logger.info("ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œå°è¯•ä»ç½‘ç»œè·å–...")
        try:
            primary_etf_list = None
            
            # 1. å°è¯•AkShareæ¥å£
            logger.info("å°è¯•ä»AkShareè·å–ETFåˆ—è¡¨...")
            primary_etf_list = fetch_all_etfs_akshare()
            
            if not primary_etf_list.empty:
                # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
                primary_etf_list = enrich_etf_data(primary_etf_list)
                
                # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                required_columns = Config.ETF_STANDARD_COLUMNS
                for col in required_columns:
                    if col not in primary_etf_list.columns:
                        primary_etf_list[col] = ""
                primary_etf_list = primary_etf_list[required_columns]
                # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
                primary_etf_list = primary_etf_list.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                primary_etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8-sig")
                logger.info(f"âœ… AkShareæ›´æ–°æˆåŠŸï¼ˆ{len(primary_etf_list)}åªETFï¼‰")
                # æ ‡è®°æ•°æ®æ¥æº
                primary_etf_list.source = "AkShare"
            else:
                logger.warning("AkShareè¿”å›ç©ºçš„ETFåˆ—è¡¨")
            
            # 2. å¦‚æœAkShareå¤±è´¥ï¼Œå°è¯•æ–°æµªæ¥å£
            if primary_etf_list is None or primary_etf_list.empty:
                logger.info("å°è¯•ä»æ–°æµªè·å–ETFåˆ—è¡¨...")
                primary_etf_list = fetch_all_etfs_sina()
                
                if not primary_etf_list.empty:
                    # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
                    primary_etf_list = enrich_etf_data(primary_etf_list)
                    
                    # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in primary_etf_list.columns:
                            primary_etf_list[col] = ""
                    primary_etf_list = primary_etf_list[required_columns]
                    primary_etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8-sig")
                    logger.info(f"âœ… æ–°æµªæ¥å£æ›´æ–°æˆåŠŸï¼ˆ{len(primary_etf_list)}åªETFï¼‰")
                    # æ ‡è®°æ•°æ®æ¥æº
                    primary_etf_list.source = "æ–°æµª"
                else:
                    logger.warning("æ–°æµªæ¥å£è¿”å›ç©ºçš„ETFåˆ—è¡¨")
            
            # 3. å¦‚æœå‰ä¸¤è€…éƒ½å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–‡ä»¶
            if primary_etf_list is None or primary_etf_list.empty:
                logger.info("å°è¯•åŠ è½½å…œåº•ETFåˆ—è¡¨æ–‡ä»¶...")
                if os.path.exists(Config.BACKUP_ETFS_PATH):
                    try:
                        backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                        # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
                        backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                        backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                        # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
                        backup_df = enrich_etf_data(backup_df)
                        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                        required_columns = Config.ETF_STANDARD_COLUMNS
                        for col in required_columns:
                            if col not in backup_df.columns:
                                backup_df[col] = ""
                        backup_df = backup_df[required_columns].drop_duplicates()
                        # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
                        backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                        logger.info(f"âœ… å…œåº•æ–‡ä»¶åŠ è½½æˆåŠŸï¼ˆ{len(backup_df)}åªETFï¼‰")
                        # æ ‡è®°æ•°æ®æ¥æº
                        backup_df.source = "å…œåº•æ–‡ä»¶"
                        
                        # ä¿å­˜å…œåº•æ–‡ä»¶ä¸ºå½“å‰ETFåˆ—è¡¨
                        backup_df.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8-sig")
                        return backup_df
                    except Exception as e:
                        logger.error(f"âŒ å…œåº•æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                
                # å¦‚æœå…œåº•æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨æˆ–å¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
                logger.error("âŒ æ— æ³•è·å–ETFåˆ—è¡¨ï¼Œæ‰€æœ‰æ•°æ®æºå‡å¤±è´¥")
                empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
                # æ ‡è®°æ•°æ®æ¥æº
                empty_df.source = "æ— æ•°æ®æº"
                return empty_df
            
            return primary_etf_list
        
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ETFåˆ—è¡¨æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            # å°è¯•åŠ è½½å…œåº•æ–‡ä»¶ä½œä¸ºæœ€åæ‰‹æ®µ
            if os.path.exists(Config.BACKUP_ETFS_PATH):
                try:
                    backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                    backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                    # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
                    backup_df = enrich_etf_data(backup_df)
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in backup_df.columns:
                            backup_df[col] = ""
                    backup_df = backup_df[required_columns].drop_duplicates()
                    backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                    logger.warning("âš ï¸ ä½¿ç”¨å…œåº•æ–‡ä»¶ä½œä¸ºæœ€åæ‰‹æ®µ")
                    # æ ‡è®°æ•°æ®æ¥æº
                    backup_df.source = "å…œåº•æ–‡ä»¶(å¼‚å¸¸)"
                    return backup_df
                except Exception as e:
                    logger.error(f"âŒ å…œåº•æ–‡ä»¶åŠ è½½ä¹Ÿå¤±è´¥: {str(e)}")
            
            # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
            empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
            # æ ‡è®°æ•°æ®æ¥æº
            empty_df.source = "æ— æ•°æ®æº(å¼‚å¸¸)"
            return empty_df
    
    else:
        logger.info("â„¹ï¸ æ— éœ€æ›´æ–°ï¼ŒåŠ è½½æœ¬åœ°ETFåˆ—è¡¨")
        try:
            etf_list = read_csv_with_encoding(Config.ALL_ETFS_PATH)
            # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
            etf_list = enrich_etf_data(etf_list)
            # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS
            for col in required_columns:
                if col not in etf_list.columns:
                    etf_list[col] = ""
            # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
            etf_list = etf_list.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
            # æ ‡è®°æ•°æ®æ¥æº
            etf_list.source = "æœ¬åœ°ç¼“å­˜"
            return etf_list
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            # å°è¯•åŠ è½½å…œåº•æ–‡ä»¶
            if os.path.exists(Config.BACKUP_ETFS_PATH):
                try:
                    backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                    backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                    # è¡¥å……å®Œæ•´ä»£ç ã€ä¸Šå¸‚æ—¥æœŸå’Œå…¶ä»–ä¿¡æ¯
                    backup_df = enrich_etf_data(backup_df)
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in backup_df.columns:
                            backup_df[col] = ""
                    backup_df = backup_df[required_columns].drop_duplicates()
                    backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                    logger.warning("âš ï¸ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–‡ä»¶")
                    # æ ‡è®°æ•°æ®æ¥æº
                    backup_df.source = "å…œåº•æ–‡ä»¶(æœ¬åœ°åŠ è½½å¤±è´¥)"
                    return backup_df
                except Exception as e:
                    logger.error(f"âŒ å…œåº•æ–‡ä»¶åŠ è½½ä¹Ÿå¤±è´¥: {str(e)}")
            
            # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
            empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
            # æ ‡è®°æ•°æ®æ¥æº
            empty_df.source = "æ— æ•°æ®æº(æœ¬åœ°åŠ è½½å¤±è´¥)"
            return empty_df

def retry_if_network_error(exception: Exception) -> bool:
    """é‡è¯•æ¡ä»¶ï¼šç½‘ç»œç›¸å…³é”™è¯¯
    :param exception: å¼‚å¸¸å¯¹è±¡
    :return: å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False"""
    return isinstance(exception, (requests.RequestException, ConnectionError, TimeoutError))

@retry(stop_max_attempt_number=3,
       wait_exponential_multiplier=1000,
       wait_exponential_max=10000,
       retry_on_exception=retry_if_network_error)

def fetch_all_etfs_akshare() -> pd.DataFrame:
    """ä½¿ç”¨AkShareæ¥å£è·å–ETFåˆ—è¡¨ï¼ˆå¸¦è§„æ¨¡å’Œæˆäº¤é¢ç­›é€‰ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame"""
    try:
        logger.info("å°è¯•ä»AkShareè·å–ETFåˆ—è¡¨...")
        # ä½¿ç”¨æ­£ç¡®çš„æ¥å£ï¼šfund_etf_spot_em
        etf_info = ak.fund_etf_spot_em()
        if etf_info.empty:
            logger.warning("AkShareè¿”å›ç©ºçš„ETFåˆ—è¡¨")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.debug(f"AkShareè¿”å›åˆ—å: {list(etf_info.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—åæ˜ å°„
        column_mapping = {
            "ä»£ç ": "ETFä»£ç ",
            "åç§°": "ETFåç§°",
            "æœ€æ–°ä»·": "æœ€æ–°ä»·æ ¼",
            "IOPVå®æ—¶ä¼°å€¼": "IOPV",
            "åŸºé‡‘æŠ˜ä»·ç‡": "æŠ˜æº¢ä»·ç‡",
            "æˆäº¤é‡": "æˆäº¤é‡",
            "æˆäº¤é¢": "æˆäº¤é¢",
            "æ¶¨è·Œå¹…": "æ¶¨è·Œå¹…",
            "æ¶¨è·Œé¢": "æ¶¨è·Œé¢",
            "æ¢æ‰‹ç‡": "æ¢æ‰‹ç‡",
            "æ›´æ–°æ—¶é—´": "æ›´æ–°æ—¶é—´",
            "è§„æ¨¡": "åŸºé‡‘è§„æ¨¡"  # æ˜ç¡®æ˜ å°„åŸºé‡‘è§„æ¨¡
        }
        
        # é‡å‘½ååˆ—
        etf_info = etf_info.rename(columns=column_mapping)
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
        required_columns = Config.ETF_STANDARD_COLUMNS
        for col in required_columns:
            if col not in etf_info.columns:
                etf_info[col] = ""
        
        # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿ä»£ç ä¸º6ä½æ•°å­—
        etf_info["ETFä»£ç "] = etf_info["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
        valid_etfs = etf_info[etf_info["ETFä»£ç "].str.match(r'^\d{6}$', na=False)].copy()
        
        # å¤„ç†åŸºé‡‘è§„æ¨¡ï¼šæå–æ•°å­—éƒ¨åˆ†å¹¶è½¬æ¢ä¸ºæ•°å€¼
        if "åŸºé‡‘è§„æ¨¡" in valid_etfs.columns:
            # æå–æ•°å­—éƒ¨åˆ†
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"].astype(str).str.extract('([0-9.]+)', expand=False)
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = pd.to_numeric(valid_etfs["åŸºé‡‘è§„æ¨¡"], errors="coerce")
            
            # === å…³é”®ä¿®å¤ï¼šç¡®ä¿åŸºé‡‘è§„æ¨¡å•ä½ä¸º"äº¿å…ƒ" ===
            # AkShare fund_etf_spot_em æ¥å£è¿”å›çš„è§„æ¨¡å•ä½æ˜¯"äº¿å…ƒ"ï¼Œæ— éœ€è½¬æ¢
            # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæ·»åŠ å•ä½éªŒè¯
            if not valid_etfs.empty and valid_etfs["åŸºé‡‘è§„æ¨¡"].mean() > 1000:
                # å¦‚æœå¹³å‡è§„æ¨¡å¤§äº1000ï¼Œå¯èƒ½æ˜¯"ä¸‡å…ƒ"å•ä½ï¼Œéœ€è¦è½¬æ¢
                logger.warning("æ£€æµ‹åˆ°åŸºé‡‘è§„æ¨¡å¯èƒ½ä¸º'ä¸‡å…ƒ'å•ä½ï¼Œè¿›è¡Œå•ä½è½¬æ¢")
                valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"] / 10000
            elif not valid_etfs.empty and valid_etfs["åŸºé‡‘è§„æ¨¡"].mean() > 100000000:
                # å¦‚æœå¹³å‡è§„æ¨¡å¤§äº1äº¿ï¼Œå¯èƒ½æ˜¯"å…ƒ"å•ä½ï¼Œéœ€è¦è½¬æ¢
                logger.warning("æ£€æµ‹åˆ°åŸºé‡‘è§„æ¨¡å¯èƒ½ä¸º'å…ƒ'å•ä½ï¼Œè¿›è¡Œå•ä½è½¬æ¢")
                valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"] / 100000000
            # ======================================
        
        # å¤„ç†æˆäº¤é¢ï¼šå‡è®¾åŸå§‹æ•°æ®å•ä½æ˜¯"å…ƒ"ï¼Œè½¬æ¢ä¸º"ä¸‡å…ƒ"
        if "æˆäº¤é¢" in valid_etfs.columns:
            valid_etfs["æˆäº¤é¢"] = pd.to_numeric(valid_etfs["æˆäº¤é¢"], errors="coerce") / 10000
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
        for col in required_columns:
            if col not in valid_etfs.columns:
                valid_etfs[col] = ""
        
        valid_etfs = valid_etfs[required_columns]
        logger.info(f"AkShareè·å–åˆ°{len(etf_info)}åªETFï¼Œç­›é€‰åå‰©ä½™{len(valid_etfs)}åª")
        return valid_etfs.drop_duplicates(subset="ETFä»£ç ")
    
    except Exception as e:
        error_msg = f"AkShareæ¥å£é”™è¯¯: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return pd.DataFrame()  # è¿”å›ç©ºDataFrameä½†ä¸æŠ›å‡ºå¼‚å¸¸

@retry(stop_max_attempt_number=3,
       wait_exponential_multiplier=1000,
       wait_exponential_max=10000,
       retry_on_exception=retry_if_network_error)

@retry(stop_max_attempt_number=3,
       wait_exponential_multiplier=1000,
       wait_exponential_max=10000,
       retry_on_exception=retry_if_network_error)
def fetch_all_etfs_sina() -> pd.DataFrame:
    """æ–°æµªæ¥å£å…œåº•è·å–ETFåˆ—è¡¨ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame"""
    try:
        logger.info("å°è¯•ä»æ–°æµªè·å–ETFåˆ—è¡¨...")
        url = "https://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getETFList"
        params = {"page": 1, "num": 1000, "sort": "symbol", "asc": 1}
        response = requests.get(url, params=params, timeout=Config.REQUEST_TIMEOUT)
        response.raise_for_status()
        
        # å¤„ç†æ–°æµªæ¥å£è¿”å›çš„æ•°æ®
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            etf_data = response.json()
        except ValueError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•å¤„ç†å¯èƒ½çš„JavaScriptæ ¼å¼
            try:
                # ç§»é™¤å¯èƒ½çš„JavaScriptå‰ç¼€
                etf_data_str = response.text.replace('var data=', '').strip(';')
                # å°è¯•è§£æä¸ºJSON
                etf_data = json.loads(etf_data_str)
            except Exception as e:
                logger.error(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•eval: {str(e)}")
                try:
                    # ä½œä¸ºæœ€åæ‰‹æ®µä½¿ç”¨eval
                    etf_data = eval(etf_data_str)
                except Exception as e:
                    logger.error(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®æ ¼å¼æ— æ³•è§£æ: {str(e)}")
                    return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(etf_data, dict):
            # å°è¯•ä»å¸¸è§å­—æ®µä¸­æå–æ•°æ®
            if 'data' in etf_data and isinstance(etf_data['data'], list):
                etf_data = etf_data['data']
            elif 'list' in etf_data and isinstance(etf_data['list'], list):
                etf_data = etf_data['list']
            elif 'result' in etf_data and 'data' in etf_data['result'] and isinstance(etf_data['result']['data'], list):
                etf_data = etf_data['result']['data']
            else:
                logger.warning("æ–°æµªæ¥å£è¿”å›çš„æ˜¯å­—å…¸ä½†æ²¡æœ‰é¢„æœŸçš„æ•°æ®ç»“æ„")
                return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # ç¡®ä¿etf_dataæ˜¯åˆ—è¡¨
        if not isinstance(etf_data, list):
            logger.error(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(etf_data)}")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # åˆ›å»ºDataFrame
        if not etf_data:
            logger.warning("æ–°æµªæ¥å£è¿”å›ç©ºåˆ—è¡¨")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        etf_list = pd.DataFrame(etf_data)
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['symbol', 'name']
        missing_columns = [col for col in required_columns if col not in etf_list.columns]
        if missing_columns:
            logger.warning(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # å°è¯•ä»å…¶ä»–åˆ—åæ˜ å°„
            column_mapping = {}
            if 'symbol' in missing_columns and 'code' in etf_list.columns:
                column_mapping['code'] = 'symbol'
            if 'name' in missing_columns and 'name' in etf_list.columns:
                column_mapping['name'] = 'name'
            
            if column_mapping:
                etf_list = etf_list.rename(columns=column_mapping)
                missing_columns = [col for col in required_columns if col not in etf_list.columns]
            
            if missing_columns:
                logger.error(f"æ— æ³•ä¿®å¤ç¼ºå¤±çš„åˆ—: {', '.join(missing_columns)}")
                return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # é‡å‘½ååˆ—
        etf_list = etf_list.rename(columns={
            "symbol": "ETFä»£ç ",
            "name": "ETFåç§°",
            "date": "ä¸Šå¸‚æ—¥æœŸ",
            "amount": "åŸºé‡‘è§„æ¨¡"  # æ˜ç¡®æ˜ å°„åŸºé‡‘è§„æ¨¡
        })
        
        # ç¡®ä¿ETFä»£ç ä¸º6ä½æ•°å­—
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„6ä½æ•°å­—ETFä»£ç 
        valid_etfs = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$', na=False)].copy()
        
        if valid_etfs.empty:
            logger.warning("æå–åæ— æœ‰æ•ˆETFä»£ç ")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # === å…³é”®ä¿®å¤ï¼šç¡®ä¿åŸºé‡‘è§„æ¨¡å•ä½ä¸º"äº¿å…ƒ" ===
        if "åŸºé‡‘è§„æ¨¡" in valid_etfs.columns:
            # æå–æ•°å­—éƒ¨åˆ†
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"].astype(str).str.extract('([0-9.]+)', expand=False)
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = pd.to_numeric(valid_etfs["åŸºé‡‘è§„æ¨¡"], errors="coerce")
            
            # æ–°æµªæ¥å£è¿”å›çš„è§„æ¨¡å•ä½é€šå¸¸æ˜¯"ä¸‡å…ƒ"ï¼Œéœ€è¦è½¬æ¢ä¸º"äº¿å…ƒ"
            if not valid_etfs.empty and valid_etfs["åŸºé‡‘è§„æ¨¡"].mean() > 10:
                logger.info(f"æ–°æµªæ¥å£è¿”å›çš„åŸºé‡‘è§„æ¨¡å•ä½ä¸º'ä¸‡å…ƒ'ï¼Œè½¬æ¢ä¸º'äº¿å…ƒ'ï¼ˆå¹³å‡è§„æ¨¡: {valid_etfs['åŸºé‡‘è§„æ¨¡'].mean():.2f}ä¸‡å…ƒï¼‰")
                valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"] / 10000
            else:
                logger.info("æ–°æµªæ¥å£è¿”å›çš„åŸºé‡‘è§„æ¨¡å•ä½ä¸º'äº¿å…ƒ'ï¼Œæ— éœ€è½¬æ¢")
        # ======================================
        
        # ç¡®ä¿ä¸Šå¸‚æ—¥æœŸæ ¼å¼æ­£ç¡®
        if "ä¸Šå¸‚æ—¥æœŸ" in valid_etfs.columns:
            # å¤„ç†å¯èƒ½çš„æ—¥æœŸæ ¼å¼ï¼Œç¡®ä¿æ˜¯YYYY-MM-DD
            valid_etfs["ä¸Šå¸‚æ—¥æœŸ"] = pd.to_datetime(valid_etfs["ä¸Šå¸‚æ—¥æœŸ"], errors="coerce").dt.strftime("%Y-%m-%d")
            # å¤„ç†NaTå€¼
            valid_etfs["ä¸Šå¸‚æ—¥æœŸ"] = valid_etfs["ä¸Šå¸‚æ—¥æœŸ"].fillna("")
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
        for col in Config.ETF_STANDARD_COLUMNS:
            if col not in valid_etfs.columns:
                valid_etfs[col] = ""
        
        valid_etfs = valid_etfs[Config.ETF_STANDARD_COLUMNS]
        # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
        valid_etfs = valid_etfs.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
        
        logger.info(f"âœ… æ–°æµªæ¥å£æˆåŠŸè·å–{len(valid_etfs)}åªETF")
        return valid_etfs.drop_duplicates(subset="ETFä»£ç ")
    
    except Exception as e:
        error_msg = f"âŒ æ–°æµªæ¥å£é”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)

def enrich_etf_data(df: pd.DataFrame) -> pd.DataFrame:
    """è¡¥å……å®Œæ•´ä»£ç å’Œä¸Šå¸‚æ—¥æœŸï¼Œå¹¶æ·»åŠ å…¶ä»–æœ‰ç”¨ä¿¡æ¯"""
    try:
        # è¡¥å……å®Œæ•´ä»£ç 
        def get_full_code(code: str) -> str:
            code = str(code).strip().zfill(6)
            if code.startswith("51"):
                return f"sh{code}"
            elif code.startswith("5") or code.startswith("159"):
                return f"sz{code}"
            return code  # ä¿æŒåŸæ ·ï¼Œå¯èƒ½æ˜¯ä¸€äº›ç‰¹æ®Šä»£ç 
        
        df["å®Œæ•´ä»£ç "] = df["ETFä»£ç "].apply(get_full_code)
        
        # æ·»åŠ é¢å¤–åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        additional_columns = [
            "åŸºé‡‘ç±»å‹", "æˆç«‹æ—¥æœŸ", "åŸºé‡‘è§„æ¨¡ï¼ˆæœ€æ–°ï¼‰", "åŸºé‡‘ç®¡ç†äºº", 
            "åŸºé‡‘æ‰˜ç®¡äºº", "è·Ÿè¸ªæ ‡çš„", "ä¸šç»©æ¯”è¾ƒåŸºå‡†", "å•ä½å‡€å€¼", 
            "ç´¯è®¡å‡€å€¼", "è¿‘ 1 æœˆæ¶¨å¹…", "è¿‘ 3 æœˆæ¶¨å¹…", "è¿‘ 6 æœˆæ¶¨å¹…", 
            "è¿‘ 1 å¹´æ¶¨å¹…", "æˆç«‹ä»¥æ¥æ¶¨å¹…"
        ]
        for col in additional_columns:
            if col not in df.columns:
                df[col] = ""
        
        # è¡¥å……é¢å¤–ä¿¡æ¯
        logger.info("æ­£åœ¨è¡¥å……ETFé¢å¤–ä¿¡æ¯...")
        for idx, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            full_code = row["å®Œæ•´ä»£ç "]  # ä½¿ç”¨å®Œæ•´ä»£ç 
            
            try:
                # å°†å®Œæ•´ä»£ç è½¬æ¢ä¸ºAkShareæ¥å£éœ€è¦çš„æ ¼å¼ï¼ˆç§»é™¤ç‚¹å·ï¼‰
                # ä» "sh.510300" è½¬æ¢ä¸º "sh510300"ï¼ˆ8ä½ä¸å¸¦ç‚¹å·ï¼‰
                akshare_code = full_code.replace(".", "")
                
                # è·å–ETFåŸºæœ¬ä¿¡æ¯ - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
                info = ak.fund_etf_fund_info_em(fund=akshare_code)
                if not info.empty:
                    # æå–æˆç«‹æ—¥æœŸä½œä¸ºä¸Šå¸‚æ—¥æœŸ
                    listing_date = info["æˆç«‹æ—¥æœŸ"].iloc[0]
                    df.at[idx, "ä¸Šå¸‚æ—¥æœŸ"] = listing_date
                    
                    # æå–å…¶ä»–ä¿¡æ¯
                    df.at[idx, "åŸºé‡‘ç±»å‹"] = info["åŸºé‡‘ç±»å‹"].iloc[0]
                    df.at[idx, "åŸºé‡‘è§„æ¨¡ï¼ˆæœ€æ–°ï¼‰"] = info["åŸºé‡‘è§„æ¨¡ï¼ˆæœ€æ–°ï¼‰"].iloc[0]
                    df.at[idx, "åŸºé‡‘ç®¡ç†äºº"] = info["åŸºé‡‘ç®¡ç†äºº"].iloc[0]
                    df.at[idx, "åŸºé‡‘æ‰˜ç®¡äºº"] = info["åŸºé‡‘æ‰˜ç®¡äºº"].iloc[0]
                    df.at[idx, "è·Ÿè¸ªæ ‡çš„"] = info["è·Ÿè¸ªæ ‡çš„"].iloc[0]
                    df.at[idx, "ä¸šç»©æ¯”è¾ƒåŸºå‡†"] = info["ä¸šç»©æ¯”è¾ƒåŸºå‡†"].iloc[0]
                    df.at[idx, "å•ä½å‡€å€¼"] = info["å•ä½å‡€å€¼"].iloc[0]
                    df.at[idx, "ç´¯è®¡å‡€å€¼"] = info["ç´¯è®¡å‡€å€¼"].iloc[0]
                    df.at[idx, "è¿‘ 1 æœˆæ¶¨å¹…"] = info["è¿‘ 1 æœˆæ¶¨å¹…"].iloc[0]
                    df.at[idx, "è¿‘ 3 æœˆæ¶¨å¹…"] = info["è¿‘ 3 æœˆæ¶¨å¹…"].iloc[0]
                    df.at[idx, "è¿‘ 6 æœˆæ¶¨å¹…"] = info["è¿‘ 6 æœˆæ¶¨å¹…"].iloc[0]
                    df.at[idx, "è¿‘ 1 å¹´æ¶¨å¹…"] = info["è¿‘ 1 å¹´æ¶¨å¹…"].iloc[0]
                    df.at[idx, "æˆç«‹ä»¥æ¥æ¶¨å¹…"] = info["æˆç«‹ä»¥æ¥æ¶¨å¹…"].iloc[0]
                    
                    # ç¡®ä¿åŸºé‡‘è§„æ¨¡å•ä½ä¸º"äº¿å…ƒ"
                    if "åŸºé‡‘è§„æ¨¡ï¼ˆæœ€æ–°ï¼‰" in df.columns:
                        # æå–æ•°å­—éƒ¨åˆ†
                        size_str = str(df.at[idx, "åŸºé‡‘è§„æ¨¡ï¼ˆæœ€æ–°ï¼‰"])
                        size_num = ''.join(filter(lambda x: x.isdigit() or x == '.', size_str))
                        if size_num:
                            size_value = float(size_num)
                            # å¦‚æœåŒ…å«"ä¸‡"å­—ï¼Œè¡¨ç¤ºå•ä½æ˜¯"ä¸‡ä»½"
                            if "ä¸‡" in size_str:
                                # ä¸‡ä»½è½¬ä¸ºäº¿ä»½ï¼Œå†ä¹˜ä»¥å•ä½å‡€å€¼ä¼°ç®—è§„æ¨¡
                                size_value = size_value * 0.0001  # ä¸‡ä»½è½¬ä¸ºäº¿ä»½
                            # å¦‚æœåŒ…å«"äº¿"å­—ï¼Œå·²ç»æ˜¯äº¿ä»½
                            df.at[idx, "åŸºé‡‘è§„æ¨¡"] = size_value
                        else:
                            df.at[idx, "åŸºé‡‘è§„æ¨¡"] = 0.0
                    
                    logger.debug(f"ETF {etf_code} ä¿¡æ¯è¡¥å……æˆåŠŸ")
                else:
                    logger.warning(f"ETF {etf_code} æ— åŸºæœ¬ä¿¡æ¯ï¼Œæ— æ³•è¡¥å……ä¿¡æ¯")
            except Exception as e:
                logger.error(f"è·å–ETF {etf_code} ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        return df
    
    except Exception as e:
        logger.error(f"âŒ è¡¥å……ETFæ•°æ®å¤±è´¥: {str(e)}")
        return df

def read_csv_with_encoding(file_path: str) -> pd.DataFrame:
    """è¯»å–CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å…¼å®¹UTF-8å’ŒGBKç¼–ç 
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: è¯»å–çš„DataFrame
    """
    try:
        return pd.read_csv(file_path, encoding='utf-8-sig')
    except UnicodeDecodeError:
        try:
            return pd.read_csv(file_path, encoding='gbk')
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            return pd.DataFrame()

def get_filtered_etf_codes() -> list:
    """è·å–è¿‡æ»¤åçš„æœ‰æ•ˆETFä»£ç åˆ—è¡¨
    :return: ETFä»£ç åˆ—è¡¨
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("âš ï¸ æ— æœ‰æ•ˆETFä»£ç åˆ—è¡¨")
            return []
        # ç¡®ä¿ETFä»£ç ä¸ºå­—ç¬¦ä¸²ç±»å‹
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip()
        valid_codes = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]["ETFä»£ç "].tolist()
        logger.info(f"ğŸ“Š æœ‰æ•ˆETFä»£ç æ•°é‡: {len(valid_codes)}")
        return valid_codes
    except Exception as e:
        logger.error(f"è·å–æœ‰æ•ˆETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []

def get_etf_name(etf_code: str) -> str:
    """æ ¹æ®ETFä»£ç è·å–ETFåç§°
    :param etf_code: ETFä»£ç 
    :return: ETFåç§°
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è·å–ETFåç§°")
            return f"ETF-{etf_code}"
        
        # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
        name_row = etf_list[etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == etf_code]
        if not name_row.empty:
            return name_row.iloc[0]["ETFåç§°"]
        else:
            logger.debug(f"æœªåœ¨å…¨å¸‚åœºåˆ—è¡¨ä¸­æ‰¾åˆ°ETFä»£ç : {etf_code}")
            return f"ETF-{etf_code}"
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
        return f"ETF-{etf_code}"
