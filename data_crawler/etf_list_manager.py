import os
import akshare as ak
import pandas as pd
import logging
import requests
import time
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any
from retrying import retry
from config import Config
from utils.date_utils import is_file_outdated, get_beijing_time  # ç¡®ä¿å¯¼å…¥get_beijing_time

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# ç¼“å­˜å˜é‡ï¼Œé¿å…é‡å¤åŠ è½½
_etf_list_cache = None
_last_load_time = None

def load_all_etf_list() -> pd.DataFrame:
    """åŠ è½½å…¨å¸‚åœºETFåˆ—è¡¨ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åŠ è½½
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame
    """
    global _etf_list_cache, _last_load_time
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ5åˆ†é’Ÿå†…ï¼‰
    if (_etf_list_cache is not None and 
        _last_load_time is not None and 
        (datetime.now() - _last_load_time).total_seconds() < 300):
        
        # ä¸¥æ ¼éªŒè¯ç¼“å­˜æ•°æ®
        if not isinstance(_etf_list_cache, pd.DataFrame):
            logger.warning("ETFåˆ—è¡¨ç¼“å­˜ä¸æ˜¯DataFrameç±»å‹ï¼Œå°†é‡æ–°åŠ è½½")
        elif _etf_list_cache.empty:
            logger.warning("ETFåˆ—è¡¨ç¼“å­˜ä¸ºç©ºï¼Œå°†é‡æ–°åŠ è½½")
        elif not validate_etf_list(_etf_list_cache):
            logger.warning("ETFåˆ—è¡¨ç¼“å­˜éªŒè¯å¤±è´¥ï¼Œå°†é‡æ–°åŠ è½½")
        else:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„ETFåˆ—è¡¨ (å…±{_etf_list_cache.shape[0]}æ¡è®°å½•)")
            # åˆ›å»ºæ·±æ‹·è´å¹¶ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
            cached_df = _etf_list_cache.copy(deep=True)
            
            # ç¡®ä¿ETFä»£ç æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”æ ¼å¼æ­£ç¡®
            if "ETFä»£ç " in cached_df.columns:
                # æ£€æŸ¥åˆ—æ˜¯å¦åŒ…å«éå­—ç¬¦ä¸²å€¼
                has_non_string = cached_df["ETFä»£ç "].apply(lambda x: not isinstance(x, str)).any()
                
                # å¦‚æœåˆ—åŒ…å«éå­—ç¬¦ä¸²å€¼ï¼Œæˆ–è€…åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼Œåˆ™è¿›è¡Œè½¬æ¢
                if has_non_string or pd.api.types.is_numeric_dtype(cached_df["ETFä»£ç "]):
                    cached_df.loc[:, "ETFä»£ç "] = cached_df["ETFä»£ç "].astype(str)
                
                # ç¡®ä¿ETFä»£ç æ˜¯6ä½æ•°å­—
                cached_df.loc[:, "ETFä»£ç "] = cached_df["ETFä»£ç "].str.strip().str.zfill(6)
            
            return cached_df
    
    # æ›´æ–°ETFåˆ—è¡¨
    try:
        new_etf_list = update_all_etf_list()
        
        # ä¸¥æ ¼éªŒè¯æ–°è·å–çš„æ•°æ®
        if not isinstance(new_etf_list, pd.DataFrame):
            logger.error("update_all_etf_list() è¿”å›çš„ä¸æ˜¯DataFrameç±»å‹")
            _etf_list_cache = pd.DataFrame()
        elif new_etf_list.empty:
            logger.warning("update_all_etf_list() è¿”å›ç©ºDataFrame")
            _etf_list_cache = pd.DataFrame()
        else:
            # éªŒè¯ETFåˆ—è¡¨
            if not validate_etf_list(new_etf_list):
                logger.warning("ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                new_etf_list = repair_etf_list(new_etf_list)
                if not validate_etf_list(new_etf_list):
                    logger.error("ETFåˆ—è¡¨ä¿®å¤å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
                    return pd.DataFrame()
            
            # åˆ›å»ºæ·±æ‹·è´é¿å…SettingWithCopyWarning
            _etf_list_cache = new_etf_list.copy(deep=True)
            
            # ç¡®ä¿åŒ…å«å¿…è¦åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS
            missing_columns = [col for col in required_columns if col not in _etf_list_cache.columns]
            
            if missing_columns:
                logger.error(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
                logger.debug(f"å®é™…åˆ—å: {list(_etf_list_cache.columns)}")
                # å°è¯•ä¿®å¤ç¼ºå¤±çš„åˆ—
                for col in missing_columns:
                    if col == "ETFä»£ç " and "ä»£ç " in _etf_list_cache.columns:
                        _etf_list_cache.rename(columns={"ä»£ç ": "ETFä»£ç "}, inplace=True)
                    elif col == "ETFåç§°" and "åç§°" in _etf_list_cache.columns:
                        _etf_list_cache.rename(columns={"åç§°": "ETFåç§°"}, inplace=True)
                    else:
                        _etf_list_cache[col] = "" if col != "åŸºé‡‘è§„æ¨¡" else 0.0
            
            # ç¡®ä¿ETFä»£ç æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”æ ¼å¼æ­£ç¡®
            if "ETFä»£ç " in _etf_list_cache.columns:
                # æ£€æŸ¥åˆ—æ˜¯å¦åŒ…å«éå­—ç¬¦ä¸²å€¼
                has_non_string = _etf_list_cache["ETFä»£ç "].apply(lambda x: not isinstance(x, str)).any()
                
                # å¦‚æœåˆ—åŒ…å«éå­—ç¬¦ä¸²å€¼ï¼Œæˆ–è€…åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼Œåˆ™è¿›è¡Œè½¬æ¢
                if has_non_string or pd.api.types.is_numeric_dtype(_etf_list_cache["ETFä»£ç "]):
                    _etf_list_cache.loc[:, "ETFä»£ç "] = _etf_list_cache["ETFä»£ç "].astype(str)
                
                # ç¡®ä¿ETFä»£ç æ˜¯6ä½æ•°å­—
                _etf_list_cache.loc[:, "ETFä»£ç "] = _etf_list_cache["ETFä»£ç "].str.strip().str.zfill(6)
                
                # è¿‡æ»¤æ— æ•ˆçš„ETFä»£ç ï¼ˆé6ä½æ•°å­—ï¼‰
                _etf_list_cache = _etf_list_cache[
                    _etf_list_cache["ETFä»£ç "].str.match(r'^\d{6}$')
                ].copy()
            
            logger.info(f"æˆåŠŸåŠ è½½ETFåˆ—è¡¨ï¼Œå…±{_etf_list_cache.shape[0]}æ¡æœ‰æ•ˆè®°å½•")
        
        _last_load_time = datetime.now()
        return _etf_list_cache.copy() if _etf_list_cache is not None else pd.DataFrame()
    
    except Exception as e:
        logger.error(f"åŠ è½½ETFåˆ—è¡¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        # å°è¯•è¿”å›ç©ºDataFrameè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return pd.DataFrame()

def update_all_etf_list() -> pd.DataFrame:
    """æ›´æ–°ETFåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œè‹¥éœ€æ›´æ–°åˆ™ä»ç½‘ç»œè·å–ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame
    """
    # ===== å…³é”®ä¿®å¤ï¼šæ·»åŠ å‘¨æ—¥å¼ºåˆ¶æ›´æ–°é€»è¾‘ =====
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´
    beijing_time = get_beijing_time()
    # åˆ¤æ–­æ˜¯å¦ä¸ºå‘¨æ—¥ï¼ˆæ˜ŸæœŸæ—¥çš„ç´¢å¼•æ˜¯6ï¼Œæ˜ŸæœŸä¸€çš„ç´¢å¼•æ˜¯0ï¼‰
    is_sunday = beijing_time.weekday() == 6
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° - å‘¨æ—¥å¼ºåˆ¶æ›´æ–°
    if is_sunday or not os.path.exists(Config.ALL_ETFS_PATH) or is_file_outdated(Config.ALL_ETFS_PATH, Config.ETF_LIST_UPDATE_INTERVAL):
        logger.info(f"{'[å¼ºåˆ¶æ›´æ–°] ' if is_sunday else ''}ETFåˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œå°è¯•ä»ç½‘ç»œè·å–...")
        try:
            primary_etf_list = None
            
            # 1. å°è¯•AkShareæ¥å£
            logger.info("å°è¯•ä»AkShareè·å–ETFåˆ—è¡¨...")
            primary_etf_list = fetch_all_etfs_akshare()
            
            if not primary_etf_list.empty:
                # éªŒè¯ETFåˆ—è¡¨
                if not validate_etf_list(primary_etf_list):
                    logger.warning("ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                    primary_etf_list = repair_etf_list(primary_etf_list)
                    if not validate_etf_list(primary_etf_list):
                        logger.error("ETFåˆ—è¡¨ä¿®å¤å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
                        primary_etf_list = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
                
                # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                required_columns = Config.ETF_STANDARD_COLUMNS
                for col in required_columns:
                    if col not in primary_etf_list.columns:
                        primary_etf_list[col] = ""
                primary_etf_list = primary_etf_list[required_columns]
                # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
                primary_etf_list = primary_etf_list.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                
                # ä¿å­˜å‰å†æ¬¡éªŒè¯
                if validate_etf_list(primary_etf_list):
                    # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
                    primary_etf_list["ETFä»£ç "] = primary_etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    # è¿‡æ»¤æ— æ•ˆçš„ETFä»£ç 
                    primary_etf_list = primary_etf_list[primary_etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
                    
                    primary_etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
                    logger.info(f"âœ… AkShareæ›´æ–°æˆåŠŸï¼ˆ{len(primary_etf_list)}åªETFï¼‰")
                    # æ ‡è®°æ•°æ®æ¥æº
                    primary_etf_list.source = "AkShare"
                else:
                    logger.error("ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
            else:
                logger.warning("AkShareè¿”å›ç©ºçš„ETFåˆ—è¡¨")
            
            # 2. å¦‚æœAkShareå¤±è´¥ï¼Œå°è¯•æ–°æµªæ¥å£
            if primary_etf_list is None or primary_etf_list.empty:
                logger.info("å°è¯•ä»æ–°æµªè·å–ETFåˆ—è¡¨...")
                primary_etf_list = fetch_all_etfs_sina()
                
                if not primary_etf_list.empty:
                    # éªŒè¯ETFåˆ—è¡¨
                    if not validate_etf_list(primary_etf_list):
                        logger.warning("ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                        primary_etf_list = repair_etf_list(primary_etf_list)
                        if not validate_etf_list(primary_etf_list):
                            logger.error("ETFåˆ—è¡¨ä¿®å¤å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
                            primary_etf_list = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
                    
                    # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in primary_etf_list.columns:
                            primary_etf_list[col] = ""
                    primary_etf_list = primary_etf_list[required_columns]
                    
                    # ä¿å­˜å‰å†æ¬¡éªŒè¯
                    if validate_etf_list(primary_etf_list):
                        # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
                        primary_etf_list["ETFä»£ç "] = primary_etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                        # è¿‡æ»¤æ— æ•ˆçš„ETFä»£ç 
                        primary_etf_list = primary_etf_list[primary_etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
                        
                        primary_etf_list.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
                        logger.info(f"âœ… æ–°æµªæ¥å£æ›´æ–°æˆåŠŸï¼ˆ{len(primary_etf_list)}åªETFï¼‰")
                        # æ ‡è®°æ•°æ®æ¥æº
                        primary_etf_list.source = "æ–°æµª"
                    else:
                        logger.error("ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜")
                else:
                    logger.warning("æ–°æµªæ¥å£è¿”å›ç©ºçš„ETFåˆ—è¡¨")
            
            # 3. å¦‚æœå‰ä¸¤è€…éƒ½å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–‡ä»¶
            if primary_etf_list is None or primary_etf_list.empty:
                logger.info("å°è¯•åŠ è½½å…œåº•ETFåˆ—è¡¨æ–‡ä»¶...")
                if os.path.exists(Config.BACKUP_ETFS_PATH):
                    try:
                        backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                        # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
                        backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                        backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                        required_columns = Config.ETF_STANDARD_COLUMNS
                        for col in required_columns:
                            if col not in backup_df.columns:
                                backup_df[col] = ""
                        backup_df = backup_df[required_columns].drop_duplicates()
                        # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
                        backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                        
                        # éªŒè¯ETFåˆ—è¡¨
                        if validate_etf_list(backup_df):
                            logger.info(f"âœ… å…œåº•æ–‡ä»¶åŠ è½½æˆåŠŸï¼ˆ{len(backup_df)}åªETFï¼‰")
                            # æ ‡è®°æ•°æ®æ¥æº
                            backup_df.source = "å…œåº•æ–‡ä»¶"
                            
                            # ä¿å­˜å…œåº•æ–‡ä»¶ä¸ºå½“å‰ETFåˆ—è¡¨
                            backup_df.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
                            return backup_df
                        else:
                            logger.warning("å…œåº•æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                            backup_df = repair_etf_list(backup_df)
                            if validate_etf_list(backup_df):
                                backup_df.to_csv(Config.ALL_ETFS_PATH, index=False, encoding="utf-8")
                                logger.info(f"âœ… å…œåº•æ–‡ä»¶ä¿®å¤ååŠ è½½æˆåŠŸï¼ˆ{len(backup_df)}åªETFï¼‰")
                                backup_df.source = "å…œåº•æ–‡ä»¶(ä¿®å¤å)"
                                return backup_df
                    except Exception as e:
                        logger.error(f"âŒ å…œåº•æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                
                # å¦‚æœå…œåº•æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨æˆ–å¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
                logger.error("âŒ æ— æ³•è·å–ETFåˆ—è¡¨ï¼Œæ‰€æœ‰æ•°æ®æºå‡å¤±è´¥")
                empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
                # æ ‡è®°æ•°æ®æ¥æº
                empty_df.source = "æ— æ•°æ®æº"
                return empty_df
            
            # è¿”å›å‰éªŒè¯æœ€ç»ˆç»“æœ
            if not primary_etf_list.empty and validate_etf_list(primary_etf_list):
                return primary_etf_list
            else:
                logger.warning("è·å–çš„ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
                return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ETFåˆ—è¡¨æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}", exc_info=True)
            # å°è¯•åŠ è½½å…œåº•æ–‡ä»¶ä½œä¸ºæœ€åæ‰‹æ®µ
            if os.path.exists(Config.BACKUP_ETFS_PATH):
                try:
                    backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                    backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in backup_df.columns:
                            backup_df[col] = ""
                    backup_df = backup_df[required_columns].drop_duplicates()
                    backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                    
                    # éªŒè¯å…œåº•æ–‡ä»¶
                    if validate_etf_list(backup_df):
                        logger.warning("âš ï¸ ä½¿ç”¨å…œåº•æ–‡ä»¶ä½œä¸ºæœ€åæ‰‹æ®µ")
                        # æ ‡è®°æ•°æ®æ¥æº
                        backup_df.source = "å…œåº•æ–‡ä»¶(å¼‚å¸¸)"
                        return backup_df
                    else:
                        logger.warning("å…œåº•æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                        backup_df = repair_etf_list(backup_df)
                        if validate_etf_list(backup_df):
                            logger.warning("âš ï¸ ä½¿ç”¨ä¿®å¤åçš„å…œåº•æ–‡ä»¶ä½œä¸ºæœ€åæ‰‹æ®µ")
                            backup_df.source = "å…œåº•æ–‡ä»¶(å¼‚å¸¸ä¿®å¤å)"
                            return backup_df
                except Exception as e:
                    logger.error(f"âŒ å…œåº•æ–‡ä»¶åŠ è½½ä¹Ÿå¤±è´¥: {str(e)}", exc_info=True)
            
            # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
            empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
            # æ ‡è®°æ•°æ®æ¥æº
            empty_df.source = "æ— æ•°æ®æº(å¼‚å¸¸)"
            return empty_df
    
    else:
        logger.info("â„¹ï¸ æ— éœ€æ›´æ–°ï¼ŒåŠ è½½æœ¬åœ°ETFåˆ—è¡¨")
        try:
            etf_list = read_csv_with_encoding(Config.ALL_ETFS_PATH)
            # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
            required_columns = Config.ETF_STANDARD_COLUMNS
            for col in required_columns:
                if col not in etf_list.columns:
                    etf_list[col] = ""
            # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
            etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
            # è¿‡æ»¤æ— æ•ˆçš„ETFä»£ç 
            etf_list = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
            # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
            etf_list = etf_list.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
            
            # éªŒè¯ETFåˆ—è¡¨
            if validate_etf_list(etf_list):
                # æ ‡è®°æ•°æ®æ¥æº
                etf_list.source = "æœ¬åœ°ç¼“å­˜"
                return etf_list
            else:
                logger.warning("æœ¬åœ°ETFåˆ—è¡¨éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                etf_list = repair_etf_list(etf_list)
                if validate_etf_list(etf_list):
                    etf_list.source = "æœ¬åœ°ç¼“å­˜(ä¿®å¤å)"
                    return etf_list
                else:
                    logger.error("æœ¬åœ°ETFåˆ—è¡¨ä¿®å¤å¤±è´¥ï¼Œå°è¯•ä»ç½‘ç»œè·å–")
                    return update_all_etf_list()
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
            # å°è¯•åŠ è½½å…œåº•æ–‡ä»¶
            if os.path.exists(Config.BACKUP_ETFS_PATH):
                try:
                    backup_df = read_csv_with_encoding(Config.BACKUP_ETFS_PATH)
                    backup_df["ETFä»£ç "] = backup_df["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
                    backup_df = backup_df[backup_df["ETFä»£ç "].str.match(r'^\d{6}$')]
                    required_columns = Config.ETF_STANDARD_COLUMNS
                    for col in required_columns:
                        if col not in backup_df.columns:
                            backup_df[col] = ""
                    backup_df = backup_df[required_columns].drop_duplicates()
                    backup_df = backup_df.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
                    
                    # éªŒè¯å…œåº•æ–‡ä»¶
                    if validate_etf_list(backup_df):
                        logger.warning("âš ï¸ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å…œåº•æ–‡ä»¶")
                        # æ ‡è®°æ•°æ®æ¥æº
                        backup_df.source = "å…œåº•æ–‡ä»¶(æœ¬åœ°åŠ è½½å¤±è´¥)"
                        return backup_df
                    else:
                        logger.warning("å…œåº•æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                        backup_df = repair_etf_list(backup_df)
                        if validate_etf_list(backup_df):
                            backup_df.source = "å…œåº•æ–‡ä»¶(æœ¬åœ°åŠ è½½å¤±è´¥ä¿®å¤å)"
                            return backup_df
                except Exception as e:
                    logger.error(f"âŒ å…œåº•æ–‡ä»¶åŠ è½½ä¹Ÿå¤±è´¥: {str(e)}", exc_info=True)
            
            # è¿”å›ç©ºDataFrameä½†åŒ…å«æ‰€æœ‰åˆ—
            empty_df = pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
            # æ ‡è®°æ•°æ®æ¥æº
            empty_df.source = "æ— æ•°æ®æº(æœ¬åœ°åŠ è½½å¤±è´¥)"
            return empty_df

def retry_if_network_error(exception: Exception) -> bool:
    """é‡è¯•æ¡ä»¶ï¼šç½‘ç»œç›¸å…³é”™è¯¯
    :param exception: å¼‚å¸¸å¯¹è±¡
    :return: å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›False"""
    return isinstance(exception, (requests.RequestException, ConnectionError, TimeoutError))

@retry(stop_max_attempt_number=3,
       wait_exponential_multiplier=1000,
       wait_exponential_max=10000,
       retry_on_exception=retry_if_network_error)

def fetch_all_etfs_akshare() -> pd.DataFrame:
    """ä½¿ç”¨AkShareæ¥å£è·å–ETFåˆ—è¡¨ï¼ˆå¸¦è§„æ¨¡å’Œæˆäº¤é¢ç­›é€‰ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame"""
    try:
        logger.info("å°è¯•ä»AkShareè·å–ETFåˆ—è¡¨...")
        # è°ƒç”¨fund_etf_spot_emæ¥å£
        etf_info = ak.fund_etf_spot_em()
        if etf_info.empty:
            logger.warning("AkShareè¿”å›ç©ºçš„ETFåˆ—è¡¨")
            return pd.DataFrame()
        
        # è®°å½•è¿”å›çš„åˆ—åï¼Œç”¨äºè°ƒè¯•
        logger.debug(f"AkShareè¿”å›åˆ—å: {list(etf_info.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—åæ˜ å°„ï¼ˆæ ¹æ®å®é™…è¿”å›åˆ—åä¿®æ­£ï¼‰
        column_mapping = {}
        for col in etf_info.columns:
            if "ä»£ç " in col:
                column_mapping[col] = "ETFä»£ç "
            elif "åç§°" in col:
                column_mapping[col] = "ETFåç§°"
            elif "æµé€šå¸‚å€¼" in col or "æœ€æ–°è§„æ¨¡" in col or "è§„æ¨¡" in col:
                column_mapping[col] = "åŸºé‡‘è§„æ¨¡"
            elif "æˆäº¤é¢" in col or "æ—¥å‡æˆäº¤é¢" in col:
                column_mapping[col] = "æ—¥å‡æˆäº¤é¢"
            elif "æ¶¨è·Œå¹…" in col:
                column_mapping[col] = "æ¶¨è·Œå¹…"
            elif "å‡€å€¼" in col:
                column_mapping[col] = "å‡€å€¼"
        
        # é‡å‘½ååˆ—
        etf_info = etf_info.rename(columns=column_mapping)
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
        required_columns = Config.ETF_STANDARD_COLUMNS
        for col in required_columns:
            if col not in etf_info.columns:
                # å¯¹äºåŸºé‡‘è§„æ¨¡ï¼Œå°è¯•ä»å…¶ä»–å¯èƒ½çš„åˆ—è·å–
                if col == "åŸºé‡‘è§„æ¨¡" and ("æœ€æ–°è§„æ¨¡" in etf_info.columns or "è§„æ¨¡" in etf_info.columns):
                    if "æœ€æ–°è§„æ¨¡" in etf_info.columns:
                        etf_info["åŸºé‡‘è§„æ¨¡"] = etf_info["æœ€æ–°è§„æ¨¡"]
                    else:
                        etf_info["åŸºé‡‘è§„æ¨¡"] = etf_info["è§„æ¨¡"]
                else:
                    etf_info[col] = "" if col != "åŸºé‡‘è§„æ¨¡" else 0.0
        
        # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿ä»£ç ä¸º6ä½æ•°å­—
        # ä¿®å¤ï¼šå…ˆç¡®ä¿ETFä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
        etf_info["ETFä»£ç "] = etf_info["ETFä»£ç "].astype(str)
        etf_info["ETFä»£ç "] = etf_info["ETFä»£ç "].str.strip().str.zfill(6)
        
        valid_etfs = etf_info[etf_info["ETFä»£ç "].str.match(r'^\d{6}$', na=False)].copy()
        
        # è½¬æ¢æ•°æ®ç±»å‹å¹¶å¤„ç†å•ä½
        # æµé€šå¸‚å€¼å•ä½ä¸ºå…ƒï¼Œè½¬æ¢ä¸ºäº¿å…ƒï¼ˆé™¤ä»¥1äº¿ï¼‰
        valid_etfs["åŸºé‡‘è§„æ¨¡"] = pd.to_numeric(valid_etfs["åŸºé‡‘è§„æ¨¡"], errors="coerce")
        # å¦‚æœåŸºé‡‘è§„æ¨¡å•ä½æ˜¯äº¿å…ƒï¼Œä¸éœ€è¦è½¬æ¢ï¼›å¦‚æœæ˜¯ä¸‡å…ƒï¼Œè½¬æ¢ä¸ºäº¿å…ƒ
        if not valid_etfs.empty and valid_etfs["åŸºé‡‘è§„æ¨¡"].max() < 1000:  # å¦‚æœæœ€å¤§è§„æ¨¡å°äº1000ï¼Œå¯èƒ½æ˜¯äº¿å…ƒå•ä½
            pass
        else:  # å¦åˆ™å¯èƒ½æ˜¯ä¸‡å…ƒå•ä½ï¼Œè½¬æ¢ä¸ºäº¿å…ƒ
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = valid_etfs["åŸºé‡‘è§„æ¨¡"] / 10000
        
        # æ£€æŸ¥æ˜¯å¦æœ‰"æ—¥å‡æˆäº¤é¢"åˆ—ï¼Œå¦‚æœæœ‰ï¼Œè½¬æ¢ä¸ºä¸‡å…ƒ
        if "æ—¥å‡æˆäº¤é¢" in valid_etfs.columns:
            valid_etfs["æ—¥å‡æˆäº¤é¢"] = pd.to_numeric(valid_etfs["æ—¥å‡æˆäº¤é¢"], errors="coerce") / 10000
        
        # ç­›é€‰æ¡ä»¶ï¼šä½¿ç”¨Configä¸­å®šä¹‰çš„ç­›é€‰å‚æ•°
        filtered_etfs = valid_etfs[
            (valid_etfs["åŸºé‡‘è§„æ¨¡"] >= Config.GLOBAL_MIN_FUND_SIZE)
        ].copy()
        
        # å¦‚æœæ²¡æœ‰ETFé€šè¿‡ç­›é€‰ï¼Œè¿”å›åŸå§‹æ•°æ®ï¼ˆä¸ç­›é€‰ï¼‰
        if filtered_etfs.empty:
            logger.warning(f"ETFç­›é€‰æ¡ä»¶è¿‡äºä¸¥æ ¼ï¼Œæ— ç¬¦åˆè¦æ±‚çš„ETFï¼ˆè§„æ¨¡â‰¥{Config.GLOBAL_MIN_FUND_SIZE}äº¿ï¼‰ï¼Œè¿”å›å…¨éƒ¨ETF")
            filtered_etfs = valid_etfs.copy()
        
        filtered_etfs = filtered_etfs[Config.ETF_STANDARD_COLUMNS]
        logger.info(f"AkShareæˆåŠŸè·å–ETFåˆ—è¡¨ï¼Œå…± {len(filtered_etfs)} æ¡æœ‰æ•ˆè®°å½•")
        return filtered_etfs
    
    except Exception as e:
        logger.error(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def fetch_all_etfs_sina() -> pd.DataFrame:
    """æ–°æµªæ¥å£å…œåº•è·å–ETFåˆ—è¡¨ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    :return: åŒ…å«ETFä¿¡æ¯çš„DataFrame"""
    try:
        logger.info("å°è¯•ä»æ–°æµªè·å–ETFåˆ—è¡¨...")
        url = "https://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/Market_Center.getETFList  "
        params = {"page": 1, "num": 1000, "sort": "symbol", "asc": 1}
        response = requests.get(url, params=params, timeout=Config.REQUEST_TIMEOUT)
        response.raise_for_status()
        
        # å¤„ç†æ–°æµªæ¥å£è¿”å›çš„æ•°æ®
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            etf_data = response.json()
        except ValueError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•å¤„ç†å¯èƒ½çš„JavaScriptæ ¼å¼
            try:
                # ç§»é™¤å¯èƒ½çš„JavaScriptå‰ç¼€
                etf_data_str = response.text.replace('var data=', '').strip(';')
                # å°è¯•è§£æä¸ºJSON
                etf_data = json.loads(etf_data_str)
            except Exception as e:
                logger.error(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•eval: {str(e)}")
                try:
                    # ä½œä¸ºæœ€åæ‰‹æ®µä½¿ç”¨eval
                    etf_data = eval(etf_data_str)
                except Exception as e:
                    logger.error(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®æ ¼å¼æ— æ³•è§£æ: {str(e)}")
                    return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(etf_data, dict):
            # å°è¯•ä»å¸¸è§å­—æ®µä¸­æå–æ•°æ®
            if 'data' in etf_data and isinstance(etf_data['data'], list):
                etf_data = etf_data['data']
            elif 'list' in etf_data and isinstance(etf_data['list'], list):
                etf_data = etf_data['list']
            elif 'result' in etf_data and 'data' in etf_data['result'] and isinstance(etf_data['result']['data'], list):
                etf_data = etf_data['result']['data']
            else:
                logger.warning("æ–°æµªæ¥å£è¿”å›çš„æ˜¯å­—å…¸ä½†æ²¡æœ‰é¢„æœŸçš„æ•°æ®ç»“æ„")
                return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # ç¡®ä¿etf_dataæ˜¯åˆ—è¡¨
        if not isinstance(etf_data, list):
            logger.error(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(etf_data)}")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # åˆ›å»ºDataFrame
        if not etf_data:
            logger.warning("æ–°æµªæ¥å£è¿”å›ç©ºåˆ—è¡¨")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        etf_list = pd.DataFrame(etf_data)
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['symbol', 'name']
        missing_columns = [col for col in required_columns if col not in etf_list.columns]
        if missing_columns:
            logger.warning(f"æ–°æµªæ¥å£è¿”å›çš„æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # å°è¯•ä»å…¶ä»–åˆ—åæ˜ å°„
            column_mapping = {}
            if 'symbol' in missing_columns and 'code' in etf_list.columns:
                column_mapping['code'] = 'symbol'
            if 'name' in missing_columns and 'name' in etf_list.columns:
                column_mapping['name'] = 'name'
            
            if column_mapping:
                etf_list = etf_list.rename(columns=column_mapping)
                missing_columns = [col for col in required_columns if col not in etf_list.columns]
            
            if missing_columns:
                logger.error(f"æ— æ³•ä¿®å¤ç¼ºå¤±çš„åˆ—: {', '.join(missing_columns)}")
                return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # é‡å‘½ååˆ—
        etf_list = etf_list.rename(columns={
            "symbol": "å®Œæ•´ä»£ç ",
            "name": "ETFåç§°"
        })
        
        # æå–çº¯æ•°å­—ä»£ç ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¡®ä¿6ä½æ•°å­—ï¼‰
        etf_list["ETFä»£ç "] = etf_list["å®Œæ•´ä»£ç "].astype(str).str.extract(r'(\d{6})', expand=False)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„6ä½æ•°å­—ETFä»£ç 
        valid_etfs = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$', na=False)].copy()
        
        if valid_etfs.empty:
            logger.warning("æå–åæ— æœ‰æ•ˆETFä»£ç ")
            return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)
        
        # æ·»åŠ åŸºé‡‘è§„æ¨¡åˆ—ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        if "amount" in valid_etfs.columns:
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = pd.to_numeric(valid_etfs["amount"], errors="coerce") / 10000
        else:
            valid_etfs["åŸºé‡‘è§„æ¨¡"] = 0.0
        
        # ç¡®ä¿åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
        for col in Config.ETF_STANDARD_COLUMNS:
            if col not in valid_etfs.columns:
                valid_etfs[col] = ""
        
        valid_etfs = valid_etfs[Config.ETF_STANDARD_COLUMNS]
        # æŒ‰åŸºé‡‘è§„æ¨¡é™åºæ’åº
        valid_etfs = valid_etfs.sort_values("åŸºé‡‘è§„æ¨¡", ascending=False)
        
        logger.info(f"âœ… æ–°æµªæ¥å£æˆåŠŸè·å–{len(valid_etfs)}åªETF")
        return valid_etfs.drop_duplicates(subset="ETFä»£ç ")
    
    except Exception as e:
        error_msg = f"âŒ æ–°æµªæ¥å£é”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return pd.DataFrame(columns=Config.ETF_STANDARD_COLUMNS)

def read_csv_with_encoding(file_path: str) -> pd.DataFrame:
    """è¯»å–CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å…¼å®¹UTF-8å’ŒGBKç¼–ç 
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: è¯»å–çš„DataFrame
    """
    # å®šä¹‰æ˜ç¡®çš„åˆ—æ•°æ®ç±»å‹
    dtype_dict = {
        "ETFä»£ç ": str,
        "ETFåç§°": str,
        "åŸºé‡‘è§„æ¨¡": float,
        "å®Œæ•´ä»£ç ": str
    }
    
    try:
        return pd.read_csv(
            file_path, 
            encoding='utf-8',
            dtype={k: v for k, v in dtype_dict.items() if k != "å®Œæ•´ä»£ç "},  # å®Œæ•´ä»£ç å¯èƒ½ä¸å­˜åœ¨
            keep_default_na=False  # é¿å…å°†ç©ºå­—ç¬¦ä¸²è½¬æ¢ä¸ºNaN
        )
    except UnicodeDecodeError:
        try:
            return pd.read_csv(
                file_path, 
                encoding='gbk',
                dtype={k: v for k, v in dtype_dict.items() if k != "å®Œæ•´ä»£ç "},
                keep_default_na=False
            )
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            return pd.DataFrame()

def get_filtered_etf_codes(min_size: float = None, exclude_money_etfs: bool = True) -> list:
    """è·å–è¿‡æ»¤åçš„æœ‰æ•ˆETFä»£ç åˆ—è¡¨
    :param min_size: æœ€å°åŸºé‡‘è§„æ¨¡(äº¿å…ƒ)ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨Config.GLOBAL_MIN_FUND_SIZE
    :param exclude_money_etfs: æ˜¯å¦æ’é™¤è´§å¸ETF(511å¼€å¤´)ï¼Œé»˜è®¤True
    :return: ETFä»£ç åˆ—è¡¨
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("âš ï¸ æ— æœ‰æ•ˆETFä»£ç åˆ—è¡¨")
            return []
        
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        min_size = min_size if min_size is not None else Config.GLOBAL_MIN_FUND_SIZE
        
        # ç¡®ä¿ETFä»£ç ä¸ºå­—ç¬¦ä¸²ç±»å‹
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip()
        
        # ç­›é€‰æœ‰æ•ˆETFä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
        valid_etfs = etf_list[etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
        
        # åº”ç”¨è§„æ¨¡è¿‡æ»¤
        if "åŸºé‡‘è§„æ¨¡" in valid_etfs.columns:
            valid_etfs = valid_etfs[valid_etfs["åŸºé‡‘è§„æ¨¡"] >= min_size]
        
        # åº”ç”¨è´§å¸ETFè¿‡æ»¤ï¼ˆ511å¼€å¤´ï¼‰
        if exclude_money_etfs:
            valid_etfs = valid_etfs[~valid_etfs["ETFä»£ç "].str.startswith("511")]
        
        valid_codes = valid_etfs["ETFä»£ç "].tolist()
        logger.info(f"ğŸ“Š æœ‰æ•ˆETFä»£ç æ•°é‡: {len(valid_codes)} (ç­›é€‰æ¡ä»¶: è§„æ¨¡â‰¥{min_size}äº¿, {'æ’é™¤' if exclude_money_etfs else 'åŒ…å«'}è´§å¸ETF)")
        return valid_codes
    except Exception as e:
        logger.error(f"è·å–æœ‰æ•ˆETFä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []

def get_etf_name(etf_code: str) -> str:
    """æ ¹æ®ETFä»£ç è·å–ETFåç§°
    :param etf_code: ETFä»£ç 
    :return: ETFåç§°
    """
    try:
        etf_list = load_all_etf_list()
        if etf_list.empty:
            logger.warning("ETFåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è·å–ETFåç§°")
            return f"ETF-{etf_code}"
        
        # ç¡®ä¿ETFä»£ç æ ¼å¼æ­£ç¡®
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6)
        name_row = etf_list[etf_list["ETFä»£ç "].astype(str).str.strip().str.zfill(6) == etf_code]
        if not name_row.empty:
            return name_row.iloc[0]["ETFåç§°"]
        else:
            logger.debug(f"æœªåœ¨å…¨å¸‚åœºåˆ—è¡¨ä¸­æ‰¾åˆ°ETFä»£ç : {etf_code}")
            return f"ETF-{etf_code}"
    except Exception as e:
        logger.error(f"è·å–ETFåç§°å¤±è´¥: {str(e)}")
        return f"ETF-{etf_code}"

def validate_etf_list(etf_list: pd.DataFrame) -> bool:
    """éªŒè¯ETFåˆ—è¡¨æ•°æ®çš„å®Œæ•´æ€§
    :param etf_list: ETFåˆ—è¡¨DataFrame
    :return: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    if etf_list.empty:
        logger.error("ETFåˆ—è¡¨ä¸ºç©º")
        return False
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = Config.ETF_STANDARD_COLUMNS
    missing_columns = [col for col in required_columns if col not in etf_list.columns]
    if missing_columns:
        logger.error(f"ETFåˆ—è¡¨ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
        return False
    
    # æ£€æŸ¥ETFä»£ç æ ¼å¼
    invalid_codes = etf_list[~etf_list["ETFä»£ç "].str.match(r'^\d{6}$')]
    if not invalid_codes.empty:
        logger.warning(f"ETFåˆ—è¡¨ä¸­å‘ç° {len(invalid_codes)} ä¸ªæ— æ•ˆETFä»£ç ")
    
    # æ£€æŸ¥åŸºé‡‘è§„æ¨¡æ˜¯å¦ä¸ºæ­£æ•°
    invalid_sizes = etf_list[etf_list["åŸºé‡‘è§„æ¨¡"] <= 0]
    if not invalid_sizes.empty:
        logger.warning(f"ETFåˆ—è¡¨ä¸­å‘ç° {len(invalid_sizes)} ä¸ªåŸºé‡‘è§„æ¨¡â‰¤0çš„ETF")
    
    return True

def repair_etf_list(etf_list: pd.DataFrame) -> pd.DataFrame:
    """ä¿®å¤ETFåˆ—è¡¨ä¸­çš„é—®é¢˜
    :param etf_list: ETFåˆ—è¡¨DataFrame
    :return: ä¿®å¤åçš„ETFåˆ—è¡¨
    """
    if etf_list.empty:
        return etf_list
    
    # åˆ›å»ºæ·±æ‹·è´
    repaired_list = etf_list.copy(deep=True)
    
    # ä¿®å¤ETFä»£ç 
    if "ETFä»£ç " in repaired_list.columns:
        # ç¡®ä¿ETFä»£ç æ˜¯å­—ç¬¦ä¸²ç±»å‹
        repaired_list["ETFä»£ç "] = repaired_list["ETFä»£ç "].astype(str)
        # ç§»é™¤éæ•°å­—å­—ç¬¦
        repaired_list["ETFä»£ç "] = repaired_list["ETFä»£ç "].str.replace(r'\D', '', regex=True)
        # ç¡®ä¿æ˜¯6ä½æ•°å­—
        repaired_list["ETFä»£ç "] = repaired_list["ETFä»£ç "].str.zfill(6)
        # è¿‡æ»¤æ— æ•ˆçš„ETFä»£ç 
        repaired_list = repaired_list[repaired_list["ETFä»£ç "].str.match(r'^\d{6}$')]
    
    # ä¿®å¤åŸºé‡‘è§„æ¨¡
    if "åŸºé‡‘è§„æ¨¡" in repaired_list.columns:
        # ç¡®ä¿åŸºé‡‘è§„æ¨¡æ˜¯æ•°å€¼ç±»å‹
        repaired_list["åŸºé‡‘è§„æ¨¡"] = pd.to_numeric(repaired_list["åŸºé‡‘è§„æ¨¡"], errors="coerce")
        # ç”¨å¹³å‡å€¼å¡«å……NaN
        if repaired_list["åŸºé‡‘è§„æ¨¡"].isna().any():
            mean_size = repaired_list["åŸºé‡‘è§„æ¨¡"].mean()
            repaired_list["åŸºé‡‘è§„æ¨¡"].fillna(mean_size, inplace=True)
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ—å
    for col in Config.ETF_STANDARD_COLUMNS:
        if col not in repaired_list.columns:
            if col == "ETFä»£ç " and "ä»£ç " in repaired_list.columns:
                repaired_list.rename(columns={"ä»£ç ": "ETFä»£ç "}, inplace=True)
            elif col == "ETFåç§°" and "åç§°" in repaired_list.columns:
                repaired_list.rename(columns={"åç§°": "ETFåç§°"}, inplace=True)
            elif col == "åŸºé‡‘è§„æ¨¡" and ("æœ€æ–°è§„æ¨¡" in repaired_list.columns or "è§„æ¨¡" in repaired_list.columns):
                if "æœ€æ–°è§„æ¨¡" in repaired_list.columns:
                    repaired_list["åŸºé‡‘è§„æ¨¡"] = repaired_list["æœ€æ–°è§„æ¨¡"]
                else:
                    repaired_list["åŸºé‡‘è§„æ¨¡"] = repaired_list["è§„æ¨¡"]
            else:
                repaired_list[col] = 0.0 if col == "åŸºé‡‘è§„æ¨¡" else ""
    
    return repaired_list
