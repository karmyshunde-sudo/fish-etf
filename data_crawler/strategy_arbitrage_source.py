# ======= 251117-1500 å¤šæ•°æ®æº-strategy_arbitrage_source-DS2.py ======

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥—åˆ©ç­–ç•¥ä¸“ç”¨æ•°æ®æºæ¨¡å— - å¤šæ•°æ®æºè½®æ¢æœºåˆ¶
è´Ÿè´£çˆ¬å–ETFå®æ—¶å¸‚åœºä»·æ ¼å’ŒIOPV(åŸºé‡‘ä»½é¢å‚è€ƒå‡€å€¼)
æ•°æ®ä¿å­˜æ ¼å¼: data/arbitrage/YYYYMMDD.csv
å¢å¼ºåŠŸèƒ½ï¼šå¢é‡ä¿å­˜æ•°æ®ã€è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®ã€æ”¯æŒæ–°ç³»ç»Ÿæ— å†å²æ•°æ®åœºæ™¯
ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨å¤šæ•°æ®æºè½®æ¢æœºåˆ¶ï¼Œé™ä½å¯¹akshareçš„ä¾èµ–
ã€é—®é¢˜ä¿®å¤ã€‘ä¿®å¤æ•°æ®åˆ—ç¼ºå¤±ã€å¼‚å¸¸æŠ˜æº¢ä»·ç‡ã€å¢å¼ºæ—¥å¿—è®°å½•
"""

import pandas as pd
import numpy as np
import logging
import akshare as ak
import yfinance as yf
import requests
import json
import os
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from config import Config
from utils.date_utils import get_beijing_time, is_trading_day, is_trading_time
from utils.file_utils import ensure_dir_exists
from data_crawler.all_etfs import get_all_etf_codes, get_etf_name

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

# ===== å¤šæ•°æ®æºé…ç½® =====
# ä¼˜å…ˆçº§é…ç½®ï¼ˆæŒ‰ç¨³å®šæ€§æ’åºï¼‰
SOURCE_PRIORITY = [
    (0, 0, 1),  # æ•°æ®æº0-æ¥å£0ï¼šè…¾è®¯è´¢ç»ï¼ˆæœ€ç¨³å®šï¼‰
    (1, 0, 2),  # æ•°æ®æº1-æ¥å£0ï¼šæ–°æµªè´¢ç»
    (2, 0, 3),  # æ•°æ®æº2-æ¥å£0ï¼šä¸œæ–¹è´¢å¯Œï¼ˆakshareï¼‰- é™çº§åˆ°ç¬¬ä¸‰ä½
    (3, 0, 4),  # æ•°æ®æº3-æ¥å£0ï¼šYahoo Financeï¼ˆæœ€ä¸ç¨³å®šï¼‰
]

# æ¨¡å—çº§å…¨å±€çŠ¶æ€
_current_priority_index = 0  # è®°å½•å½“å‰ä¼˜å…ˆçº§ä½ç½®

def clean_old_arbitrage_data(days_to_keep: int = 7) -> None:
    """
    æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„å¥—åˆ©æ•°æ®æ–‡ä»¶ï¼ˆä»…æ¸…ç†å®æ—¶è¡Œæƒ…æ•°æ®ï¼Œä¸æ¸…ç†äº¤æ˜“æµæ°´ï¼‰
    
    Args:
        days_to_keep: ä¿ç•™å¤©æ•°
    """
    try:
        arbitrage_dir = os.path.join(Config.DATA_DIR, "arbitrage")
        if not os.path.exists(arbitrage_dir):
            logger.info("å¥—åˆ©æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
            return
        
        current_date = get_beijing_time()
        logger.info(f"æ¸…ç†æ—§å¥—åˆ©æ•°æ®ï¼šä¿ç•™æœ€è¿‘ {days_to_keep} å¤©çš„æ•°æ®")
        logger.info(f"å½“å‰æ—¥æœŸ: {current_date}")
        
        files_to_keep = []
        files_to_delete = []
        
        for file_name in os.listdir(arbitrage_dir):
            if not file_name.endswith(".csv"):
                continue
                
            try:
                file_date_str = file_name.split(".")[0]
                file_date = datetime.strptime(file_date_str, "%Y%m%d")
                
                if file_date.tzinfo is None:
                    file_date = file_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                days_diff = (current_date - file_date).days
                
                logger.debug(f"æ£€æŸ¥æ–‡ä»¶: {file_name}, æ–‡ä»¶æ—¥æœŸ: {file_date}, æ—¥æœŸå·®: {days_diff}å¤©")
                
                if days_diff > days_to_keep:
                    files_to_delete.append((file_name, file_date, days_diff))
                else:
                    files_to_keep.append((file_name, file_date, days_diff))
            except (ValueError, TypeError) as e:
                logger.warning(f"è§£ææ–‡ä»¶æ—¥æœŸå¤±è´¥: {file_name}, é”™è¯¯: {str(e)}")
                continue
        
        for file_name, file_date, days_diff in files_to_delete:
            file_path = os.path.join(arbitrage_dir, file_name)
            try:
                os.remove(file_path)
                logger.info(f"å·²åˆ é™¤æ—§å¥—åˆ©æ•°æ®æ–‡ä»¶: {file_name} (æ–‡ä»¶æ—¥æœŸ: {file_date}, è¶…æœŸ: {days_diff - days_to_keep}å¤©)")
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        
        logger.info(f"ä¿ç•™å¥—åˆ©æ•°æ®æ–‡ä»¶: {len(files_to_keep)} ä¸ª")
        if files_to_keep:
            logger.debug("ä¿ç•™çš„æ–‡ä»¶åˆ—è¡¨:")
            for file_name, file_date, days_diff in files_to_keep:
                logger.debug(f"  - {file_name} (æ–‡ä»¶æ—¥æœŸ: {file_date}, å‰©ä½™ä¿ç•™å¤©æ•°: {days_to_keep - days_diff}å¤©)")
        
        logger.info(f"å·²åˆ é™¤å¥—åˆ©æ•°æ®æ–‡ä»¶: {len(files_to_delete)} ä¸ª")
    
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def append_arbitrage_data(df: pd.DataFrame) -> str:
    """
    å¢é‡ä¿å­˜å¥—åˆ©æ•°æ®åˆ°CSVæ–‡ä»¶
    
    Args:
        df: å¥—åˆ©æ•°æ®DataFrame
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        if df.empty:
            logger.warning("å¥—åˆ©æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return ""
        
        df = df.copy(deep=True)
        
        if "timestamp" not in df.columns:
            timestamp = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
            df.loc[:, "timestamp"] = timestamp
        
        arbitrage_dir = os.path.join(Config.DATA_DIR, "arbitrage")
        ensure_dir_exists(arbitrage_dir)
        
        beijing_time = get_beijing_time()
        file_date = beijing_time.strftime("%Y%m%d")
        file_path = os.path.join(arbitrage_dir, f"{file_date}.csv")
        
        if os.path.exists(file_path):
            existing_df = pd.read_csv(file_path, encoding="utf-8-sig").copy(deep=True)
            if "timestamp" in existing_df.columns:
                existing_df["timestamp"] = pd.to_datetime(existing_df["timestamp"], errors='coerce')
            
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(
                subset=["ETFä»£ç ", "timestamp"], 
                keep="last"
            )
            combined_df.to_csv(file_path, index=False, encoding="utf-8-sig")
        else:
            df.to_csv(file_path, index=False, encoding="utf-8-sig")
        
        logger.info(f"å¥—åˆ©æ•°æ®å·²å¢é‡ä¿å­˜è‡³: {file_path} (æ–°å¢{len(df)}æ¡è®°å½•)")
        return file_path
    
    except Exception as e:
        logger.error(f"å¢é‡ä¿å­˜å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return ""

def get_trading_etf_list() -> List[str]:
    """
    è·å–ç”¨äºå¥—åˆ©ç›‘æ§çš„ETFåˆ—è¡¨ï¼ˆç»Ÿä¸€æ•°æ®æºï¼‰
    
    Returns:
        List[str]: ETFä»£ç åˆ—è¡¨
    """
    try:
        etf_codes = get_all_etf_codes()
        if not etf_codes:
            logger.error("æ— æ³•è·å–ETFä»£ç åˆ—è¡¨")
            return []
        
        etf_list = pd.DataFrame({
            "ETFä»£ç ": etf_codes,
            "ETFåç§°": [get_etf_name(code) for code in etf_codes]
        })
        
        etf_list["ETFä»£ç "] = etf_list["ETFä»£ç "].astype(str)
        
        etf_list = etf_list[
            (~etf_list["ETFä»£ç "].str.startswith("511")) &  # æ’é™¤è´§å¸ETF
            (etf_list["ETFä»£ç "].str.len() == 6)  # ç¡®ä¿ä»£ç é•¿åº¦ä¸º6ä½
        ].copy()
        
        etf_list = etf_list.drop_duplicates(subset=["ETFä»£ç "])
        
        logger.info(f"ç­›é€‰åç”¨äºå¥—åˆ©ç›‘æ§çš„ETFæ•°é‡: {len(etf_list)}")
        return etf_list["ETFä»£ç "].tolist()
    except Exception as e:
        logger.error(f"è·å–äº¤æ˜“ETFåˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

def fetch_arbitrage_realtime_data() -> pd.DataFrame:
    """
    çˆ¬å–æ‰€æœ‰ETFçš„å®æ—¶å¸‚åœºä»·æ ¼å’ŒIOPVæ•°æ® - å¤šæ•°æ®æºç‰ˆæœ¬
    
    Returns:
        pd.DataFrame: åŒ…å«ETFä»£ç ã€åç§°ã€å¸‚åœºä»·æ ¼ã€IOPVç­‰ä¿¡æ¯çš„DataFrame
    """
    global _current_priority_index
    
    try:
        logger.info("=== å¼€å§‹æ‰§è¡Œå¥—åˆ©æ•°æ®çˆ¬å–ï¼ˆå¤šæ•°æ®æºè½®æ¢ï¼‰===")
        beijing_time = get_beijing_time()
        logger.info(f"å½“å‰åŒ—äº¬æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        #if not is_trading_day():
        #    logger.warning("å½“å‰ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡å¥—åˆ©æ•°æ®çˆ¬å–")
        #    return pd.DataFrame()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
        current_time = beijing_time.time()
        trading_start = datetime.strptime(Config.TRADING_START_TIME, "%H:%M").time()
        trading_end = datetime.strptime(Config.TRADING_END_TIME, "%H:%M").time()
        
        if not (trading_start <= current_time <= trading_end):
            logger.warning(f"å½“å‰ä¸æ˜¯äº¤æ˜“æ—¶é—´ ({trading_start} - {trading_end})ï¼Œè·³è¿‡å¥—åˆ©æ•°æ®çˆ¬å–")
            return pd.DataFrame()
        
        # è·å–éœ€è¦ç›‘æ§çš„ETFåˆ—è¡¨
        etf_codes = get_trading_etf_list()
        if not etf_codes:
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„ETFä»£ç åˆ—è¡¨")
            return pd.DataFrame()
        
        logger.info(f"è·å–åˆ° {len(etf_codes)} åªç¬¦åˆæ¡ä»¶çš„ETFè¿›è¡Œå¥—åˆ©ç›‘æ§")
        
        # ===== å¤šæ•°æ®æºè½®æ¢é€»è¾‘ =====
        DATA_SOURCES = [
            # æ•°æ®æº0ï¼šè…¾è®¯è´¢ç»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            {
                "name": "è…¾è®¯è´¢ç»",
                "interfaces": [
                    {
                        "name": "ETFå®æ—¶è¡Œæƒ…",
                        "func": _fetch_tencent_etf_data,
                        "delay_range": (1.0, 1.5),
                        "source_type": "tencent"
                    }
                ]
            },
            # æ•°æ®æº1ï¼šæ–°æµªè´¢ç»
            {
                "name": "æ–°æµªè´¢ç»",
                "interfaces": [
                    {
                        "name": "ETFå®æ—¶è¡Œæƒ…",
                        "func": _fetch_sina_etf_data,
                        "delay_range": (1.0, 1.5),
                        "source_type": "sina"
                    }
                ]
            },
            # æ•°æ®æº2ï¼šä¸œæ–¹è´¢å¯Œï¼ˆakshareï¼‰- é™çº§åˆ°ç¬¬ä¸‰ä½
            {
                "name": "ä¸œæ–¹è´¢å¯Œ",
                "interfaces": [
                    {
                        "name": "ETFå®æ—¶è¡Œæƒ…",
                        "func": _fetch_akshare_etf_data,
                        "delay_range": (3.0, 4.0),
                        "source_type": "akshare"
                    }
                ]
            },
            # æ•°æ®æº3ï¼šYahoo Finance
            {
                "name": "Yahoo Finance",
                "interfaces": [
                    {
                        "name": "ETFå®æ—¶è¡Œæƒ…",
                        "func": _fetch_yfinance_etf_data,
                        "delay_range": (2.0, 2.5),
                        "source_type": "yfinance"
                    }
                ]
            }
        ]
        
        # æ™ºèƒ½è½®æ¢é€»è¾‘
        success = False
        result_df = pd.DataFrame()
        last_error = None
        total_priority = len(SOURCE_PRIORITY)
        
        for offset in range(total_priority):
            priority_idx = (_current_priority_index + offset) % total_priority
            ds_idx, if_idx, _ = SOURCE_PRIORITY[priority_idx]
            
            if ds_idx >= len(DATA_SOURCES) or if_idx >= len(DATA_SOURCES[ds_idx]["interfaces"]):
                continue
                
            source = DATA_SOURCES[ds_idx]
            interface = source["interfaces"][if_idx]
            
            try:
                func = interface["func"]
                
                # åŠ¨æ€å»¶æ—¶
                delay_min, delay_max = interface["delay_range"]
                if priority_idx < 2:  # å‰ä¸¤ä¸ªä¼˜å…ˆçº§
                    delay_factor = 0.8
                elif priority_idx < 4:  # ä¸­é—´ä¸¤ä¸ªä¼˜å…ˆçº§
                    delay_factor = 1.0
                else:
                    delay_factor = 1.2
                
                time.sleep(random.uniform(delay_min * delay_factor, delay_max * delay_factor))
                
                logger.debug(f"å°è¯• [{source['name']}->{interface['name']}] è·å–ETFå®æ—¶æ•°æ® "
                            f"(ä¼˜å…ˆçº§: {priority_idx+1}/{total_priority})")
                
                # è°ƒç”¨æ¥å£
                df = func(etf_codes)
                
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if df is None or df.empty:
                    raise ValueError("è¿”å›ç©ºæ•°æ®")
                
                # æ•°æ®æ ‡å‡†åŒ–
                df = _standardize_etf_data(df, interface["source_type"], logger)
                
                # æ£€æŸ¥æ ‡å‡†åŒ–åæ•°æ®
                required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
                if not all(col in df.columns for col in required_columns):
                    missing = [col for col in required_columns if col not in df.columns]
                    raise ValueError(f"æ ‡å‡†åŒ–åä»ç¼ºå¤±å¿…è¦åˆ—: {', '.join(missing)}")
                
                # ä¿å­˜æˆåŠŸçŠ¶æ€
                result_df = df
                success = True
                _current_priority_index = priority_idx  # é”å®šå½“å‰ä¼˜å…ˆçº§
                logger.info(f"âœ… ã€{source['name']}->{interface['name']}] æˆåŠŸè·å– {len(result_df)} æ¡ETFå®æ—¶æ•°æ® (é”å®šä¼˜å…ˆçº§: {priority_idx+1})")
                break
                
            except Exception as e:
                last_error = e
                logger.error(f"âŒ [{source['name']}->{interface['name']}] å¤±è´¥: {str(e)}", exc_info=True)
                continue
        
        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        if not success:
            logger.error(f"æ‰€æœ‰æ•°æ®æºå‡æ— æ³•è·å–ETFå®æ—¶æ•°æ®: {str(last_error)}")
            _current_priority_index = (_current_priority_index + 1) % total_priority
            return pd.DataFrame()
        
        return result_df
    
    except Exception as e:
        logger.error(f"çˆ¬å–å¥—åˆ©å®æ—¶æ•°æ®è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}", exc_info=True)
        return pd.DataFrame()

def _fetch_tencent_etf_data(etf_codes: List[str]) -> pd.DataFrame:
    """ä»è…¾è®¯è´¢ç»è·å–ETFå®æ—¶æ•°æ®"""
    try:
        logger.info("å°è¯•ä»è…¾è®¯è´¢ç»è·å–ETFå®æ—¶æ•°æ®")
        
        # è…¾è®¯è´¢ç»ETFå®æ—¶æ•°æ®API
        base_url = "http://qt.gtimg.cn/q="
        
        all_data = []
        for code in etf_codes:
            try:
                # æ„å»ºä»£ç æ ¼å¼
                if code.startswith('5'):
                    tencent_code = f"sh{code}"
                else:
                    tencent_code = f"sz{code}"
                
                url = f"{base_url}{tencent_code}"
                response = requests.get(url, timeout=10)
                
                if response.status_code != 200:
                    continue
                
                content = response.text
                logger.debug(f"è…¾è®¯è´¢ç»åŸå§‹è¿”å›æ•°æ®: {content}")
                
                if not content or "pv_none_match" in content:
                    continue
                
                # è§£ææ•°æ®æ ¼å¼: v_sh510050="1~åå¤ä¸Šè¯50ETF~510050~2.345~2.350~2.340..."
                parts = content.split('~')
                if len(parts) < 40:
                    continue
                
                # æå–å…³é”®æ•°æ®
                etf_name = parts[1] if len(parts) > 1 else ""
                current_price = float(parts[3]) if len(parts) > 3 and parts[3] else 0
                iopv = float(parts[38]) if len(parts) > 38 and parts[38] else current_price  # IOPVåœ¨è…¾è®¯æ•°æ®ä¸­çš„ä½ç½®å¯èƒ½ä¸åŒ
                
                if current_price > 0:
                    all_data.append({
                        "ETFä»£ç ": code,
                        "ETFåç§°": etf_name,
                        "å¸‚åœºä»·æ ¼": current_price,
                        "IOPV": iopv,
                        "æ”¶ç›˜": current_price,  # æ·»åŠ æ”¶ç›˜ä»·åˆ—
                        "æ—¥æœŸ": get_beijing_time().strftime("%Y-%m-%d")  # æ·»åŠ æ—¥æœŸåˆ—
                    })
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.1)
                
            except Exception as e:
                logger.debug(f"è·å–ETF {code} æ•°æ®å¤±è´¥: {str(e)}")
                continue
        
        df = pd.DataFrame(all_data)
        if not df.empty:
            logger.info(f"è…¾è®¯è´¢ç»è·å–åˆ° {len(df)} åªETFçš„å®æ—¶æ•°æ®")
            logger.info(f"è…¾è®¯è´¢ç»æ•°æ®æ ·æœ¬: {df.iloc[0].to_dict() if len(df) > 0 else 'æ— æ•°æ®'}")
        return df
        
    except Exception as e:
        logger.error(f"è…¾è®¯è´¢ç»ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise ValueError(f"è…¾è®¯è´¢ç»ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")

def _fetch_sina_etf_data(etf_codes: List[str]) -> pd.DataFrame:
    """ä»æ–°æµªè´¢ç»è·å–ETFå®æ—¶æ•°æ®"""
    try:
        logger.info("å°è¯•ä»æ–°æµªè´¢ç»è·å–ETFå®æ—¶æ•°æ®")
        
        # æ–°æµªè´¢ç»ETFå®æ—¶æ•°æ®API
        base_url = "http://hq.sinajs.cn/list="
        
        all_data = []
        batch_size = 50  # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…URLè¿‡é•¿
        
        for i in range(0, len(etf_codes), batch_size):
            batch_codes = etf_codes[i:i + batch_size]
            
            # æ„å»ºä»£ç åˆ—è¡¨
            code_list = []
            for code in batch_codes:
                if code.startswith('5'):
                    sina_code = f"sh{code}"
                else:
                    sina_code = f"sz{code}"
                code_list.append(sina_code)
            
            url = f"{base_url}{','.join(code_list)}"
            response = requests.get(url, timeout=15)
            
            if response.status_code != 200:
                continue
            
            content = response.text
            logger.debug(f"æ–°æµªè´¢ç»åŸå§‹è¿”å›æ•°æ®: {content}")
            
            lines = content.split(';')
            
            for line in lines:
                if not line.strip():
                    continue
                
                try:
                    # è§£ææ•°æ®æ ¼å¼: var hq_str_sh510050="åå¤ä¸Šè¯50ETF,2.345,2.350,2.340,...";
                    parts = line.split('="')
                    if len(parts) < 2:
                        continue
                    
                    code_part = parts[0].split('_')[-1]
                    data_part = parts[1].rstrip('";')
                    
                    data_items = data_part.split(',')
                    if len(data_items) < 30:
                        continue
                    
                    # æå–è‚¡ç¥¨ä»£ç 
                    etf_code = code_part[2:]  # å»æ‰å¸‚åœºå‰ç¼€
                    
                    # æå–å…³é”®æ•°æ®
                    etf_name = data_items[0] if data_items[0] else ""
                    current_price = float(data_items[3]) if len(data_items) > 3 and data_items[3] else 0
                    
                    if current_price > 0:
                        all_data.append({
                            "ETFä»£ç ": etf_code,
                            "ETFåç§°": etf_name,
                            "å¸‚åœºä»·æ ¼": current_price,
                            "IOPV": current_price,  # æ–°æµªæ•°æ®ä¸­IOPVå¯èƒ½éœ€è¦å…¶ä»–æ–¹å¼è·å–
                            "æ”¶ç›˜": current_price,  # æ·»åŠ æ”¶ç›˜ä»·åˆ—
                            "æ—¥æœŸ": get_beijing_time().strftime("%Y-%m-%d")  # æ·»åŠ æ—¥æœŸåˆ—
                        })
                        
                except Exception as e:
                    logger.debug(f"è§£æETFæ•°æ®å¤±è´¥: {str(e)}")
                    continue
            
            # æ‰¹æ¬¡é—´å»¶æ—¶
            time.sleep(0.5)
        
        df = pd.DataFrame(all_data)
        if not df.empty:
            logger.info(f"æ–°æµªè´¢ç»è·å–åˆ° {len(df)} åªETFçš„å®æ—¶æ•°æ®")
            logger.info(f"æ–°æµªè´¢ç»æ•°æ®æ ·æœ¬: {df.iloc[0].to_dict() if len(df) > 0 else 'æ— æ•°æ®'}")
        return df
        
    except Exception as e:
        logger.error(f"æ–°æµªè´¢ç»ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise ValueError(f"æ–°æµªè´¢ç»ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")

def _fetch_akshare_etf_data(etf_codes: List[str]) -> pd.DataFrame:
    """ä»ä¸œæ–¹è´¢å¯Œï¼ˆakshareï¼‰è·å–ETFå®æ—¶æ•°æ®"""
    try:
        logger.info("å°è¯•ä»ä¸œæ–¹è´¢å¯Œè·å–ETFå®æ—¶æ•°æ®")
        
        # ä½¿ç”¨akshareè·å–æ‰€æœ‰ETFå®æ—¶æ•°æ®
        df = ak.fund_etf_spot_em()
        
        logger.info(f"fund_etf_spot_em æ¥å£è¿”å›åˆ—å: {df.columns.tolist()}")
        if not df.empty:
            logger.info(f"akshareåŸå§‹æ•°æ®æ ·æœ¬: {df.iloc[0].to_dict() if len(df) > 0 else 'æ— æ•°æ®'}")
        
        if df.empty:
            logger.error("AkShareæœªè¿”å›ETFå®æ—¶è¡Œæƒ…æ•°æ®")
            return pd.DataFrame()
        
        df = df.copy(deep=True)
        
        # è¿‡æ»¤å‡ºéœ€è¦çš„ETF
        df = df[df['ä»£ç '].isin(etf_codes)].copy(deep=True)
        
        if df.empty:
            logger.warning("ç­›é€‰åæ— ç¬¦åˆæ¡ä»¶çš„ETFæ•°æ®")
            return pd.DataFrame()
        
        # é‡å‘½ååˆ—å
        column_mapping = {
            'ä»£ç ': 'ETFä»£ç ',
            'åç§°': 'ETFåç§°',
            'æœ€æ–°ä»·': 'å¸‚åœºä»·æ ¼',
            'IOPVå®æ—¶ä¼°å€¼': 'IOPV',
            'åŸºé‡‘æŠ˜ä»·ç‡': 'æŠ˜æº¢ä»·ç‡',
            'æ›´æ–°æ—¶é—´': 'å‡€å€¼æ—¶é—´'
        }
        
        available_columns = [col for col in column_mapping.keys() if col in df.columns]
        df = df[available_columns].rename(columns=column_mapping).copy(deep=True)
        
        df["ETFä»£ç "] = df["ETFä»£ç "].astype(str)
        
        beijing_time = get_beijing_time()
        df.loc[:, 'è®¡ç®—æ—¶é—´'] = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
        df.loc[:, 'æ”¶ç›˜'] = df['å¸‚åœºä»·æ ¼']  # æ·»åŠ æ”¶ç›˜ä»·åˆ—
        df.loc[:, 'æ—¥æœŸ'] = beijing_time.strftime("%Y-%m-%d")  # æ·»åŠ æ—¥æœŸåˆ—
        
        logger.info(f"ä¸œæ–¹è´¢å¯Œè·å–æˆåŠŸ: {len(df)} åªETFçš„å®æ—¶æ•°æ®")
        if not df.empty:
            logger.info(f"ä¸œæ–¹è´¢å¯Œå¤„ç†åçš„æ•°æ®æ ·æœ¬: {df.iloc[0].to_dict()}")
        return df
        
    except Exception as e:
        logger.error(f"ä¸œæ–¹è´¢å¯ŒETFæ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise ValueError(f"ä¸œæ–¹è´¢å¯ŒETFæ•°æ®è·å–å¤±è´¥: {str(e)}")

def _fetch_yfinance_etf_data(etf_codes: List[str]) -> pd.DataFrame:
    """ä»Yahoo Financeè·å–ETFå®æ—¶æ•°æ®"""
    try:
        logger.info("å°è¯•ä»Yahoo Financeè·å–ETFå®æ—¶æ•°æ®")
        
        all_data = []
        
        for code in etf_codes:
            try:
                # è½¬æ¢ä»£ç æ ¼å¼
                if code.startswith('5'):
                    yf_symbol = f"{code}.SS"
                else:
                    yf_symbol = f"{code}.SZ"
                
                # è·å–å®æ—¶æ•°æ®
                etf = yf.Ticker(yf_symbol)
                info = etf.info
                history = etf.history(period="1d")
                
                if history.empty:
                    continue
                
                current_price = history['Close'].iloc[-1]
                etf_name = info.get('longName', '') or info.get('shortName', '')
                
                # Yahoo Financeå¯èƒ½ä¸æä¾›IOPVï¼Œä½¿ç”¨å½“å‰ä»·æ ¼ä½œä¸ºè¿‘ä¼¼å€¼
                iopv = current_price
                
                if current_price > 0:
                    all_data.append({
                        "ETFä»£ç ": code,
                        "ETFåç§°": etf_name,
                        "å¸‚åœºä»·æ ¼": current_price,
                        "IOPV": iopv,
                        "æ”¶ç›˜": current_price,  # æ·»åŠ æ”¶ç›˜ä»·åˆ—
                        "æ—¥æœŸ": get_beijing_time().strftime("%Y-%m-%d")  # æ·»åŠ æ—¥æœŸåˆ—
                    })
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.2)
                
            except Exception as e:
                logger.debug(f"è·å–ETF {code} æ•°æ®å¤±è´¥: {str(e)}")
                continue
        
        df = pd.DataFrame(all_data)
        if not df.empty:
            logger.info(f"Yahoo Financeè·å–åˆ° {len(df)} åªETFçš„å®æ—¶æ•°æ®")
            logger.info(f"Yahoo Financeæ•°æ®æ ·æœ¬: {df.iloc[0].to_dict() if len(df) > 0 else 'æ— æ•°æ®'}")
        return df
        
    except Exception as e:
        logger.error(f"Yahoo Finance ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise ValueError(f"Yahoo Finance ETFæ•°æ®è·å–å¤±è´¥: {str(e)}")

def _standardize_etf_data(df: pd.DataFrame, source_type: str, logger) -> pd.DataFrame:
    """æ ‡å‡†åŒ–ETFå®æ—¶æ•°æ®æ ¼å¼"""
    
    if df.empty:
        return df
    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV", "æ”¶ç›˜", "æ—¥æœŸ"]
    
    # æ ¹æ®æ•°æ®æºç±»å‹å¤„ç†
    if source_type == "akshare":
        # akshareæ•°æ®å·²ç»è¿‡åˆæ­¥å¤„ç†ï¼Œåªéœ€ç¡®ä¿æ ¼å¼
        df = df.rename(columns={
            "ä»£ç ": "ETFä»£ç ",
            "åç§°": "ETFåç§°",
            "æœ€æ–°ä»·": "å¸‚åœºä»·æ ¼",
            "IOPVå®æ—¶ä¼°å€¼": "IOPV"
        })
    
    # ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å€¼ç±»å‹
    numeric_columns = ["å¸‚åœºä»·æ ¼", "IOPV", "æ”¶ç›˜"]
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # è¿‡æ»¤æ— æ•ˆæ•°æ®
    df = df[
        (df["å¸‚åœºä»·æ ¼"] > 0) & 
        (df["IOPV"] > 0) &
        (df["æ”¶ç›˜"] > 0) &
        (df["ETFä»£ç "].notna()) &
        (df["ETFåç§°"].notna())
    ].copy()
    
    # è®¡ç®—æŠ˜ä»·ç‡
    if "å¸‚åœºä»·æ ¼" in df.columns and "IOPV" in df.columns:
        df["æŠ˜ä»·ç‡"] = ((df["å¸‚åœºä»·æ ¼"] - df["IOPV"]) / df["IOPV"]) * 100
        
        # æ£€æŸ¥å¼‚å¸¸æŠ˜æº¢ä»·ç‡ - æ­£å¸¸èŒƒå›´åº”è¯¥åœ¨ -20% åˆ° +20% ä¹‹é—´
        abnormal_discount = df[df["æŠ˜ä»·ç‡"] < -20]
        abnormal_premium = df[df["æŠ˜ä»·ç‡"] > 20]
        
        if len(abnormal_discount) > 0:
            logger.error(f"âš ï¸ å‘ç° {len(abnormal_discount)} ä¸ªå¼‚å¸¸æŠ˜ä»·ç‡ (<-20%): {abnormal_discount[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
            logger.error("è¿™å¯èƒ½è¡¨æ˜æ•°æ®æºæˆ–è®¡ç®—é€»è¾‘æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼")
        
        if len(abnormal_premium) > 0:
            logger.error(f"âš ï¸ å‘ç° {len(abnormal_premium)} ä¸ªå¼‚å¸¸æº¢ä»·ç‡ (>20%): {abnormal_premium[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
            logger.error("è¿™å¯èƒ½è¡¨æ˜æ•°æ®æºæˆ–è®¡ç®—é€»è¾‘æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼")
        
        # è®°å½•æŠ˜ä»·ç‡ç»Ÿè®¡ä¿¡æ¯
        if not df.empty:
            min_discount = df["æŠ˜ä»·ç‡"].min()
            max_discount = df["æŠ˜ä»·ç‡"].max()
            avg_discount = df["æŠ˜ä»·ç‡"].mean()
            logger.info(f"æŠ˜ä»·ç‡ç»Ÿè®¡ - æœ€å°å€¼: {min_discount:.2f}%, æœ€å¤§å€¼: {max_discount:.2f}%, å¹³å‡å€¼: {avg_discount:.2f}%")
            
            # å¦‚æœå‡ºç°æç«¯å€¼ï¼Œå‘å‡ºä¸¥é‡è­¦å‘Š
            if min_discount < -50 or max_discount > 50:
                logger.critical("ğŸš¨ å‘ç°æç«¯æŠ˜æº¢ä»·ç‡ï¼è¿™å‡ ä¹è‚¯å®šæ˜¯æ•°æ®é”™è¯¯ï¼Œè¯·ç«‹å³æ£€æŸ¥æ•°æ®æºå’Œè®¡ç®—é€»è¾‘ï¼")
    
    # ç¡®ä¿æ‰€æœ‰å¿…è¦åˆ—å­˜åœ¨
    for col in required_columns:
        if col not in df.columns:
            if col == "æ”¶ç›˜" and "å¸‚åœºä»·æ ¼" in df.columns:
                df[col] = df["å¸‚åœºä»·æ ¼"]
            elif col == "æ—¥æœŸ":
                df[col] = get_beijing_time().strftime("%Y-%m-%d")
            else:
                df[col] = np.nan
    
    # æ·»åŠ æˆäº¤é¢åˆ—ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
    if "æˆäº¤é¢" not in df.columns:
        df["æˆäº¤é¢"] = 0  # é»˜è®¤å€¼
    
    # æ·»åŠ æŒ¯å¹…åˆ—ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
    if "æŒ¯å¹…" not in df.columns:
        df["æŒ¯å¹…"] = 0  # é»˜è®¤å€¼
    
    # ç§»é™¤å®Œå…¨æ— æ•ˆçš„è¡Œ
    df = df.dropna(subset=["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"])
    
    logger.info(f"æ ‡å‡†åŒ–åæ•°æ®: {len(df)} æ¡æœ‰æ•ˆè®°å½•")
    
    return df

def load_arbitrage_data(date_str: Optional[str] = None) -> pd.DataFrame:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸçš„å¥—åˆ©æ•°æ®
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
    
    Returns:
        pd.DataFrame: å¥—åˆ©æ•°æ®
    """
    try:
        beijing_time = get_beijing_time()
        
        if not date_str:
            date_str = beijing_time.strftime("%Y%m%d")
        
        arbitrage_dir = os.path.join(Config.DATA_DIR, "arbitrage")
        file_path = os.path.join(arbitrage_dir, f"{date_str}.csv")
        
        if not os.path.exists(file_path):
            logger.debug(f"å¥—åˆ©æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path, encoding="utf-8-sig").copy(deep=True)
        
        if "ETFä»£ç " in df.columns:
            df["ETFä»£ç "] = df["ETFä»£ç "].astype(str)
        
        if "timestamp" in df.columns:
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors='coerce')
        
        logger.debug(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®: {file_path} (å…±{len(df)}æ¡è®°å½•)")
        return df
    
    except Exception as e:
        logger.error(f"åŠ è½½å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def crawl_arbitrage_data() -> str:
    """
    æ‰§è¡Œå¥—åˆ©æ•°æ®çˆ¬å–å¹¶ä¿å­˜
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡Œå¥—åˆ©æ•°æ®çˆ¬å–ï¼ˆå¤šæ•°æ®æºï¼‰===")
        beijing_time = get_beijing_time()
        logger.info(f"å½“å‰åŒ—äº¬æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥å’Œäº¤æ˜“æ—¶é—´
        #if not is_trading_day():
        #    logger.warning("å½“å‰ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡å¥—åˆ©æ•°æ®çˆ¬å–")
        #    return ""
        
        current_time = beijing_time.time()
        trading_start = datetime.strptime(Config.TRADING_START_TIME, "%H:%M").time()
        trading_end = datetime.strptime(Config.TRADING_END_TIME, "%H:%M").time()
        
        if not (trading_start <= current_time <= trading_end):
            logger.warning(f"å½“å‰ä¸æ˜¯äº¤æ˜“æ—¶é—´ ({trading_start} - {trading_end})ï¼Œè·³è¿‡å¥—åˆ©æ•°æ®çˆ¬å–")
            return ""
        
        # ä½¿ç”¨å¤šæ•°æ®æºçˆ¬å–æ•°æ®
        df = fetch_arbitrage_realtime_data()
        
        if df.empty:
            logger.warning("æœªè·å–åˆ°æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®ï¼Œçˆ¬å–ç»“æœä¸ºç©º")
            return ""
        else:
            logger.info(f"æˆåŠŸè·å– {len(df)} åªETFçš„å®æ—¶æ•°æ®")
        
        return append_arbitrage_data(df)
    
    except Exception as e:
        logger.error(f"å¥—åˆ©æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        return ""

def get_latest_arbitrage_opportunities() -> pd.DataFrame:
    """
    è·å–æœ€æ–°çš„å¥—åˆ©æœºä¼šæ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼‰
    
    Returns:
        pd.DataFrame: åŸå§‹å¥—åˆ©æ•°æ®ï¼Œä¸åšä»»ä½•ç­›é€‰å’Œæ’åº
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        #if not is_trading_day():
        #    logger.warning("å½“å‰ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡è·å–å¥—åˆ©æœºä¼š")
        #    return pd.DataFrame()
        
        beijing_time = get_beijing_time()
        today = beijing_time.strftime("%Y%m%d")
        
        # å°è¯•åŠ è½½ä»Šå¤©çš„å¥—åˆ©æ•°æ®
        df = load_arbitrage_data(today)
        
        # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œå°è¯•é‡æ–°çˆ¬å–
        if df.empty:
            logger.warning("æ— ä»Šæ—¥å¥—åˆ©æ•°æ®ï¼Œå°è¯•é‡æ–°çˆ¬å–")
            file_path = crawl_arbitrage_data()
            
            if file_path and os.path.exists(file_path):
                logger.info(f"æˆåŠŸçˆ¬å–å¹¶ä¿å­˜å¥—åˆ©æ•°æ®åˆ°: {file_path}")
                df = load_arbitrage_data(today)
            else:
                logger.warning("é‡æ–°çˆ¬å–åä»æ— å¥—åˆ©æ•°æ®")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if df.empty:
            logger.warning("åŠ è½½çš„å¥—åˆ©æ•°æ®ä¸ºç©ºï¼Œå°†å°è¯•åŠ è½½æœ€è¿‘æœ‰æ•ˆæ•°æ®")
            df = load_latest_valid_arbitrage_data()
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if df.empty:
            logger.error("æ— æ³•è·å–ä»»ä½•æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®")
            return pd.DataFrame()
        
        df = df.copy(deep=True)
        
        if "ETFä»£ç " in df.columns:
            df["ETFä»£ç "] = df["ETFä»£ç "].astype(str)
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV", "æ”¶ç›˜", "æ—¥æœŸ"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            logger.debug(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        # ç¡®ä¿æ•°æ®è´¨é‡
        df = df.dropna(subset=["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"])
        
        # è®°å½•æœ€ç»ˆæ•°æ®é‡
        logger.info(f"æˆåŠŸåŠ è½½ {len(df)} æ¡åŸå§‹å¥—åˆ©æ•°æ®")
        return df
    
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def load_latest_valid_arbitrage_data(days_back: int = 7) -> pd.DataFrame:
    """
    åŠ è½½æœ€è¿‘æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®
    
    Args:
        days_back: å‘å‰æŸ¥æ‰¾çš„å¤©æ•°
    
    Returns:
        pd.DataFrame: æœ€è¿‘æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®
    """
    try:
        beijing_now = get_beijing_time()
        
        for i in range(days_back):
            date = (beijing_now - timedelta(days=i)).strftime("%Y%m%d")
            df = load_arbitrage_data(date)
            
            if not df.empty:
                df = df.copy(deep=True)
                
                required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV", "æ”¶ç›˜", "æ—¥æœŸ"]
                if all(col in df.columns for col in required_columns) and len(df) > 0:
                    logger.info(f"æ‰¾åˆ°æœ‰æ•ˆå†å²å¥—åˆ©æ•°æ®: {date}, å…± {len(df)} ä¸ªæœºä¼š")
                    return df
        
        logger.warning(f"åœ¨æœ€è¿‘ {days_back} å¤©å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®")
        return pd.DataFrame()
    
    except Exception as e:
        logger.error(f"åŠ è½½æœ€è¿‘æœ‰æ•ˆå¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Config.init_dirs()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger.info("å¥—åˆ©æ•°æ®æºæ¨¡å—åˆå§‹åŒ–å®Œæˆï¼ˆå¤šæ•°æ®æºç‰ˆæœ¬ï¼‰")
    
    # æ¸…ç†è¿‡æœŸçš„å¥—åˆ©æ•°æ®
    try:
        clean_old_arbitrage_data(days_to_keep=7)
        logger.info("å·²æ¸…ç†è¶…è¿‡7å¤©çš„å¥—åˆ©æ•°æ®æ–‡ä»¶")
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§å¥—åˆ©æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
    
except Exception as e:
    error_msg = f"å¥—åˆ©æ•°æ®æºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    logger.error(error_msg, exc_info=True)
    
    try:
        # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
        logging.basicConfig(
            level="INFO",
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler()]
        )
        logging.error(error_msg)
    except Exception as basic_log_error:
        print(f"åŸºç¡€æ—¥å¿—é…ç½®å¤±è´¥: {str(basic_log_error)}")
        print(error_msg)
