#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‡æ•° Yes/No ç­–ç•¥æ‰§è¡Œå™¨
æ¯å¤©è®¡ç®—æŒ‡å®šæŒ‡æ•°çš„ç­–ç•¥ä¿¡å·å¹¶æ¨é€å¾®ä¿¡é€šçŸ¥
"""

import os
import logging
import pandas as pd
import akshare as ak
import time
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time
from wechat_push.push import send_wechat_message

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# æŒ‡å®šè®¡ç®—çš„æŒ‡æ•°åˆ—è¡¨ï¼ˆç¡¬ç¼–ç ï¼ŒåŒ…å«å®Œæ•´ç­–ç•¥ä¿¡æ¯ï¼‰
INDICES = [
    # æ–°å¢çš„4ä¸ªETFæ”¾åœ¨æœ€å‰é¢
    {
        "code": "^NDX",
        "name": "çº³æ–¯è¾¾å…‹100",
        "akshare_code": "^NDX",
        "etf_code": "159892",
        "etf_name": "åå¤çº³æ–¯è¾¾å…‹100ETF",
        "description": "è·Ÿè¸ªçº³æ–¯è¾¾å…‹100æŒ‡æ•°ï¼Œç¾è‚¡ç§‘æŠ€é¾™å¤´"
    },
    {
        "code": "^NDX",
        "name": "çº³æ–¯è¾¾å…‹100",
        "akshare_code": "^NDX",
        "etf_code": "513100",
        "etf_name": "å›½æ³°çº³æ–¯è¾¾å…‹100ETF",
        "description": "è·Ÿè¸ªçº³æ–¯è¾¾å…‹100æŒ‡æ•°ï¼Œç¾è‚¡ç§‘æŠ€é¾™å¤´"
    },
    {
        "code": "H30533.CSI",
        "name": "ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘",
        "akshare_code": "H30533.CSI",
        "etf_code": "513500",
        "etf_name": "æ˜“æ–¹è¾¾ä¸­æ¦‚äº’è”ç½‘ETF",
        "description": "è·Ÿè¸ªä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°ï¼Œæ¶µç›–æµ·å¤–ä¸Šå¸‚ä¸­æ¦‚è‚¡"
    },
    {
        "code": "^HSI",  # å·²ä¿®æ”¹ä¸ºæ’ç”ŸæŒ‡æ•°ä»£ç 
        "name": "æ’ç”ŸæŒ‡æ•°",
        "akshare_code": "^HSI",
        "etf_code": "513400",
        "etf_name": "åå¤æ’ç”Ÿäº’è”ç½‘ETF",
        "description": "è·Ÿè¸ªæ’ç”ŸæŒ‡æ•°ï¼Œæ¸¯è‚¡é¾™å¤´"  # å·²ä¿®æ”¹æè¿°
    },
    
    # åŸæœ‰ETFåˆ—è¡¨ï¼Œä¿æŒå®Œå…¨ä¸å˜
    {
        "code": "000300",
        "name": "æ²ªæ·±300",
        "akshare_code": "sh000300",
        "etf_code": "510300",
        "etf_name": "åæ³°æŸç‘æ²ªæ·±300ETF",
        "description": "å®½åŸºæ ¸å¿ƒï¼Œæ—¥å‡æˆäº¤é¢è¶…10äº¿"
    },
    {
        "code": "000905",
        "name": "ä¸­è¯500",
        "akshare_code": "sh000905",
        "etf_code": "510500",
        "etf_name": "å—æ–¹ä¸­è¯500ETF",
        "description": "ä¸­è¯500æµåŠ¨æ€§æ ‡æ†ETF"
    },
    {
        "code": "000688",
        "name": "ç§‘åˆ›50",
        "akshare_code": "sh000688",
        "etf_code": "588000",
        "etf_name": "åå¤ç§‘åˆ›50ETF",
        "description": "ç§‘åˆ›æ¿æ ¸å¿ƒå®½åŸºETF"
    },
    {
        "code": "399006",
        "name": "åˆ›ä¸šæ¿æŒ‡æ•°",
        "akshare_code": "sz399006",
        "etf_code": "159915",
        "etf_name": "æ˜“æ–¹è¾¾åˆ›ä¸šæ¿ETF",
        "description": "åˆ›ä¸šæ¿è§„æ¨¡æœ€å¤§ETFä¹‹ä¸€"
    },
    {
        "code": "399005",
        "name": "ä¸­å°æ¿æŒ‡æ•°",
        "akshare_code": "sz399005",
        "etf_code": "159902",
        "etf_name": "åå¤ä¸­å°æ¿ETF",
        "description": "è·Ÿè¸ªä¸­å°æ¿å…¨æŒ‡"
    },
    {
        "code": "399395",
        "name": "å›½è¯æœ‰è‰²é‡‘å±",
        "akshare_code": "sz399395",
        "etf_code": "512400",
        "etf_name": "å—æ–¹æœ‰è‰²é‡‘å±ETF",
        "description": "è¦†ç›–æœ‰è‰²å…¨äº§ä¸šé“¾"
    },
    {
        "code": "399967",
        "name": "ä¸­è¯å†›å·¥æŒ‡æ•°",
        "akshare_code": "sz399967",
        "etf_code": "512660",
        "etf_name": "å¯Œå›½ä¸­è¯å†›å·¥ETF",
        "description": "å†›å·¥è¡Œä¸šè§„æ¨¡é¢†å…ˆETF"
    },
    {
        "code": "399975",
        "name": "ä¸­è¯è¯åˆ¸æŒ‡æ•°",
        "akshare_code": "sz399975",
        "etf_code": "512880",
        "etf_name": "å›½æ³°ä¸­è¯å…¨æŒ‡è¯åˆ¸å…¬å¸ETF",
        "description": "è¯åˆ¸è¡Œä¸šæµåŠ¨æ€§é¦–é€‰"
    },
    {
        "code": "930713",
        "name": "ä¸­è¯AIäº§ä¸š",
        "akshare_code": "sh930713",
        "etf_code": "515070",
        "etf_name": "åå¤ä¸­è¯AIäº§ä¸šETF",
        "description": "AIå…¨äº§ä¸šé“¾è¦†ç›–"
    },
    {
        "code": "990001",
        "name": "ä¸­è¯å…¨æŒ‡åŠå¯¼ä½“",
        "akshare_code": "sh990001",
        "etf_code": "159813",
        "etf_name": "å›½æ³°CESåŠå¯¼ä½“ETF",
        "description": "åŠå¯¼ä½“è¡Œä¸šä¸»æµæ ‡çš„"
    },
    {
        "code": "000821",
        "name": "ä¸­è¯çº¢åˆ©ä½æ³¢åŠ¨æŒ‡æ•°",
        "akshare_code": "sh000821",
        "etf_code": "515450",
        "etf_name": "åæ³°æŸç‘ä¸­è¯çº¢åˆ©ä½æ³¢åŠ¨ETF",
        "description": "ç¨³å¥å‹çº¢åˆ©ç±»ETF"
    },
    {
        "code": "000829",
        "name": "ä¸Šæµ·é‡‘ETFæŒ‡æ•°",
        "akshare_code": "sh000829",
        "etf_code": "518850",
        "etf_name": "åå®‰é»„é‡‘ETF",
        "description": "å›½å†…è§„æ¨¡æœ€å¤§é»„é‡‘ETF"
    },
    {
        "code": "000012",
        "name": "ä¸Šè¯å›½å€ºæŒ‡æ•°",
        "akshare_code": "sh000012",
        "etf_code": "511260",
        "etf_name": "åšæ—¶ä¸Šè¯å›½å€ºETF",
        "description": "è·Ÿè¸ªä¸Šè¯å›½å€ºæŒ‡æ•°ï¼Œä½æ³¢åŠ¨"
    }
]

# ç­–ç•¥å‚æ•°
CRITICAL_VALUE_DAYS = 20  # è®¡ç®—ä¸´ç•Œå€¼çš„å‘¨æœŸï¼ˆ20æ—¥å‡çº¿ï¼‰
DEVIATION_THRESHOLD = 0.02  # åç¦»é˜ˆå€¼ï¼ˆ2%ï¼‰
PATTERN_CONFIDENCE_THRESHOLD = 0.7  # å½¢æ€ç¡®è®¤é˜ˆå€¼ï¼ˆ70%ç½®ä¿¡åº¦ï¼‰

def check_network_connection():
    """æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸"""
    try:
        import requests
        response = requests.get('https://www.baidu.com  ', timeout=5)
        return response.status_code == 200
    except Exception:
        return False

def fetch_index_data(index_code: str, days: int = 250) -> pd.DataFrame:
    """
    ä»å¯é æ•°æ®æºè·å–æŒ‡æ•°å†å²æ•°æ®
    
    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚"000300"ï¼‰
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # è®¡ç®—æ—¥æœŸèŒƒå›´ - ä¿æŒä¸ºdatetimeå¯¹è±¡
        end_date_dt = datetime.now()
        start_date_dt = end_date_dt - timedelta(days=days)
        
        # ä»…åœ¨éœ€è¦å­—ç¬¦ä¸²æ—¶è½¬æ¢
        end_date = end_date_dt.strftime("%Y%m%d")
        start_date = start_date_dt.strftime("%Y%m%d")
        
        logger.info(f"è·å–æŒ‡æ•° {index_code} æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        
        # ç‰¹æ®Šå¤„ç†æ’ç”ŸæŒ‡æ•°
        if index_code == "^HSI":
            logger.info("ç‰¹æ®Šå¤„ç†æ’ç”ŸæŒ‡æ•°: ^HSI")
            logger.info("ä½¿ç”¨ yfinance è·å–æ’ç”ŸæŒ‡æ•° (^HSI) æ•°æ®")
            
            try:
                # ä½¿ç”¨datetimeå¯¹è±¡è¿›è¡Œè½¬æ¢
                start_dt = start_date_dt.strftime("%Y-%m-%d")
                end_dt = end_date_dt.strftime("%Y-%m-%d")
                
                # è·å–æ•°æ®
                df = yf.download('^HSI', start=start_dt, end=end_dt)
                
                if isinstance(df, pd.DataFrame) and not df.empty:
                    logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(df)} æ¡æ’ç”ŸæŒ‡æ•°æ•°æ®")
                    logger.info(f"æ•°æ®åˆ—å: {', '.join(df.columns)}")
                    
                    # æ ‡å‡†åŒ–åˆ—å
                    df = df.reset_index()
                    df = df.rename(columns={
                        'Date': 'æ—¥æœŸ',
                        'Open': 'å¼€ç›˜',
                        'High': 'æœ€é«˜',
                        'Low': 'æœ€ä½',
                        'Close': 'æ”¶ç›˜',
                        'Volume': 'æˆäº¤é‡',
                        'Adj Close': 'å¤æƒæ”¶ç›˜'
                    })
                    
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                    
                    # æ’åº
                    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                    
                    # æ£€æŸ¥æ•°æ®é‡
                    if len(df) <= 1:
                        logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
                        return pd.DataFrame()
                    
                    logger.info(f"âœ… è·å–åˆ°æ’ç”ŸæŒ‡æ•°å†å²æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´: {df['æ—¥æœŸ'].min()} è‡³ {df['æ—¥æœŸ'].max()}ï¼Œå…±{len(df)}æ¡è®°å½•")
                    return df
                else:
                    logger.warning("âš ï¸ yfinance è¿”å›ç©ºæ•°æ®")
                    return pd.DataFrame()
            except Exception as e:
                logger.error(f"âŒ yfinance.download æ–¹æ³•è·å–æ’ç”ŸæŒ‡æ•°å†å²æ•°æ®å¤±è´¥: {str(e)}")
                return pd.DataFrame()
        
        # æ ¹æ®æŒ‡æ•°ç±»å‹ä½¿ç”¨ä¸åŒçš„æ•°æ®æ¥å£
        if index_code.startswith('^'):
            # ç¾è‚¡æŒ‡æ•°å¤„ç† - ä½¿ç”¨YFinance
            return fetch_us_index_from_yfinance(index_code, start_date_dt, end_date_dt)
        
        elif index_code.endswith('.CSI'):
            # ä¸­è¯ç³»åˆ—æŒ‡æ•°
            index_name = index_code.replace('.CSI', '')
            # ä¼ é€’datetimeå¯¹è±¡
            return ak.index_zh_a_hist(
                symbol=index_name,
                period="daily",
                start_date=start_date,
                end_date=end_date
            )
        
        elif index_code.endswith('.HI'):
            # æ’ç”Ÿç³»åˆ—æŒ‡æ•° - ä½¿ç”¨ä¸“é—¨çš„å‡½æ•°å¤„ç†
            # ä¼ é€’datetimeå¯¹è±¡
            return fetch_hang_seng_index_data(index_code, start_date_dt, end_date_dt)
        
        else:
            # Aè‚¡æŒ‡æ•°
            # ä¼ é€’datetimeå¯¹è±¡
            return ak.index_zh_a_hist(
                symbol=index_code,
                period="daily",
                start_date=start_date,
                end_date=end_date
            )
    
    except Exception as e:
        logger.error(f"è·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def fetch_hang_seng_index_data(index_code: str, start_date_dt: datetime, end_date_dt: datetime) -> pd.DataFrame:
    """
    ä¸“é—¨å¤„ç†æ’ç”ŸæŒ‡æ•°æ•°æ®è·å–
    ä»…æä¾›è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼Œä¸ä¿å­˜ä»»ä½•æ–‡ä»¶
    
    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚"HSNDXIT.HI"ï¼‰
        start_date_dt: å¼€å§‹æ—¥æœŸï¼ˆdatetimeå¯¹è±¡ï¼‰
        end_date_dt: ç»“æŸæ—¥æœŸï¼ˆdatetimeå¯¹è±¡ï¼‰
        
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    index_name = index_code.replace('.HI', '')
    logger.info(f"è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®: {index_code} ({index_name})")
    
    # 1. å°è¯•ä½¿ç”¨AKShareçš„stock_hk_index_daily_emè·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ®
    try:
        logger.info("å°è¯•ä½¿ç”¨ ak.stock_hk_index_daily_em è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ®")
        
        # æ’ç”Ÿç§‘æŠ€æŒ‡æ•°çš„æ­£ç¡®ä»£ç æ˜¯ "HSNDXIT" è€Œä¸æ˜¯ "HSNDXIT.HI"
        symbol = index_code.replace('.HI', '')
        logger.info(f"ä½¿ç”¨æŒ‡æ•°ä»£ç : {symbol} (å·²ç§»é™¤'.HI'åç¼€)")
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        start_date = start_date_dt.strftime("%Y%m%d")
        end_date = end_date_dt.strftime("%Y%m%d")
        
        # è·å–æ•°æ®
        df = ak.stock_hk_index_daily_em(symbol=symbol, period="daily", 
                                      start_date=start_date, end_date=end_date)
        
        if not df.empty:
            logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(df)} æ¡æ•°æ®")
            
            # æ£€æŸ¥æ•°æ®åˆ—
            logger.info(f"æ•°æ®åˆ—å: {', '.join(df.columns)}")
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
            if 'æ—¥æœŸ' in df.columns:
                try:
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                except Exception as e:
                    logger.error(f"æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
                    return pd.DataFrame()
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            if 'æ—¥æœŸ' in df.columns:
                first_date = df['æ—¥æœŸ'].min()
                last_date = df['æ—¥æœŸ'].max()
                logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {first_date} è‡³ {last_date}")
            
            # æ£€æŸ¥æ•°æ®é‡
            if len(df) <= 1:
                logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
                return pd.DataFrame()
            
            return df
        else:
            logger.warning("âš ï¸ ak.stock_hk_index_daily_em è¿”å›ç©ºæ•°æ®")
            logger.warning("å¯èƒ½åŸå› :")
            logger.warning("  - æŒ‡æ•°ä»£ç ä¸æ­£ç¡®ï¼ˆåº”ä¸º 'HSNDXIT' è€Œä¸æ˜¯ 'HSNDXIT.HI'ï¼‰")
            logger.warning("  - æ•°æ®æºæ— å†å²æ•°æ®")
            logger.warning("  - ç½‘ç»œè¿æ¥é—®é¢˜")
            logger.warning("  - æ—¥æœŸèŒƒå›´ä¸æ­£ç¡®")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"âŒ ak.stock_hk_index_daily_em æ–¹æ³•è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ®å¤±è´¥: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        logger.error(str(e))
        
        # æ£€æŸ¥akshareç‰ˆæœ¬
        import akshare as ak
        logger.error(f"å½“å‰akshareç‰ˆæœ¬: {ak.__version__}")
        logger.error("å¯èƒ½è§£å†³æ–¹æ¡ˆ:")
        logger.error("  - ç¡®è®¤æŒ‡æ•°ä»£ç ä¸º 'HSNDXIT'ï¼ˆæ— .HIåç¼€ï¼‰")
        logger.error("  - æ›´æ–°akshare: pip install --upgrade akshare")
        logger.error("  - æ£€æŸ¥akshareæ–‡æ¡£: https://www.akshare.xyz/  ")
        logger.error("  - æŸ¥çœ‹akshare issue: https://github.com/akshare/akshare/issues  ")
        
        return pd.DataFrame()
    
    # 2. å¦‚æœç¬¬ä¸€æ­¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨AKShareçš„stock_hk_index_spot_emè·å–å®æ—¶æ•°æ®
    try:
        logger.info("å°è¯•ä½¿ç”¨ ak.stock_hk_index_spot_em è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å®æ—¶æ•°æ®")
        
        # è·å–æ•°æ®
        df = ak.stock_hk_index_spot_em()
        
        if not df.empty:
            # è¿‡æ»¤æŒ‡å®šçš„æŒ‡æ•°
            filtered = df[df["ä»£ç "] == index_code.replace('.HI', '')]
            
            if not filtered.empty:
                logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(filtered)} æ¡å®æ—¶æ•°æ®")
                logger.info(f"æ•°æ®åˆ—å: {', '.join(filtered.columns)}")
                
                # ä»…ç”¨äºè¯Šæ–­ï¼Œä¸è¿”å›å®æ—¶æ•°æ®
                logger.info("âš ï¸ æ³¨æ„: å®æ—¶æ•°æ®æ— æ³•ç”¨äºå†å²åˆ†æ")
                logger.info(f"å®æ—¶æ•°æ®: {filtered.iloc[0].to_dict()}")
                return pd.DataFrame()
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šæŒ‡æ•°çš„å®æ—¶æ•°æ®")
        else:
            logger.warning("âš ï¸ ak.stock_hk_index_spot_em è¿”å›ç©ºæ•°æ®")
    except Exception as e:
        logger.error(f"âŒ ak.stock_hk_index_spot_em æ–¹æ³•è·å–æ•°æ®å¤±è´¥: {str(e)}")
    
    # 3. å°è¯•ä½¿ç”¨yfinanceè·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°
    try:
        import yfinance as yf
        # ä½¿ç”¨datetimeå¯¹è±¡
        start_dt = start_date_dt.strftime("%Y-%m-%d")
        end_dt = end_date_dt.strftime("%Y-%m-%d")
        
        logger.info(f"å°è¯•ä½¿ç”¨ yfinance.download è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ® (HSTECH.HK)")
        df = yf.download('HSTECH.HK', start=start_dt, end=end_dt)
        
        if isinstance(df, pd.DataFrame) and not df.empty:
            logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(df)} æ¡æ•°æ®")
            logger.info(f"æ•°æ®åˆ—å: {', '.join(df.columns)}")
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            if 'Date' in df.columns:
                first_date = df.index.min().strftime("%Y-%m-%d")
                last_date = df.index.max().strftime("%Y-%m-%d")
                logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {first_date} è‡³ {last_date}")
            
            # æ£€æŸ¥æ•°æ®é‡
            if len(df) <= 1:
                logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
                return pd.DataFrame()
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.reset_index()
            df = df.rename(columns={
                'Date': 'æ—¥æœŸ',
                'Open': 'å¼€ç›˜',
                'High': 'æœ€é«˜',
                'Low': 'æœ€ä½',
                'Close': 'æ”¶ç›˜',
                'Volume': 'æˆäº¤é‡',
                'Adj Close': 'å¤æƒæ”¶ç›˜'
            })
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            
            return df
        else:
            logger.warning("âš ï¸ yfinance è¿”å›ç©ºæ•°æ®æˆ–éDataFrame")
            logger.warning("å¯èƒ½åŸå› :")
            logger.warning("  - æŒ‡æ•°ä»£ç ä¸æ­£ç¡®ï¼ˆåº”ä¸º 'HSTECH.HK'ï¼‰")
            logger.warning("  - æ•°æ®æºæ— å†å²æ•°æ®")
            logger.warning("  - ç½‘ç»œè¿æ¥é—®é¢˜")
            logger.warning("  - æ—¥æœŸèŒƒå›´ä¸æ­£ç¡®")
    except Exception as e:
        logger.error(f"âŒ yfinance.download æ–¹æ³•è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ®å¤±è´¥: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        logger.error(str(e))
    
    # æ²¡æœ‰è·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®
    logger.error(f"âŒ æ— æ³•è·å–æ’ç”Ÿç§‘æŠ€æŒ‡æ•°å†å²æ•°æ®: {index_code}")
    logger.error("âŒ å¯èƒ½åŸå› ï¼š")
    logger.error("  1. æŒ‡æ•°ä»£ç ä¸æ­£ç¡®ï¼š")
    logger.error("     - åº”ä½¿ç”¨ 'HSNDXIT' è€Œä¸æ˜¯ 'HSNDXIT.HI'")
    logger.error("     - æ’ç”Ÿç§‘æŠ€æŒ‡æ•°ä»£ç åº”ä¸º 'HSNDXIT'")
    logger.error("  2. æ—¥æœŸèŒƒå›´é—®é¢˜ï¼š")
    logger.error("     - å¼€å§‹æ—¥æœŸ: " + start_date_dt.strftime("%Y%m%d"))
    logger.error("     - ç»“æŸæ—¥æœŸ: " + end_date_dt.strftime("%Y%m%d"))
    logger.error("  3. AKShareæ¥å£é—®é¢˜ï¼š")
    logger.error("     - ç¡®è®¤akshareç‰ˆæœ¬: " + ak.__version__)
    logger.error("     - æŸ¥çœ‹æ–‡æ¡£: https://www.akshare.xyz/  ")
    logger.error("     - æ£€æŸ¥issue: https://github.com/akshare/akshare/issues  ")
    
    return pd.DataFrame()

def fetch_us_index_from_yfinance(index_code: str, start_date_dt: datetime, end_date_dt: datetime) -> pd.DataFrame:
    """
    ä½¿ç”¨YFinanceè·å–ç¾è‚¡æŒ‡æ•°æ•°æ®
    
    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚"^NDX"ï¼‰
        start_date_dt: å¼€å§‹æ—¥æœŸï¼ˆdatetimeå¯¹è±¡ï¼‰
        end_date_dt: ç»“æŸæ—¥æœŸï¼ˆdatetimeå¯¹è±¡ï¼‰
        
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼ - ä»…åœ¨éœ€è¦æ—¶è½¬æ¢
        start_dt = start_date_dt.strftime("%Y-%m-%d")
        end_dt = end_date_dt.strftime("%Y-%m-%d")
        
        # æŒ‡æ•°ä»£ç æ˜ å°„
        symbol_map = {
            '^NDX': '^NDX',  # çº³æ–¯è¾¾å…‹100
            '^DJI': '^DJI',  # é“ç¼æ–¯å·¥ä¸šæŒ‡æ•°
            '^GSPC': '^GSPC', # æ ‡å‡†æ™®å°”500
            '^HSTECH': '^HSTECH' # æ’ç”Ÿç§‘æŠ€æŒ‡æ•°
        }
        
        symbol = symbol_map.get(index_code, index_code)
        
        # è·å–æ•°æ®
        df = yf.download(symbol, start=start_dt, end=end_dt)
        
        if df.empty:
            logger.warning(f"é€šè¿‡yfinanceè·å–{index_code}æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # æ ‡å‡†åŒ–åˆ—å
        df = df.reset_index()
        df = df.rename(columns={
            'Date': 'æ—¥æœŸ',
            'Open': 'å¼€ç›˜',
            'High': 'æœ€é«˜',
            'Low': 'æœ€ä½',
            'Close': 'æ”¶ç›˜',
            'Volume': 'æˆäº¤é‡',
            'Adj Close': 'å¤æƒæ”¶ç›˜'
        })
        
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        
        logger.info(f"æˆåŠŸé€šè¿‡yfinanceè·å–{index_code}æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
        return df
    
    except Exception as e:
        logger.error(f"é€šè¿‡yfinanceè·å–{index_code}å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def calculate_critical_value(df: pd.DataFrame) -> float:
    """è®¡ç®—ä¸´ç•Œå€¼ï¼ˆ20æ—¥å‡çº¿ï¼‰"""
    if len(df) < CRITICAL_VALUE_DAYS:
        logger.warning(f"æ•°æ®ä¸è¶³{CRITICAL_VALUE_DAYS}å¤©ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—ä¸´ç•Œå€¼")
        return df["æ”¶ç›˜"].mean() if not df.empty else 0.0
    
    return df['æ”¶ç›˜'].rolling(window=CRITICAL_VALUE_DAYS).mean().iloc[-1]

def calculate_deviation(current: float, critical: float) -> float:
    """è®¡ç®—åç¦»ç‡"""
    return (current - critical) / critical * 100

def calculate_consecutive_days_above(df: pd.DataFrame, critical_value: float) -> int:
    """è®¡ç®—è¿ç»­ç«™ä¸Šå‡çº¿çš„å¤©æ•°"""
    if len(df) < 2:
        return 0
    
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS).mean().values
    
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥
    consecutive_days = 0
    for i in range(len(close_prices)-1, -1, -1):
        if i < CRITICAL_VALUE_DAYS - 1:
            break
            
        if close_prices[i] >= ma_values[i]:
            consecutive_days += 1
        else:
            break
    
    return consecutive_days

def calculate_consecutive_days_below(df: pd.DataFrame, critical_value: float) -> int:
    """è®¡ç®—è¿ç»­è·Œç ´å‡çº¿çš„å¤©æ•°"""
    if len(df) < 2:
        return 0
    
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS).mean().values
    
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥
    consecutive_days = 0
    for i in range(len(close_prices)-1, -1, -1):
        if i < CRITICAL_VALUE_DAYS - 1:
            break
            
        if close_prices[i] < ma_values[i]:
            consecutive_days += 1
        else:
            break
    
    return consecutive_days

def calculate_volume_change(df: pd.DataFrame) -> float:
    """
    è®¡ç®—æˆäº¤é‡å˜åŒ–ç‡
    
    Args:
        df: ETFæ—¥çº¿æ•°æ®
    
    Returns:
        float: æˆäº¤é‡å˜åŒ–ç‡ï¼ˆå½“å‰æˆäº¤é‡ç›¸æ¯”å‰ä¸€æ—¥çš„å˜åŒ–ç™¾åˆ†æ¯”ï¼‰
    """
    try:
        if len(df) < 2:
            logger.warning("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æˆäº¤é‡å˜åŒ–")
            return 0.0
        
        # è·å–æœ€æ–°ä¸¤ä¸ªäº¤æ˜“æ—¥çš„æˆäº¤é‡
        current_volume = df['æˆäº¤é‡'].values[-1]
        previous_volume = df['æˆäº¤é‡'].values[-2]
        
        # ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
        if not isinstance(current_volume, (int, float)) or not isinstance(previous_volume, (int, float)):
            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            try:
                current_volume = float(current_volume)
                previous_volume = float(previous_volume)
            except:
                logger.warning("æˆäº¤é‡æ•°æ®æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
                return 0.0
        
        # è®¡ç®—å˜åŒ–ç‡
        if previous_volume > 0:
            volume_change = (current_volume - previous_volume) / previous_volume
            return volume_change
        else:
            return 0.0
    
    except Exception as e:
        logger.error(f"è®¡ç®—æˆäº¤é‡å˜åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def calculate_loss_percentage(df: pd.DataFrame) -> float:
    """è®¡ç®—å½“å‰äºæŸæ¯”ä¾‹ï¼ˆç›¸å¯¹äºæœ€è¿‘ä¸€æ¬¡ä¹°å…¥ç‚¹ï¼‰"""
    if len(df) < 2:
        return 0.0
    
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS).mean().values
    
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥ï¼Œæ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ç«™ä¸Šå‡çº¿çš„ç‚¹
    buy_index = -1
    for i in range(len(close_prices)-1, -1, -1):
        if i < CRITICAL_VALUE_DAYS - 1:
            continue
            
        if close_prices[i] >= ma_values[i]:
            buy_index = i
            break
    
    # å¦‚æœæ‰¾ä¸åˆ°ä¹°å…¥ç‚¹ï¼Œä½¿ç”¨30å¤©å‰ä½œä¸ºå‚è€ƒ
    if buy_index == -1:
        buy_index = max(0, len(close_prices) - 30)
    
    current_price = close_prices[-1]
    buy_price = close_prices[buy_index]
    
    loss_percentage = (current_price - buy_price) / buy_price * 100
    return loss_percentage

def is_in_volatile_market(df: pd.DataFrame) -> tuple:
    """åˆ¤æ–­æ˜¯å¦å¤„äºéœ‡è¡å¸‚
    
    Returns:
        tuple: (æ˜¯å¦éœ‡è¡å¸‚, ç©¿è¶Šæ¬¡æ•°, æœ€è¿‘10å¤©åç¦»ç‡èŒƒå›´)
    """
    if len(df) < 10:
        return False, 0, (0, 0)
    
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS).mean().values
    
    # æ£€æŸ¥æ˜¯å¦è¿ç»­10å¤©åœ¨å‡çº¿é™„è¿‘æ³¢åŠ¨ï¼ˆ-5%~+5%ï¼‰
    last_10_days = df.tail(10)
    deviations = []
    for i in range(len(last_10_days)):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1 or np.isnan(ma_values[-10 + i]):
            continue
            
        deviation = (close_prices[-10 + i] - ma_values[-10 + i]) / ma_values[-10 + i] * 100
        if abs(deviation) > 5.0:
            return False, 0, (0, 0)
        deviations.append(deviation)
    
    # æ£€æŸ¥ä»·æ ¼æ˜¯å¦åå¤ç©¿è¶Šå‡çº¿
    cross_count = 0
    for i in range(len(close_prices)-10, len(close_prices)-1):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1 or np.isnan(ma_values[i]) or np.isnan(ma_values[i+1]):
            continue
            
        if (close_prices[i] >= ma_values[i] and close_prices[i+1] < ma_values[i+1]) or \
           (close_prices[i] < ma_values[i] and close_prices[i+1] >= ma_values[i+1]):
            cross_count += 1
    
    # è‡³å°‘éœ€è¦5æ¬¡ç©¿è¶Šæ‰è®¤å®šä¸ºéœ‡è¡å¸‚
    min_cross_count = 5
    is_volatile = cross_count >= min_cross_count
    
    # è®¡ç®—æœ€è¿‘10å¤©åç¦»ç‡èŒƒå›´
    if deviations:
        min_deviation = min(deviations)
        max_deviation = max(deviations)
    else:
        # å½“æ²¡æœ‰æœ‰æ•ˆæ•°æ®æ—¶ï¼Œä½¿ç”¨0ä½œä¸ºé»˜è®¤å€¼
        min_deviation = 0
        max_deviation = 0
    
    return is_volatile, cross_count, (min_deviation, max_deviation)

def detect_head_and_shoulders(df: pd.DataFrame) -> dict:
    """æ£€æµ‹Må¤´å’Œå¤´è‚©é¡¶å½¢æ€
    
    Returns:
        dict: å½¢æ€æ£€æµ‹ç»“æœ
    """
    if len(df) < 20:  # éœ€è¦è¶³å¤Ÿæ•°æ®
        return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": []}
    
    # è·å–æ”¶ç›˜ä»·
    close_prices = df["æ”¶ç›˜"].values
    
    # å¯»æ‰¾å±€éƒ¨é«˜ç‚¹
    peaks = []
    for i in range(5, len(close_prices)-5):
        if close_prices[i] > max(close_prices[i-5:i]) and close_prices[i] > max(close_prices[i+1:i+6]):
            peaks.append((i, close_prices[i]))
    
    # å¦‚æœæ‰¾åˆ°çš„é«˜ç‚¹å°‘äº3ä¸ªï¼Œæ— æ³•å½¢æˆå¤´è‚©é¡¶
    if len(peaks) < 3:
        return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
    
    # æ£€æµ‹Må¤´ï¼ˆä¸¤ä¸ªé«˜ç‚¹ï¼‰
    m_top_detected = False
    m_top_confidence = 0.0
    if len(peaks) >= 2:
        # ä¸¤ä¸ªé«˜ç‚¹ï¼Œç¬¬äºŒä¸ªç•¥ä½äºç¬¬ä¸€ä¸ªï¼Œä¸­é—´æœ‰æ˜æ˜¾ä½ç‚¹
        peak1_idx, peak1_price = peaks[-2]
        peak2_idx, peak2_price = peaks[-1]
        
        # æ£€æŸ¥ç¬¬äºŒä¸ªé«˜ç‚¹æ˜¯å¦ä½äºç¬¬ä¸€ä¸ª
        if peak2_price < peak1_price and peak2_price > peak1_price * 0.95:
            # æ£€æŸ¥ä¸­é—´æ˜¯å¦æœ‰æ˜æ˜¾ä½ç‚¹
            trough_idx = peak1_idx + np.argmin(close_prices[peak1_idx:peak2_idx])
            trough_price = close_prices[trough_idx]
            
            # æ£€æŸ¥ä½ç‚¹æ˜¯å¦æ˜æ˜¾
            if trough_price < peak1_price * 0.97 and trough_price < peak2_price * 0.97:
                m_top_detected = True
                # è®¡ç®—ç½®ä¿¡åº¦
                price_diff = (peak1_price - peak2_price) / peak1_price
                trough_depth = (peak1_price - trough_price) / peak1_price
                m_top_confidence = 0.5 + 0.5 * min(price_diff / 0.05, 1) + 0.5 * min(trough_depth / 0.05, 1)
                m_top_confidence = min(m_top_confidence, 1.0)
    
    # æ£€æµ‹å¤´è‚©é¡¶ï¼ˆä¸‰ä¸ªé«˜ç‚¹ï¼‰
    head_and_shoulders_detected = False
    head_and_shoulders_confidence = 0.0
    
    if len(peaks) >= 3:
        # ä¸‰ä¸ªé«˜ç‚¹ï¼Œä¸­é—´æœ€é«˜ï¼Œä¸¤ä¾§è¾ƒä½
        shoulder1_idx, shoulder1_price = peaks[-3]
        head_idx, head_price = peaks[-2]
        shoulder2_idx, shoulder2_price = peaks[-1]
        
        # æ£€æŸ¥ä¸­é—´æ˜¯å¦ä¸ºæœ€é«˜ç‚¹
        if head_price > shoulder1_price and head_price > shoulder2_price:
            # æ£€æŸ¥ä¸¤ä¾§è‚©è†€æ˜¯å¦å¤§è‡´å¯¹ç§°
            shoulder_similarity = min(shoulder1_price, shoulder2_price) / max(shoulder1_price, shoulder2_price)
            
            # æ£€æŸ¥ä¸­é—´ä½ç‚¹
            trough1_idx = shoulder1_idx + np.argmin(close_prices[shoulder1_idx:head_idx])
            trough2_idx = head_idx + np.argmin(close_prices[head_idx:shoulder2_idx])
            neckline_price = (close_prices[trough1_idx] + close_prices[trough2_idx]) / 2
            
            # æ£€æŸ¥å¤´è‚©æ¯”ä¾‹æ˜¯å¦åˆç†
            if shoulder_similarity > 0.85 and head_price > neckline_price * 1.1:
                head_and_shoulders_detected = True
                # è®¡ç®—ç½®ä¿¡åº¦
                shoulder_diff = 1 - shoulder_similarity
                head_height = (head_price - neckline_price) / neckline_price
                head_and_shoulders_confidence = 0.5 + 0.3 * min(shoulder_diff / 0.15, 1) + 0.2 * min(head_height / 0.15, 1)
                head_and_shoulders_confidence = min(head_and_shoulders_confidence, 1.0)
    
    # ç¡®å®šä¸»è¦æ£€æµ‹ç»“æœ
    if head_and_shoulders_detected and head_and_shoulders_confidence > m_top_confidence:
        return {
            "pattern_type": "å¤´è‚©é¡¶",
            "detected": True,
            "confidence": head_and_shoulders_confidence,
            "peaks": peaks[-3:]
        }
    elif m_top_detected:
        return {
            "pattern_type": "Må¤´",
            "detected": True,
            "confidence": m_top_confidence,
            "peaks": peaks[-2:]
        }
    else:
        return {
            "pattern_type": "æ— ",
            "detected": False,
            "confidence": 0,
            "peaks": peaks[-3:] if len(peaks) >= 3 else peaks
        }

def generate_signal_message(index_info: dict, df: pd.DataFrame, current: float, critical: float, deviation: float) -> str:
    """ç”Ÿæˆç­–ç•¥ä¿¡å·æ¶ˆæ¯"""
    # è®¡ç®—è¿ç»­ç«™ä¸Š/è·Œç ´å‡çº¿çš„å¤©æ•°
    consecutive_above = calculate_consecutive_days_above(df, critical)
    consecutive_below = calculate_consecutive_days_below(df, critical)
    
    # è®¡ç®—æˆäº¤é‡å˜åŒ–
    volume_change = calculate_volume_change(df)
    
    # æ£€æµ‹Må¤´/å¤´è‚©é¡¶å½¢æ€
    pattern_detection = detect_head_and_shoulders(df)
    
    # 3. éœ‡è¡å¸‚åˆ¤æ–­ - ä¼˜å…ˆçº§æœ€é«˜
    is_volatile, cross_count, (min_dev, max_dev) = is_in_volatile_market(df)
    if is_volatile:
        # è®¡ç®—ä¸Šè½¨å’Œä¸‹è½¨ä»·æ ¼
        upper_band = critical * (1 + max_dev/100)
        lower_band = critical * (1 + min_dev/100)
        
        message = (
            f"ã€éœ‡è¡å¸‚ã€‘è¿ç»­10æ—¥ä»·æ ¼åå¤ç©¿å‡çº¿ï¼ˆç©¿è¶Š{cross_count}æ¬¡ï¼‰ï¼Œåç¦»ç‡èŒƒå›´[{min_dev:.2f}%~{max_dev:.2f}%]\n"
            f"âœ… æ“ä½œå»ºè®®ï¼š\n"
            f"  â€¢ ä¸Šæ²¿æ“ä½œï¼ˆä»·æ ¼â‰ˆ{upper_band:.2f}ï¼‰ï¼šå°å¹…å‡ä»“10%-20%ï¼ˆå¦‚{index_info['etf_code']}ï¼‰\n"
            f"  â€¢ ä¸‹æ²¿æ“ä½œï¼ˆä»·æ ¼â‰ˆ{lower_band:.2f}ï¼‰ï¼šå°å¹…åŠ ä»“10%-20%ï¼ˆå¦‚{index_info['etf_code']}ï¼‰\n"
            f"  â€¢ æ€»ä»“ä½ä¸¥æ ¼æ§åˆ¶åœ¨â‰¤50%\n"
            f"âš ï¸ é¿å…é¢‘ç¹äº¤æ˜“ï¼Œç­‰å¾…è¶‹åŠ¿æ˜æœ—\n"
        )
        return message
    
    # 1. YESä¿¡å·ï¼šå½“å‰ä»·æ ¼ â‰¥ 20æ—¥å‡çº¿
    if current >= critical:
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡çªç ´ï¼ˆä»·æ ¼åˆšç«™ä¸Šå‡çº¿ï¼Œè¿ç»­2-3æ—¥ç«™ç¨³+æˆäº¤é‡æ”¾å¤§20%+ï¼‰
        if consecutive_above == 1 and volume_change > 0.2:
            message = (
                f"ã€é¦–æ¬¡çªç ´ã€‘è¿ç»­{consecutive_above}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%\n"
                f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰ç«‹å³å»ºä»“30%\n"
                f"  â€¢ å«æ˜Ÿè¡Œä¸šETFç«‹å³å»ºä»“20%\n"
                f"  â€¢ å›è°ƒè‡³5æ—¥å‡çº¿ï¼ˆçº¦{current * 0.99:.2f}ï¼‰å¯åŠ ä»“20%\n"
                f"âš ï¸ æ­¢æŸï¼šä¹°å…¥ä»·ä¸‹æ–¹5%ï¼ˆå®½åŸºETFï¼‰æˆ–3%ï¼ˆé«˜æ³¢åŠ¨ETFï¼‰\n"
            )
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡çªç ´ï¼ˆä»·æ ¼åˆšç«™ä¸Šå‡çº¿ï¼Œè¿ç»­2-3æ—¥ç«™ç¨³+æˆäº¤é‡æ”¾å¤§20%+ï¼‰
        elif 2 <= consecutive_above <= 3 and volume_change > 0.2:
            message = (
                f"ã€é¦–æ¬¡çªç ´ç¡®è®¤ã€‘è¿ç»­{consecutive_above}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%\n"
                f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰å¯åŠ ä»“è‡³50%\n"
                f"  â€¢ å«æ˜Ÿè¡Œä¸šETFå¯åŠ ä»“è‡³35%\n"
                f"  â€¢ ä¸¥æ ¼è·Ÿè¸ª5æ—¥å‡çº¿ä½œä¸ºæ­¢æŸä½ï¼ˆçº¦{current * 0.99:.2f}ï¼‰\n"
                f"âš ï¸ æ³¨æ„ï¼šè‹¥æ”¶ç›˜è·Œç ´5æ—¥å‡çº¿ï¼Œç«‹å³å‡ä»“50%\n"
            )
        # å­æ¡ä»¶2ï¼šæŒç»­ç«™ç¨³ï¼ˆä»·æ ¼ç»´æŒåœ¨å‡çº¿ä¸Šï¼‰
        else:
            # åœºæ™¯Aï¼šåç¦»ç‡â‰¤+5%ï¼ˆè¶‹åŠ¿ç¨³å¥ï¼‰
            if deviation <= 5.0:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“10%-15%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“5%-10%"
                
                message = (
                    f"ã€è¶‹åŠ¿ç¨³å¥ã€‘è¿ç»­{consecutive_above}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ æŒä»“ä¸åŠ¨ï¼Œä¸æ–°å¢ä»“ä½\n"
                    f"  â€¢ è·Ÿè¸ªæ­¢æŸä¸Šç§»è‡³5æ—¥å‡çº¿ï¼ˆçº¦{current * 0.99:.2f}ï¼‰\n"
                    f"  â€¢ è‹¥æ”¶ç›˜è·Œç ´5æ—¥å‡çº¿ï¼Œå‡ä»“50%\n"
                    f"{pattern_msg}\n"
                )
            # åœºæ™¯Bï¼š+5%ï¼œåç¦»ç‡â‰¤+10%ï¼ˆè¶‹åŠ¿è¾ƒå¼ºï¼‰
            elif 5.0 < deviation <= 10.0:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œç«‹å³å‡ä»“10%-15%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“5%-10%"
                
                message = (
                    f"ã€è¶‹åŠ¿è¾ƒå¼ºã€‘è¿ç»­{consecutive_above}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ è§‚æœ›ï¼Œä¸æ–°å¢ä»“ä½\n"
                    f"  â€¢ é€¢é«˜å‡ä»“10%-15%ï¼ˆ{index_info['etf_code']}ï¼‰\n"
                    f"  â€¢ è‹¥æ”¶ç›˜è·Œç ´10æ—¥å‡çº¿ï¼Œå‡ä»“30%\n"
                    f"{pattern_msg}\n"
                )
            # åœºæ™¯Cï¼šåç¦»ç‡ï¼+10%ï¼ˆè¶…ä¹°é£é™©ï¼‰
            else:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œç«‹å³å‡ä»“20%-30%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“15%-25%"
                
                message = (
                    f"ã€è¶…ä¹°é£é™©ã€‘è¿ç»­{consecutive_above}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ é€¢é«˜å‡ä»“20%-30%ï¼ˆä»…å«æ˜ŸETFï¼‰\n"
                    f"  â€¢ å½“å‰ä»·æ ¼å·²å¤„é«˜ä½ï¼Œé¿å…æ–°å¢ä»“ä½\n"
                    f"  â€¢ ç­‰å¾…åç¦»ç‡å›è½è‡³â‰¤+5%ï¼ˆçº¦{critical * 1.05:.2f}ï¼‰æ—¶åŠ å›\n"
                    f"{pattern_msg}\n"
                )
    
    # 2. NOä¿¡å·ï¼šå½“å‰ä»·æ ¼ ï¼œ 20æ—¥å‡çº¿
    else:
        # è®¡ç®—äºæŸæ¯”ä¾‹
        loss_percentage = calculate_loss_percentage(df)
        
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡è·Œç ´ï¼ˆä»·æ ¼åˆšè·Œç©¿å‡çº¿ï¼Œè¿ç»­1-2æ—¥æœªæ”¶å›+æˆäº¤é‡æ”¾å¤§ï¼‰
        if consecutive_below == 1 and volume_change > 0.2:
            if loss_percentage > -15.0:  # äºæŸ<15%
                message = (
                    f"ã€é¦–æ¬¡è·Œç ´ã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰ç«‹å³å‡ä»“50%\n"
                    f"  â€¢ å«æ˜Ÿè¡Œä¸šETFç«‹å³å‡ä»“70%-80%\n"
                    f"  â€¢ æ­¢æŸä½ï¼š20æ—¥å‡çº¿ä¸Šæ–¹5%ï¼ˆçº¦{critical * 1.05:.2f}ï¼‰\n"
                    f"âš ï¸ è‹¥æ”¶ç›˜æœªæ”¶å›å‡çº¿ï¼Œæ˜æ—¥ç»§ç»­å‡ä»“è‡³20%\n"
                )
            else:  # äºæŸâ‰¥15%
                message = (
                    f"ã€é¦–æ¬¡è·Œç ´-ä¸¥é‡äºæŸã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%ï¼ŒäºæŸ{loss_percentage:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰ç«‹å³æ¸…ä»“\n"
                    f"  â€¢ å«æ˜Ÿè¡Œä¸šETFä¿ç•™20%-30%åº•ä»“è§‚å¯Ÿ\n"
                    f"  â€¢ ä¸¥æ ¼æ­¢æŸï¼šæ”¶ç›˜ä»·ç«™ä¸Š20æ—¥å‡çº¿æ‰è€ƒè™‘å›è¡¥\n"
                    f"âš ï¸ é‡å¤§äºæŸä¿¡å·ï¼Œé¿å…ç›²ç›®æŠ„åº•\n"
                )
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡è·Œç ´ï¼ˆä»·æ ¼åˆšè·Œç©¿å‡çº¿ï¼Œè¿ç»­1-2æ—¥æœªæ”¶å›+æˆäº¤é‡æ”¾å¤§ï¼‰
        elif consecutive_below == 2 and volume_change > 0.2:
            message = (
                f"ã€é¦–æ¬¡è·Œç ´ç¡®è®¤ã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%\n"
                f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰ä¸¥æ ¼æ­¢æŸæ¸…ä»“\n"
                f"  â€¢ å«æ˜Ÿè¡Œä¸šETFä»…ä¿ç•™20%-30%åº•ä»“\n"
                f"  â€¢ ä¸¥æ ¼æ­¢æŸï¼š20æ—¥å‡çº¿ä¸‹æ–¹5%ï¼ˆçº¦{critical * 0.95:.2f}ï¼‰\n"
                f"âš ï¸ ä¿¡å·ç¡®è®¤ï¼Œé¿å…ä¾¥å¹¸å¿ƒç†\n"
            )
        # å­æ¡ä»¶2ï¼šæŒç»­è·Œç ´ï¼ˆä»·æ ¼ç»´æŒåœ¨å‡çº¿ä¸‹ï¼‰
        else:
            # åœºæ™¯Aï¼šåç¦»ç‡â‰¥-5%ï¼ˆä¸‹è·ŒåˆæœŸï¼‰
            if deviation >= -5.0:
                message = (
                    f"ã€ä¸‹è·ŒåˆæœŸã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ è½»ä»“è§‚æœ›ï¼ˆä»“ä½â‰¤20%ï¼‰\n"
                    f"  â€¢ åå¼¹è‡³å‡çº¿é™„è¿‘ï¼ˆçº¦{critical:.2f}ï¼‰å‡ä»“å‰©ä½™ä»“ä½\n"
                    f"  â€¢ æš‚ä¸è€ƒè™‘æ–°å¢ä»“ä½\n"
                    f"âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šæ”¶ç›˜ç«™ä¸Š5æ—¥å‡çº¿ï¼Œå¯è½»ä»“è¯•å¤š\n"
                )
            # åœºæ™¯Bï¼š-10%â‰¤åç¦»ç‡ï¼œ-5%ï¼ˆä¸‹è·Œä¸­æœŸï¼‰
            elif -10.0 <= deviation < -5.0:
                message = (
                    f"ã€ä¸‹è·Œä¸­æœŸã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ ç©ºä»“ä¸ºä¸»ï¼Œé¿å…æŠ„åº•\n"
                    f"  â€¢ ä»…æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰å¯è¯•ä»“5%-10%\n"
                    f"  â€¢ ä¸¥æ ¼æ­¢æŸï¼šæ”¶ç›˜è·Œç ´å‰ä½å³ç¦»åœº\n"
                    f"âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šè¡Œä¸šåŸºæœ¬é¢æ˜¯å¦æœ‰åˆ©ç©ºï¼Œæœ‰åˆ©ç©ºåˆ™æ¸…ä»“\n"
                )
            # åœºæ™¯Cï¼šåç¦»ç‡ï¼œ-10%ï¼ˆè¶…å–æœºä¼šï¼‰
            else:
                message = (
                    f"ã€è¶…å–æœºä¼šã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œåç¦»ç‡{deviation:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etf_code']}ï¼‰å°å¹…åŠ ä»“10%-15%\n"
                    f"  â€¢ ç›®æ ‡ä»·ï¼šåç¦»ç‡â‰¥-5%ï¼ˆçº¦{critical * 0.95:.2f}ï¼‰\n"
                    f"  â€¢ è¾¾åˆ°ç›®æ ‡å³å–å‡ºåŠ ä»“éƒ¨åˆ†\n"
                    f"âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šè‹¥è·Œç ´å‰ä½ï¼Œç«‹å³æ­¢æŸ\n"
                )
    
    return message

def generate_report():
    """ç”Ÿæˆç­–ç•¥æŠ¥å‘Šå¹¶æ¨é€å¾®ä¿¡"""
    try:
        beijing_time = get_beijing_time()
        
        # ç”¨äºå­˜å‚¨æ‰€æœ‰æŒ‡æ•°çš„ç®€è¦ä¿¡æ¯ï¼Œç”¨äºæ€»ç»“æ¶ˆæ¯
        summary_lines = []
        valid_indices_count = 0
        
        # ç›´æ¥æŒ‰INDICESé¡ºåºéå†
        for idx in INDICES:
            code = idx["code"]
            name = idx["name"]
            
            # ç›´æ¥ä»AkShareè·å–æŒ‡æ•°æ•°æ®ï¼ˆä¸ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼‰
            df = fetch_index_data(code)
            if df.empty:
                logger.warning(f"æ— æ•°æ®: {name}({code})")
                # å³ä½¿æ²¡æœ‰æ•°æ®ï¼Œä¹Ÿå‘é€ä¸€æ¡æ¶ˆæ¯é€šçŸ¥
                message_lines = []
                message_lines.append(f"{name} ã€{code}ï¼›ETFï¼š{idx['etf_code']}ï¼Œ{idx['description']}ã€‘\n")
                message_lines.append(f"ğŸ“Š å½“å‰ï¼šæ•°æ®è·å–å¤±è´¥ | ä¸´ç•Œå€¼ï¼šN/A | åç¦»ç‡ï¼šN/A\n")
                # ä¿®æ­£ï¼šé”™è¯¯ä¿¡å·ç±»å‹æ˜¾ç¤ºé—®é¢˜
                message_lines.append(f"âŒ ä¿¡å·ï¼šæ•°æ®è·å–å¤±è´¥\n")
                message_lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
                message_lines.append("âš ï¸ è·å–æŒ‡æ•°æ•°æ®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æº\n")
                message_lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
                message_lines.append(f"ğŸ“… è®¡ç®—æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M')}\n")
                message_lines.append("ğŸ“Š æ•°æ®æ¥æºï¼šGITï¼šfish-etf\n")
                
                message = "\n".join(message_lines)
                logger.info(f"æ¨é€ {name} ç­–ç•¥ä¿¡å·ï¼ˆæ•°æ®è·å–å¤±è´¥ï¼‰\n")
                send_wechat_message(message)
                time.sleep(1)
                continue
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
            if len(df) < CRITICAL_VALUE_DAYS:
                logger.warning(f"æŒ‡æ•° {name}({code}) æ•°æ®ä¸è¶³{CRITICAL_VALUE_DAYS}å¤©ï¼Œè·³è¿‡è®¡ç®—\n")
                # å‘é€æ•°æ®ä¸è¶³çš„æ¶ˆæ¯
                message_lines = []
                message_lines.append(f"{name} ã€{code}ï¼›ETFï¼š{idx['etf_code']}ï¼Œ{idx['description']}ã€‘\n")
                message_lines.append(f"ğŸ“Š å½“å‰ï¼šæ•°æ®ä¸è¶³ | ä¸´ç•Œå€¼ï¼šN/A | åç¦»ç‡ï¼šN/A\n")
                # ä¿®æ­£ï¼šé”™è¯¯ä¿¡å·ç±»å‹æ˜¾ç¤ºé—®é¢˜
                message_lines.append(f"âš ï¸ ä¿¡å·ï¼šæ•°æ®ä¸è¶³\n")
                message_lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
                message_lines.append(f"âš ï¸ éœ€è¦è‡³å°‘{CRITICAL_VALUE_DAYS}å¤©æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œå½“å‰åªæœ‰{len(df)}å¤©\n")
                message_lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
                message_lines.append(f"ğŸ“… è®¡ç®—æ—¶é—´: {beijing_time.strftime('%Y-%m-%d %H:%M')}\n")
                message_lines.append("ğŸ“Š æ•°æ®æ¥æºï¼šGITï¼šfish-etf\n")
                
                message = "\n".join(message_lines)
                logger.info(f"\næ¨é€ {name} ç­–ç•¥ä¿¡å·ï¼ˆæ•°æ®ä¸è¶³ï¼‰\n")
                send_wechat_message(message)
                time.sleep(2)
                continue
            
            # ä¿®å¤ï¼šç¡®ä¿è·å–æ ‡é‡å€¼è€Œä¸æ˜¯Series
            # ä½¿ç”¨.values[-1]ç¡®ä¿è·å–æ ‡é‡å€¼
            close_price = df['æ”¶ç›˜'].values[-1]
            
            # ä¿®å¤ï¼šç¡®ä¿critical_valueæ˜¯æ ‡é‡å€¼
            critical_value = calculate_critical_value(df)
            # å¦‚æœè¿”å›çš„æ˜¯Seriesï¼Œè·å–æœ€åä¸€ä¸ªå€¼
            if isinstance(critical_value, pd.Series):
                critical_value = critical_value.values[-1]
            # å¦‚æœè¿”å›çš„æ˜¯DataFrameï¼Œè·å–æœ€åä¸€ä¸ªå€¼
            elif isinstance(critical_value, pd.DataFrame):
                critical_value = critical_value.iloc[-1, 0]
            
            # ä¿®å¤ï¼šç¡®ä¿close_priceå’Œcritical_valueéƒ½æ˜¯æ•°å€¼ç±»å‹
            try:
                close_price = float(close_price)
                critical_value = float(critical_value)
            except (TypeError, ValueError) as e:
                logger.error(f"è½¬æ¢ä»·æ ¼å€¼å¤±è´¥: {str(e)}")
                continue
            
            # è®¡ç®—åç¦»ç‡
            deviation = calculate_deviation(close_price, critical_value)
            
            # çŠ¶æ€åˆ¤æ–­ï¼ˆæ”¶ç›˜ä»·åœ¨ä¸´ç•Œå€¼ä¹‹ä¸Šä¸ºYESï¼Œå¦åˆ™ä¸ºNOï¼‰
            # ä¿®å¤ï¼šç°åœ¨close_priceå’Œcritical_valueéƒ½æ˜¯æ ‡é‡å€¼ï¼Œå¯ä»¥å®‰å…¨æ¯”è¾ƒ
            status = "YES" if close_price >= critical_value else "NO"
            
            # ç”Ÿæˆè¯¦ç»†ç­–ç•¥ä¿¡å·
            signal_message = generate_signal_message(idx, df, close_price, critical_value, deviation)
            
            # æ„å»ºæ¶ˆæ¯
            message_lines = []
            message_lines.append(f"{name} ã€{code}ï¼›ETFï¼š{idx['etf_code']}ï¼Œ{idx['description']}ã€‘\n")
            message_lines.append(f"ğŸ“Š å½“å‰ï¼š{close_price:.2f} | ä¸´ç•Œå€¼ï¼š{critical_value:.2f} | åç¦»ç‡ï¼š{deviation:.2f}%\n")
            # ä¿®æ­£ï¼šæ ¹æ®ä¿¡å·ç±»å‹é€‰æ‹©æ­£ç¡®çš„ç¬¦å·
            signal_symbol = "âœ…" if status == "YES" else "âŒ"
            message_lines.append(f"{signal_symbol} ä¿¡å·ï¼š{status}\n")
            message_lines.append(signal_message)            
            message = "\n".join(message_lines)
            
            # å‘é€æ¶ˆæ¯
            logger.info(f"æ¨é€ {name} ç­–ç•¥ä¿¡å·")
            send_wechat_message(message)
            
            # æ·»åŠ åˆ°æ€»ç»“æ¶ˆæ¯
            # ç¡®ä¿åç§°å¯¹é½ - ä½¿ç”¨å›ºå®šå®½åº¦
            name_padding = 10 if len(name) <= 4 else 8  # ä¸­æ–‡åç§°é€šå¸¸2-4ä¸ªå­—
            name_with_padding = f"{name}{' ' * (name_padding - len(name))}"
            
            # ä¿®æ­£ï¼šæ ¹æ®ä¿¡å·ç±»å‹é€‰æ‹©æ­£ç¡®çš„ç¬¦å·
            signal_symbol = "âœ…" if status == "YES" else "âŒ"
            summary_line = f"{name_with_padding}ã€{code}ï¼›ETFï¼š{idx['etf_code']}ã€‘{signal_symbol} ä¿¡å·ï¼š{status} ğŸ“Š å½“å‰ï¼š{close_price:.2f} | ä¸´ç•Œå€¼ï¼š{critical_value:.2f} | åç¦»ç‡ï¼š{deviation:.2f}%\n"
            summary_lines.append(summary_line)
            
            valid_indices_count += 1
            time.sleep(1)
        
        # å¦‚æœæœ‰æœ‰æ•ˆçš„æŒ‡æ•°æ•°æ®ï¼Œå‘é€æ€»ç»“æ¶ˆæ¯
        if valid_indices_count > 0:
            # æ„å»ºæ€»ç»“æ¶ˆæ¯
            summary_message = "\n".join(summary_lines) 
            
            logger.info("æ¨é€æ€»ç»“æ¶ˆæ¯")
            send_wechat_message(summary_message)
            time.sleep(1)
        
        logger.info(f"æ‰€æœ‰æŒ‡æ•°ç­–ç•¥æŠ¥å‘Šå·²æˆåŠŸå‘é€è‡³ä¼ä¸šå¾®ä¿¡ï¼ˆå…±{valid_indices_count}ä¸ªæœ‰æ•ˆæŒ‡æ•°ï¼‰")
    
    except Exception as e:
        logger.error(f"ç­–ç•¥æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # ä¿®æ­£ï¼šé”™è¯¯æ¶ˆæ¯ä¸æ­£å¸¸ä¿¡å·æ¶ˆæ¯åˆ†ç¦»
        send_wechat_message(f"ğŸš¨ ã€é”™è¯¯é€šçŸ¥ã€‘ç­–ç•¥æ‰§è¡Œå¼‚å¸¸: {str(e)}", message_type="error")

if __name__ == "__main__":
    logger.info("===== å¼€å§‹æ‰§è¡Œ æŒ‡æ•°Yes/Noç­–ç•¥ =====")
    
    # æ·»åŠ å»¶æ—¶ï¼Œé¿å…åœ¨æ¯å¤©23:00æ•´ç‚¹æ—¶AkShareæ¥å£å¯èƒ½è¿˜æœªæ›´æ–°å½“æ—¥æ•°æ®
    time.sleep(30)
    
    generate_report()
    logger.info("=== æŒ‡æ•°Yes/Noç­–ç•¥æ‰§è¡Œå®Œæˆ ===")
