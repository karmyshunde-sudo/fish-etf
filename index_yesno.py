#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#æŒ‡æ•° Yes/No ç­–ç•¥æ‰§è¡Œå™¨
#æ¯å¤©è®¡ç®—æŒ‡å®šæŒ‡æ•°çš„ç­–ç•¥ä¿¡å·å¹¶æ¨é€å¾®ä¿¡é€šçŸ¥
# ä½¿ç”¨çš„APIæ¥å£:
# 1. baostock:
#    - bs.login() - ç™»å½•baostock
#    - bs.logout() - é€€å‡ºbaostock
#    - bs.query_history_k_data_plus() - è·å–å†å²Kçº¿æ•°æ®
# 2. yfinance:
#    - yf.download() - ä¸‹è½½å†å²æ•°æ®
# 3. akshare:
#    - ak.index_zh_a_hist() - è·å–Aè‚¡æŒ‡æ•°å†å²è¡Œæƒ…æ•°æ®
#    - ak.stock_hk_index_daily_em() - è·å–ä¸œæ–¹è´¢å¯Œæ¸¯è‚¡æŒ‡æ•°è¡Œæƒ…æ•°æ®
# 4. pandas:
#    - pd.to_datetime() - è½¬æ¢æ—¥æœŸæ ¼å¼
#    - pd.to_numeric() - è½¬æ¢æ•°å€¼ç±»å‹
#    - pd.DataFrame() - åˆ›å»ºæ•°æ®æ¡†
# 5. numpy:
#    - np.isnan() - æ£€æŸ¥NaNå€¼
import os
import logging
import pandas as pd
import akshare as ak
import baostock as bs  # ç”¨äºAè‚¡æŒ‡æ•°æ•°æ®
import time
import numpy as np
import random
import yfinance as yf  # ç”¨äºå›½é™…/æ¸¯è‚¡/ç¾è‚¡æŒ‡æ•°
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import get_beijing_time
from wechat_push.push import send_wechat_message
# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
#logger.setLevel(logging.INFO)
#handler = logging.StreamHandler()
#formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#handler.setFormatter(formatter)
#logger.addHandler(handler)

# =============== æŒ‡æ•°é…ç½®åŒº (å¯åœ¨æ­¤å¤„ä¿®æ”¹æŒ‡æ•°é…ç½®) ===============
# æ ¼å¼: [switch, code, name, description, source, etfs]
# etfsæ ¼å¼: [[code1, name1, description1], [code2, name2, description2], ...]

#======2ä¸ªæŒ‡æ•°ä¸èƒ½çˆ¬å–=====
#INDICES_CONFIG = [
#    [1, "883418", "6ã€å¾®ç›˜è‚¡(SH883418)", "å°å¾®ç›˜è‚¡ç¥¨æŒ‡æ•°", "baostock", [["510530", "åå¤ä¸­è¯500ETF", "å¾®ç›˜è‚¡ETF"]]],
#    [1, "932000", "12ã€ä¸­è¯2000(SH932000)", "ä¸­ç›˜è‚¡æŒ‡æ•°", "baostock", [["561020", "å—æ–¹ä¸­è¯2000ETF", "ä¸­è¯2000ETF"]]]
#]

INDICES_CONFIG = [
    [1, "GC=F", "1ã€ä¼¦æ•¦é‡‘ç°(XAU)", "å›½é™…é»„é‡‘ä»·æ ¼", "yfinance", [["518880", "åå®‰é»„é‡‘ETF", "é»„é‡‘åŸºé‡‘"]]],
    [1, "^HSTECH", "2ã€æ’ç”Ÿç§‘æŠ€æŒ‡æ•°(HSTECH)", "æ¸¯è‚¡ç§‘æŠ€é¾™å¤´ä¼ä¸šæŒ‡æ•°", "baostock", [["513130", "åå¤æ’ç”Ÿç§‘æŠ€ETF", "æ’ç”Ÿç§‘æŠ€ETF"]]],
    [1, "^NDX", "3ã€çº³æ–¯è¾¾å…‹100(NDX)", "ç¾å›½ç§‘æŠ€è‚¡ä»£è¡¨æŒ‡æ•°", "yfinance", [["159892", "åå¤çº³æ–¯è¾¾å…‹100ETF", "çº³æŒ‡ç§‘æŠ€"], ["513100", "å›½æ³°çº³æ–¯è¾¾å…‹100ETF", "çº³æ–¯è¾¾å…‹"]]],
    [1, "sh.000016", "4ã€ä¸Šè¯50(SH000016)", "ä¸Šè¯50è“ç­¹è‚¡æŒ‡æ•°", "baostock", [["510050", "åå¤ä¸Šè¯50ETF", "ä¸Šè¯50ETF"]]],
    [1, "sh.000300", "5ã€æ²ªæ·±300(SH000300)", "Aè‚¡å¤§ç›˜è“ç­¹è‚¡æŒ‡æ•°", "baostock", [["510300", "åæ³°æŸç‘æ²ªæ·±300ETF", "æ²ªæ·±300ETF"]]],
    [1, "sz.399006", "7ã€åˆ›ä¸šæ¿æŒ‡(SZ399006)", "åˆ›ä¸šæ¿é¾™å¤´å…¬å¸", "baostock", [["159915", "æ˜“æ–¹è¾¾åˆ›ä¸šæ¿ETF", "åˆ›ä¸šæ¿ETF"]]],
    [1, "000688", "8ã€ç§‘åˆ›50(SH000688)", "ç§‘åˆ›æ¿é¾™å¤´å…¬å¸", "baostock", [["588000", "åå¤ç§‘åˆ›50ETF", "ç§‘åˆ›50ETF"]]],
    [1, "899050", "9ã€åŒ—è¯50(BJ899050)", "åŒ—äº¤æ‰€é¾™å¤´å…¬å¸", "baostock", [["515200", "åå¤åŒ—è¯50ETF", "åŒ—è¯50ETF"]]],
    [1, "sh.000905", "10ã€ä¸­è¯500(SH000905)", "Aè‚¡ä¸­å°ç›˜è‚¡æŒ‡æ•°", "baostock", [["510500", "å—æ–¹ä¸­è¯500ETF", "ä¸­è¯500ETF"]]],
    [1, "HSCEI.HK", "11ã€æ’ç”Ÿå›½ä¼æŒ‡æ•°(HSCEI)", "æ¸¯è‚¡å›½ä¼æŒ‡æ•°", "baostock", [["510900", "æ˜“æ–¹è¾¾æ’ç”Ÿå›½ä¼ETF", "Hè‚¡ETF"]]],
    [1, "sh.000852", "13ã€ä¸­è¯1000(SH000852)", "ä¸­ç›˜è‚¡æŒ‡æ•°", "baostock", [["512100", "å—æ–¹ä¸­è¯1000ETF", "ä¸­è¯1000ETF"]]],
    [1, "KWEB", "14ã€ä¸­æ¦‚äº’è”æŒ‡æ•°(HXC)", "æµ·å¤–ä¸Šå¸‚ä¸­å›½äº’è”ç½‘å…¬å¸", "yfinance", [["513500", "æ˜“æ–¹è¾¾ä¸­æ¦‚äº’è”ç½‘ETF", "ä¸­æ¦‚äº’è”"]]],
    [1, "^HSI", "15ã€æ’ç”Ÿç»¼åˆæŒ‡æ•°(HSI)", "é¦™æ¸¯è‚¡å¸‚ç»¼åˆè“ç­¹æŒ‡æ•°", "yfinance", [["513400", "åå¤æ’ç”Ÿäº’è”ç½‘ETF", "æ’ç”ŸETF"]]],
    # ä»¥ä¸‹ä¸ºæ–°å¢æŒ‡æ•°ï¼Œä»ç¼–å·20å¼€å§‹ï¼Œæ•°æ®æºä½¿ç”¨baostockæˆ–yfinance
    [1, "sh.000012", "20ã€ä¸Šè¯å›½å€ºæŒ‡æ•°", "ä¸Šæµ·äº¤æ˜“æ‰€å›½å€ºæŒ‡æ•°", "baostock", [["511260", "åå¤ä¸Šè¯å›½å€ºETF", "å›½å€ºETF"]]],
    [1, "sz.399005", "21ã€ä¸­å°æ¿æŒ‡(SZ399005)", "ä¸­å°æ¿ä»£è¡¨æŒ‡æ•°", "baostock", [["159902", "åå¤ä¸­å°æ¿ETF", "ä¸­å°æ¿ETF"]]],
    [1, "sz.399395", "22ã€å›½è¯æœ‰è‰²é‡‘å±æŒ‡æ•°", "æœ‰è‰²é‡‘å±è¡Œä¸šæŒ‡æ•°", "baostock", [["512400", "å—æ–¹ä¸­è¯æœ‰è‰²é‡‘å±ETF", "æœ‰è‰²é‡‘å±ETF"]]],
    [1, "sz.399967", "23ã€ä¸­è¯å†›å·¥æŒ‡æ•°", "å†›å·¥è¡Œä¸šä»£è¡¨æŒ‡æ•°", "baostock", [["512560", "å›½æ³°ä¸­è¯å†›å·¥ETF", "å†›å·¥ETF"]]],
    [1, "sz.399975", "24ã€ä¸­è¯è¯åˆ¸æŒ‡æ•°", "è¯åˆ¸å…¬å¸è¡Œä¸šæŒ‡æ•°", "baostock", [["512000", "åå®ä¸­è¯è¯åˆ¸ETF", "è¯åˆ¸ETF"]]],
    [1, "sh.930713", "25ã€ä¸­è¯AIäº§ä¸šæŒ‡æ•°", "äººå·¥æ™ºèƒ½äº§ä¸šæŒ‡æ•°", "baostock", [["512930", "å¹³å®‰ä¸­è¯äººå·¥æ™ºèƒ½ETF", "AIETF"]]],
    [1, "sh.990001", "26ã€ä¸­è¯å…¨æŒ‡åŠå¯¼ä½“æŒ‡æ•°", "åŠå¯¼ä½“è¡Œä¸šæŒ‡æ•°", "baostock", [["512760", "å›½æ³°ä¸­è¯å…¨æŒ‡åŠå¯¼ä½“ETF", "èŠ¯ç‰‡ETF"]]],
    [1, "sh.000821", "27ã€ä¸­è¯çº¢åˆ©ä½æ³¢åŠ¨æŒ‡æ•°", "çº¢åˆ©ä½æ³¢åŠ¨ç­–ç•¥æŒ‡æ•°", "baostock", [["512890", "åæ³°æŸç‘çº¢åˆ©ä½æ³¢ETF", "çº¢åˆ©ä½æ³¢ETF"]]],
    [1, "H30533.CSI", "28ã€ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°", "æµ·å¤–ä¸Šå¸‚ä¸­å›½äº’è”ç½‘å…¬å¸", "yfinance", [["513050", "æ˜“æ–¹è¾¾ä¸­è¯æµ·å¤–äº’è”ETF", "ä¸­æ¦‚äº’è”ETF"]]],
    [1, "sh.000829", "29ã€ä¸Šæµ·é‡‘ETFæŒ‡æ•°", "å›½å†…é»„é‡‘ä»·æ ¼æŒ‡æ•°", "baostock", [["518880", "åå®‰é»„é‡‘ETF", "é»„é‡‘ETF"]]]
]


# å°†é…ç½®æ•°ç»„è½¬æ¢ä¸ºåŸå§‹çš„INDICESç»“æ„
INDICES = []
for config in INDICES_CONFIG:
    etfs = [{"code": e[0], "name": e[1], "description": e[2]} for e in config[5]]
    INDICES.append({
        "switch": config[0],
        "code": config[1],
        "name": config[2],
        "description": config[3],
        "source": config[4],
        "etfs": etfs
    })
# =============== æŒ‡æ•°é…ç½®åŒºç»“æŸ ===============

# =============== æ¶ˆæ¯é…ç½®åŒº ===============
# æ¶ˆæ¯æ¨¡æ¿ï¼Œå®Œå…¨é‡‡ç”¨æ•°ç»„å½¢å¼ï¼Œä¸æŒ‡æ•°é…ç½®æ ¼å¼ä¸€è‡´
# æ ¼å¼: [signal_type, scenario_type, [message_line1, message_line2, ...]]
SCENARIO_MESSAGES = [
    ["YES", "initial_breakout", [
        "ã€é¦–æ¬¡çªç ´ã€‘è¿ç»­{consecutive}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume:.1f}%",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰ç«‹å³å»ºä»“30%",
        "  â€¢ å«æ˜Ÿè¡Œä¸šETFç«‹å³å»ºä»“20%",
        "  â€¢ å›è°ƒè‡³5æ—¥çº¿{target_price:.2f}å¯åŠ ä»“20%",
        "âš ï¸ æ­¢æŸï¼šä¹°å…¥ä»·ä¸‹æ–¹5%ï¼ˆå®½åŸºETFï¼‰æˆ–3%ï¼ˆé«˜æ³¢åŠ¨ETFï¼‰"
    ]],
    ["YES", "confirmed_breakout", [
        "ã€é¦–æ¬¡çªç ´ç¡®è®¤ã€‘è¿ç»­{consecutive}å¤©ç«™ä¸Š20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume:.1f}%",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰å¯åŠ ä»“è‡³50%",
        "  â€¢ å«æ˜Ÿè¡Œä¸šETFå¯åŠ ä»“è‡³35%",
        "  â€¢ ä¸¥æ ¼è·Ÿè¸ª5æ—¥çº¿ä½œä¸ºæ­¢æŸä½{target_price:.2f}",
        "âš ï¸ æ³¨æ„ï¼šè‹¥æ”¶ç›˜è·Œç ´5æ—¥å‡çº¿ï¼Œç«‹å³å‡ä»“50%"
    ]],
    ["YES", "trend_stable", [
        "ã€è¶‹åŠ¿ç¨³å¥ã€‘è¿ç»­{consecutive}å¤©ç«™ä¸Š20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æŒä»“ä¸åŠ¨ï¼Œä¸æ–°å¢ä»“ä½",
        "  â€¢ è·Ÿè¸ªæ­¢æŸä¸Šç§»è‡³5æ—¥çº¿{target_price:.2f}",
        "  â€¢ è‹¥æ”¶ç›˜è·Œç ´5æ—¥å‡çº¿ï¼Œå‡ä»“50%",
        "{pattern_msg}"
    ]],
    ["YES", "trend_strong", [
        "ã€è¶‹åŠ¿è¾ƒå¼ºã€‘è¿ç»­{consecutive}å¤©ç«™ä¸Š20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ è§‚æœ›ï¼Œä¸æ–°å¢ä»“ä½",
        "  â€¢ é€¢é«˜å‡ä»“10%-15%ï¼ˆ{etf_code}ï¼‰",
        "  â€¢ è‹¥æ”¶ç›˜è·Œç ´10æ—¥å‡çº¿ï¼Œå‡ä»“30%",
        "{pattern_msg}"
    ]],
    ["YES", "overbought", [
        "ã€è¶…ä¹°é£é™©ã€‘è¿ç»­{consecutive}å¤©ç«™ä¸Š20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ é€¢é«˜å‡ä»“20%-30%ï¼ˆä»…å«æ˜ŸETFï¼‰",
        "  â€¢ å½“å‰ä»·æ ¼å·²å¤„é«˜ä½ï¼Œé¿å…æ–°å¢ä»“ä½",
        "  â€¢ ç­‰å¾…åç¦»ç‡å›è½è‡³â‰¤+5%çº¦{target_price:.2f}æ—¶åŠ å›",
        "{pattern_msg}"
    ]],
    ["NO", "initial_breakdown", [
        "ã€é¦–æ¬¡è·Œç ´ã€‘è¿ç»­{consecutive}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume:.1f}%",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰ç«‹å³å‡ä»“50%",
        "  â€¢ å«æ˜Ÿè¡Œä¸šETFç«‹å³å‡ä»“70%-80%",
        "  â€¢ æ­¢æŸä½ï¼š20æ—¥å‡çº¿ä¸Šæ–¹5%çº¦{target_price:.2f}",
        "âš ï¸ è‹¥æ”¶ç›˜æœªæ”¶å›å‡çº¿ï¼Œæ˜æ—¥ç»§ç»­å‡ä»“è‡³20%"
    ]],
    ["NO", "confirmed_breakdown", [
        "ã€é¦–æ¬¡è·Œç ´ç¡®è®¤ã€‘è¿ç»­{consecutive}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume:.1f}%",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰ä¸¥æ ¼æ­¢æŸæ¸…ä»“",
        "  â€¢ å«æ˜Ÿè¡Œä¸šETFä»…ä¿ç•™20%-30%åº•ä»“",
        "  â€¢ ä¸¥æ ¼æ­¢æŸï¼š20æ—¥å‡çº¿ä¸‹æ–¹5%ï¼ˆçº¦{target_price:.2f}ï¼‰",
        "âš ï¸ ä¿¡å·ç¡®è®¤ï¼Œé¿å…ä¾¥å¹¸å¿ƒç†"
    ]],
    ["NO", "decline_initial", [
        "ã€ä¸‹è·ŒåˆæœŸã€‘è¿ç»­{consecutive}å¤©è·Œç ´20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ è½»ä»“è§‚æœ›ï¼ˆä»“ä½â‰¤20%ï¼‰",
        "  â€¢ åå¼¹è‡³å‡çº¿é™„è¿‘{target_price:.2f}å‡ä»“å‰©ä½™ä»“ä½",
        "  â€¢ æš‚ä¸è€ƒè™‘æ–°å¢ä»“ä½",
        "âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šæ”¶ç›˜ç«™ä¸Š5æ—¥å‡çº¿ï¼Œå¯è½»ä»“è¯•å¤š"
    ]],
    ["NO", "decline_medium", [
        "ã€ä¸‹è·Œä¸­æœŸã€‘è¿ç»­{consecutive}å¤©è·Œç ´20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ ç©ºä»“ä¸ºä¸»ï¼Œé¿å…æŠ„åº•",
        "  â€¢ ä»…æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰å¯è¯•ä»“5%-10%",
        "  â€¢ ä¸¥æ ¼æ­¢æŸï¼šæ”¶ç›˜è·Œç ´å‰ä½å³ç¦»åœº",
        "âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šè¡Œä¸šåŸºæœ¬é¢æ˜¯å¦æœ‰åˆ©ç©ºï¼Œæœ‰åˆ©ç©ºåˆ™æ¸…ä»“"
    ]],
    ["NO", "oversold", [
        "ã€è¶…å–æœºä¼šã€‘è¿ç»­{consecutive}å¤©è·Œç ´20æ—¥å‡çº¿",
        "âœ… æ“ä½œå»ºè®®ï¼š",
        "  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{etf_code}ï¼‰å°å¹…åŠ ä»“10%-15%",
        "  â€¢ ç›®æ ‡ä»·ï¼šåç¦»ç‡â‰¥-5%çº¦{target_price:.2f}",
        "  â€¢ è¾¾åˆ°ç›®æ ‡å³å–å‡ºåŠ ä»“éƒ¨åˆ†",
        "âš ï¸ é‡ç‚¹è§‚å¯Ÿï¼šè‹¥è·Œç ´å‰ä½ï¼Œç«‹å³æ­¢æŸ"
    ]]
]

# è½¬æ¢ä¸ºå­—å…¸ç»“æ„ä»¥ä¾¿äºæŸ¥æ‰¾
SCENARIO_MESSAGES_DICT = {}
for message in SCENARIO_MESSAGES:
    signal_type = message[0]
    scenario_type = message[1]
    if signal_type not in SCENARIO_MESSAGES_DICT:
        SCENARIO_MESSAGES_DICT[signal_type] = {}
    SCENARIO_MESSAGES_DICT[signal_type][scenario_type] = message[2]
# =============== æ¶ˆæ¯é…ç½®åŒºç»“æŸ ===============

# =============== æ ¹æ®æµ‹è¯•ç»“æœä¼˜åŒ–çš„æ•°æ®æºé…ç½® ===============
# æµ‹è¯•æˆåŠŸçš„æŒ‡æ•°æ•°æ®æºæ˜ å°„
SUCCESSFUL_DATA_SOURCES = {
    # Aè‚¡æŒ‡æ•° - é€šè¿‡akshare stock_zh_index_dailyæ¥å£æˆåŠŸ
    "000688": {"primary": "akshare", "code": "sh000688", "interface": "stock_zh_index_daily"},
    "399006": {"primary": "akshare", "code": "sz399006", "interface": "stock_zh_index_daily"},
    "000016": {"primary": "akshare", "code": "sh000016", "interface": "stock_zh_index_daily"},
    "000300": {"primary": "akshare", "code": "sh000300", "interface": "stock_zh_index_daily"},
    "000905": {"primary": "akshare", "code": "sh000905", "interface": "stock_zh_index_daily"},
    "000852": {"primary": "akshare", "code": "sh000852", "interface": "stock_zh_index_daily"},
    
    # æ¸¯è‚¡æŒ‡æ•° - é€šè¿‡yfinance ETFæˆåŠŸï¼ˆæ³¨æ„ï¼šè¿™æ˜¯ETFï¼Œä¸æ˜¯æŒ‡æ•°æœ¬èº«ï¼‰
    "HSTECH": {"primary": "yfinance", "code": "3077.HK", "interface": "yfinance_etf"},
}

def get_optimized_data_source(index_code):
    """æ ¹æ®æµ‹è¯•ç»“æœè¿”å›ä¼˜åŒ–çš„æ•°æ®æºé…ç½®"""
    if index_code in SUCCESSFUL_DATA_SOURCES:
        return SUCCESSFUL_DATA_SOURCES[index_code]
    return None

# =============== å°†æŒ‡æ•°ä»£ç è½¬æ¢ä¸ºbaostockè¦æ±‚çš„æ ¼å¼ ===============
def convert_index_code_to_baostock_format(code: str) -> str:
    """
    Args:
        code: åŸå§‹æŒ‡æ•°ä»£ç 
    Returns:
        str: baostockæ ¼å¼çš„ä»£ç 
    """
    # baostockæŒ‡æ•°ä»£ç æ ¼å¼æ˜ å°„ - ä¿®å¤ä»£ç æ ¼å¼é—®é¢˜
    code_mapping = {
        "^HSTECH": "hk.807500",  # æ’ç”Ÿç§‘æŠ€æŒ‡æ•° - è¡¥è¶³6ä½
        "883418": "sh.883418",   # å¾®ç›˜è‚¡æŒ‡æ•°
        "000688": "sh.000688",   # ç§‘åˆ›50
        "899050": "bj.899050",   # åŒ—è¯50
        "HSCEI.HK": "hk.807000", # æ’ç”Ÿå›½ä¼æŒ‡æ•° - è¡¥è¶³6ä½
        "932000": "sh.932000",   # ä¸­è¯2000
        "GC=F": "",              # é»„é‡‘ä¸åœ¨baostockä¸­
        "^NDX": "",              # çº³æ–¯è¾¾å…‹ä¸åœ¨baostockä¸­
        "sh.000016": "sh.000016", # ä¸Šè¯50
        "sh.000300": "sh.000300", # æ²ªæ·±300
        "sz.399006": "sz.399006", # åˆ›ä¸šæ¿æŒ‡
        "sh.000905": "sh.000905", # ä¸­è¯500
        "sh.000852": "sh.000852", # ä¸­è¯1000
        "KWEB": "",              # ä¸­æ¦‚äº’è”ä¸åœ¨baostockä¸­
        "^HSI": "hk.800000",     # æ’ç”ŸæŒ‡æ•° - è¡¥è¶³6ä½
        # æ–°å¢æŒ‡æ•°çš„baostockä»£ç æ˜ å°„
        "sh.000012": "sh.000012", # ä¸Šè¯å›½å€ºæŒ‡æ•°
        "sz.399005": "sz.399005", # ä¸­å°æ¿æŒ‡
        "sz.399395": "sz.399395", # å›½è¯æœ‰è‰²é‡‘å±æŒ‡æ•°
        "sz.399967": "sz.399967", # ä¸­è¯å†›å·¥æŒ‡æ•°
        "sz.399975": "sz.399975", # ä¸­è¯è¯åˆ¸æŒ‡æ•°
        "sh.930713": "sh.930713", # ä¸­è¯AIäº§ä¸šæŒ‡æ•°
        "sh.990001": "sh.990001", # ä¸­è¯å…¨æŒ‡åŠå¯¼ä½“æŒ‡æ•°
        "sh.000821": "sh.000821", # ä¸­è¯çº¢åˆ©ä½æ³¢åŠ¨æŒ‡æ•°
        "H30533.CSI": "",        # ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°ä¸åœ¨baostockä¸­
        "sh.000829": "sh.000829"  # ä¸Šæµ·é‡‘ETFæŒ‡æ•°
    }
    
    return code_mapping.get(code, code)
# =============== å°†æŒ‡æ•°ä»£ç è½¬æ¢ä¸ºbaostockè¦æ±‚çš„æ ¼å¼ ===============

# ç­–ç•¥å‚æ•°
CRITICAL_VALUE_DAYS = 20  # è®¡ç®—ä¸´ç•Œå€¼çš„å‘¨æœŸï¼ˆ20æ—¥å‡çº¿ï¼‰
DEVIATION_THRESHOLD = 0.02  # åç¦»é˜ˆå€¼ï¼ˆ2%ï¼‰
PATTERN_CONFIDENCE_THRESHOLD = 0.7  # å½¢æ€ç¡®è®¤é˜ˆå€¼ï¼ˆ70%ç½®ä¿¡åº¦ï¼‰

def fetch_baostock_data_simplified(index_code: str, days: int = 250) -> pd.DataFrame:
    """
    ç®€åŒ–çš„baostockæ•°æ®è·å–å‡½æ•°ï¼ˆä¸åŒ…å«ç™»å½•é€€å‡ºï¼‰
    Args:
        index_code: å·²è½¬æ¢çš„baostockæ ¼å¼ä»£ç 
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # å¦‚æœä»£ç ä¸ºç©ºï¼Œè¡¨ç¤ºä¸æ”¯æŒè¯¥æŒ‡æ•°
        if not index_code:
            return pd.DataFrame()
            
        # æ·»åŠ éšæœºå»¶æ—¶é¿å…è¢«å°
        time.sleep(random.uniform(5.0, 8.0))
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date_dt = datetime.now()
        start_date_dt = end_date_dt - timedelta(days=days)
        start_date = start_date_dt.strftime("%Y-%m-%d")
        end_date = end_date_dt.strftime("%Y-%m-%d")
        
        # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
        logger.info(f"ğŸ“¡ è°ƒç”¨baostockæ¥å£ query_history_k_data_plus")
        logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: code={index_code}, start_date={start_date}, end_date={end_date}, frequency=d, adjustflag=3, fields=date,open,high,low,close,volume,amount")
        
        # ä½¿ç”¨baostockè·å–æ•°æ®ï¼ˆå·²ç»åœ¨å¤–å±‚ç™»å½•ï¼‰
        rs = bs.query_history_k_data_plus(index_code,
                                         "date,open,high,low,close,volume,amount",
                                         start_date=start_date,
                                         end_date=end_date,
                                         frequency="d",
                                         adjustflag="3")
        # æ£€æŸ¥è¿”å›ç»“æœ
        if rs.error_code != '0':
            logger.error(f"è·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {rs.error_msg}")
            return pd.DataFrame()
            
        # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
        data_list = []
        while rs.next():
            data_list.append(rs.get_row_data())
            
        if not data_list:
            logger.warning(f"è·å–æŒ‡æ•° {index_code} æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
            
        df = pd.DataFrame(data_list, columns=rs.fields)
        
        # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
        logger.info(f"âœ… baostockæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®æ¡æ•°: {len(df)}")
        if len(df) > 0:
            logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict()}")
            logger.info(f"ğŸ“Š æœ€åä¸€æ¡è¿”å›æ•°æ®: {df.iloc[-1].to_dict()}")
        
        # æ ‡å‡†åŒ–åˆ—åå’Œå¤„ç†æ•°æ®æ ¼å¼
        df = df.rename(columns={
            'date': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢'
        })
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        
        # ç¡®ä¿ä»·æ ¼åˆ—æ˜¯æ•°å€¼ç±»å‹
        price_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']
        for col in price_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        # ç¡®ä¿æˆäº¤é‡å’Œæˆäº¤é¢æ˜¯æ•°å€¼ç±»å‹
        volume_columns = ['æˆäº¤é‡', 'æˆäº¤é¢']
        for col in volume_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        initial_count = len(df)
        df = df.dropna(subset=price_columns)
        after_count = len(df)
        logger.info(f"åˆ é™¤NaNåæ•°æ®é‡: {after_count}/{initial_count}")
        
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        if len(df) <= 1:
            logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
            return pd.DataFrame()
            
        logger.info(f"âœ… é€šè¿‡baostockæˆåŠŸè·å–åˆ° {len(df)} æ¡æŒ‡æ•°æ•°æ®")
        return df
        
    except Exception as e:
        logger.error(f"é€šè¿‡baostockè·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def fetch_yfinance_data(index_code: str, days: int = 250) -> pd.DataFrame:
    """
    ä»yfinanceè·å–å›½é™…/æ¸¯è‚¡/ç¾è‚¡æŒ‡æ•°å†å²æ•°æ®
    Args:
        index_code: æŒ‡æ•°ä»£ç 
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # æ·»åŠ éšæœºå»¶æ—¶é¿å…è¢«å°
        time.sleep(random.uniform(5.0, 8.0))
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date_dt = datetime.now()
        start_date_dt = end_date_dt - timedelta(days=days)
        end_date = end_date_dt.strftime("%Y-%m-%d")
        start_date = start_date_dt.strftime("%Y-%m-%d")
        
        # æ¸¯è‚¡æŒ‡æ•°ä»£ç æ˜ å°„ - å°è¯•å¤šç§æ ¼å¼
        hk_code_mapping = {
            "^HSTECH": ["3077.HK", "HSTECH.HK", "HSTEC.HK"],  # æ’ç”Ÿç§‘æŠ€æŒ‡æ•°ETF
            "HSCEI.HK": ["2828.HK", "HSCEI.HK"],  # æ’ç”Ÿå›½ä¼æŒ‡æ•°ETF
            "^HSI": ["2800.HK", "HSI.HK"],  # æ’ç”ŸæŒ‡æ•°ETF
            "H30533.CSI": ["KWEB", "3067.HK"]  # ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘æŒ‡æ•°ETF
        }
        
        # ä½¿ç”¨æ˜ å°„åçš„ä»£ç 
        actual_codes = hk_code_mapping.get(index_code, [index_code])
        
        for actual_code in actual_codes:
            try:
                # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                logger.info(f"ğŸ“¡ è°ƒç”¨yfinanceæ¥å£ download")
                logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={actual_code}, start={start_date}, end={end_date}, auto_adjust=False")
                
                df = yf.download(actual_code, start=start_date, end=end_date, auto_adjust=False)
                
                # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                logger.info(f"âœ… yfinanceæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape}")
                if not df.empty:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®ç´¢å¼•: {df.index[0] if hasattr(df.index, '__len__') else 'N/A'}")
                    logger.info(f"ğŸ“Š æ•°æ®åˆ—å: {df.columns.tolist()}")
                    if len(df) > 0:
                        first_row = df.iloc[0] if isinstance(df, pd.DataFrame) else None
                        if first_row is not None:
                            logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®å€¼: {first_row.to_dict()}")
                
                # å¤„ç†yfinanceè¿”å›çš„MultiIndexåˆ—å
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
                    
                if df.empty:
                    logger.warning(f"yfinanceè·å–æŒ‡æ•° {actual_code} æ•°æ®ä¸ºç©ºï¼Œå°è¯•ä¸‹ä¸€ä¸ªä»£ç ")
                    continue
                
                # æ ‡å‡†åŒ–åˆ—å
                df = df.reset_index()
                df = df.rename(columns={
                    'Date': 'æ—¥æœŸ',
                    'Open': 'å¼€ç›˜',
                    'High': 'æœ€é«˜',
                    'Low': 'æœ€ä½',
                    'Close': 'æ”¶ç›˜',
                    'Volume': 'æˆäº¤é‡'
                })
                
                # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                
                # ç¡®ä¿ä»·æ ¼åˆ—æ˜¯æ•°å€¼ç±»å‹
                price_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']
                for col in price_columns:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # ç¡®ä¿æˆäº¤é‡æ˜¯æ•°å€¼ç±»å‹
                if 'æˆäº¤é‡' in df.columns:
                    df['æˆäº¤é‡'] = pd.to_numeric(df['æˆäº¤é‡'], errors='coerce')
                
                # æ·»åŠ æˆäº¤é¢åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if 'æˆäº¤é¢' not in df.columns:
                    df['æˆäº¤é¢'] = np.nan
                
                # åˆ é™¤åŒ…å«NaNçš„è¡Œ
                if 'æ”¶ç›˜' in df.columns:
                    df = df.dropna(subset=['æ”¶ç›˜'])
                
                # æ’åº
                df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                
                if len(df) <= 1:
                    logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
                    continue
                
                logger.info(f"âœ… é€šè¿‡yfinanceæˆåŠŸè·å–åˆ° {len(df)} æ¡æŒ‡æ•°æ•°æ®ï¼Œä»£ç : {actual_code}")
                return df
                
            except Exception as e:
                logger.warning(f"yfinanceä»£ç  {actual_code} è·å–å¤±è´¥: {str(e)}ï¼Œå°è¯•ä¸‹ä¸€ä¸ª")
                continue
        
        logger.warning(f"æ‰€æœ‰yfinanceä»£ç å°è¯•éƒ½å¤±è´¥: {actual_codes}")
        return pd.DataFrame()
        
    except Exception as e:
        logger.error(f"è·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def fetch_akshare_data(index_code: str, days: int = 250) -> pd.DataFrame:
    """
    ä»akshareè·å–æŒ‡æ•°å†å²æ•°æ®
    Args:
        index_code: æŒ‡æ•°ä»£ç 
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # æ·»åŠ éšæœºå»¶æ—¶é¿å…è¢«å°
        time.sleep(random.uniform(5.0, 8.0))
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date_dt = datetime.now()
        start_date_dt = end_date_dt - timedelta(days=days)
        end_date = end_date_dt.strftime("%Y%m%d")
        start_date = start_date_dt.strftime("%Y%m%d")
        
        logger.info(f"ä½¿ç”¨akshareè·å–æŒ‡æ•° {index_code} æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        
        # æ ¹æ®æŒ‡æ•°ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„akshareæ¥å£
        if index_code.startswith(('0', '3', '6', '8', '9')):  # Aè‚¡æŒ‡æ•°
            try:
                # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ index_zh_a_hist")
                logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}, period=daily, start_date={start_date}, end_date={end_date}")
                
                df = ak.index_zh_a_hist(
                    symbol=index_code,
                    period="daily",
                    start_date=start_date,
                    end_date=end_date
                )
                
                # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                logger.info(f"âœ… akshare index_zh_a_histæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
                if not df.empty:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                    
            except Exception as e:
                logger.warning(f"index_zh_a_histæ¥å£å¤±è´¥ï¼Œå°è¯•stock_zh_index_daily: {str(e)}")
                # å°è¯•å¤‡ç”¨æ¥å£
                market_code = f"sh{index_code}" if index_code.startswith(('00', '60', '88', '93')) else f"sz{index_code}" if index_code.startswith('399') else f"bj{index_code}" if index_code.startswith('899') else index_code
                
                # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ stock_zh_index_daily")
                logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={market_code}")
                
                df = ak.stock_zh_index_daily(symbol=market_code)
                
                # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                logger.info(f"âœ… akshare stock_zh_index_dailyæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
                if not df.empty:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                    
        elif index_code.startswith('H') or index_code.startswith('^'):  # æ¸¯è‚¡æŒ‡æ•°
            # ç§»é™¤ä¸å­˜åœ¨çš„index_hk_histï¼Œç›´æ¥ä½¿ç”¨æœ‰æ•ˆçš„æ¥å£
            try:
                # æ¸…ç†ä»£ç æ ¼å¼
                clean_code = index_code.replace('^', '')
                
                # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ stock_hk_index_daily_em")
                logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={clean_code}, start_date={start_date}, end_date={end_date}")
                
                df = ak.stock_hk_index_daily_em(
                    symbol=clean_code,
                    start_date=start_date,
                    end_date=end_date
                )
                
                # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                logger.info(f"âœ… akshare stock_hk_index_daily_emæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
                if not df.empty:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                    
            except Exception as e:
                logger.warning(f"stock_hk_index_daily_emå¤±è´¥: {str(e)}")
                # å°è¯•å…¶ä»–æ¸¯è‚¡æ¥å£
                try:
                    # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                    logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ index_global_hist_em")
                    logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={clean_code}")
                    
                    df = ak.index_global_hist_em(symbol=clean_code)
                    
                    # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                    logger.info(f"âœ… akshare index_global_hist_emæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
                    if not df.empty:
                        logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                        
                except Exception as e2:
                    logger.warning(f"index_global_hist_emä¹Ÿå¤±è´¥: {str(e2)}")
                    return pd.DataFrame()
        else:
            # é»˜è®¤å¤„ç†
            try:
                # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
                logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ index_zh_a_hist (é»˜è®¤)")
                logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}, period=daily, start_date={start_date}, end_date={end_date}")
                
                df = ak.index_zh_a_hist(
                    symbol=index_code,
                    period="daily",
                    start_date=start_date,
                    end_date=end_date
                )
                
                # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
                logger.info(f"âœ… akshare index_zh_a_histæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
                if not df.empty:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                    
            except Exception as e:
                logger.warning(f"é»˜è®¤æ¥å£å¤±è´¥: {str(e)}")
                return pd.DataFrame()
        
        if df.empty:
            logger.warning(f"é€šè¿‡akshareè·å–æŒ‡æ•° {index_code} æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
            
        # æ ‡å‡†åŒ–åˆ—å
        df = df.rename(columns={
            'date': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢'
        })
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        
        # ç¡®ä¿ä»·æ ¼åˆ—æ˜¯æ•°å€¼ç±»å‹
        price_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']
        for col in price_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç¡®ä¿æˆäº¤é‡å’Œæˆäº¤é¢æ˜¯æ•°å€¼ç±»å‹
        volume_columns = ['æˆäº¤é‡', 'æˆäº¤é¢']
        for col in volume_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna(subset=['æ”¶ç›˜'])
        
        # æ’åº
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        # æ£€æŸ¥æ•°æ®é‡
        if len(df) <= 1:
            logger.warning(f"âš ï¸ åªè·å–åˆ°{len(df)}æ¡æ•°æ®ï¼Œå¯èƒ½æ˜¯å½“å¤©æ•°æ®ï¼Œæ— æ³•ç”¨äºå†å²åˆ†æ")
            return pd.DataFrame()
            
        logger.info(f"âœ… é€šè¿‡akshareæˆåŠŸè·å–åˆ° {len(df)} æ¡æŒ‡æ•°æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´: {df['æ—¥æœŸ'].min()} è‡³ {df['æ—¥æœŸ'].max()}")
        return df
        
    except Exception as e:
        logger.error(f"é€šè¿‡akshareè·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def fetch_akshare_stock_zh_index_daily(index_code: str, days: int = 250) -> pd.DataFrame:
    """
    ä½¿ç”¨akshareçš„stock_zh_index_dailyæ¥å£è·å–æŒ‡æ•°æ•°æ®
    Args:
        index_code: akshareæ ¼å¼çš„æŒ‡æ•°ä»£ç ï¼ˆå¦‚sh000688ï¼‰
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        pd.DataFrame: æŒ‡æ•°æ—¥çº¿æ•°æ®
    """
    try:
        # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
        logger.info(f"ğŸ“¡ è°ƒç”¨akshareæ¥å£ stock_zh_index_daily")
        logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}")
        
        df = ak.stock_zh_index_daily(symbol=index_code)
        
        # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
        logger.info(f"âœ… akshare stock_zh_index_dailyæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape}")
        if not df.empty:
            logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®ç´¢å¼•: {df.index[0] if hasattr(df.index, '__len__') else 'N/A'}")
            logger.info(f"ğŸ“Š æ•°æ®åˆ—å: {df.columns.tolist()}")
            if len(df) > 0:
                first_row = df.iloc[0] if isinstance(df, pd.DataFrame) else None
                if first_row is not None:
                    logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®å€¼: {first_row.to_dict()}")
        
        if not df.empty:
            # æ ‡å‡†åŒ–åˆ—å
            df = df.reset_index()
            df = df.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜', 
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡'
            })
            
            # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            
            # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®
            end_date_dt = datetime.now()
            start_date_dt = end_date_dt - timedelta(days=days)
            df = df[(df['æ—¥æœŸ'] >= start_date_dt) & (df['æ—¥æœŸ'] <= end_date_dt)]
            
            # ç¡®ä¿ä»·æ ¼åˆ—æ˜¯æ•°å€¼ç±»å‹
            price_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜']
            for col in price_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    
            # ç¡®ä¿æˆäº¤é‡æ˜¯æ•°å€¼ç±»å‹
            if 'æˆäº¤é‡' in df.columns:
                df['æˆäº¤é‡'] = pd.to_numeric(df['æˆäº¤é‡'], errors='coerce')
                
            # æ·»åŠ æˆäº¤é¢åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'æˆäº¤é¢' not in df.columns:
                df['æˆäº¤é¢'] = np.nan
                
            logger.info(f"âœ… é€šè¿‡akshare stock_zh_index_dailyæˆåŠŸè·å– {len(df)} æ¡æŒ‡æ•°æ•°æ®")
            return df
        else:
            logger.warning(f"akshare stock_zh_index_dailyè·å–æŒ‡æ•° {index_code} æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"é€šè¿‡akshare stock_zh_index_dailyè·å–æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {str(e)}")
        return pd.DataFrame()

# =============== é’ˆå¯¹å¤±è´¥æŒ‡æ•°çš„å¢å¼ºæ•°æ®è·å–å‡½æ•° ===============
def fetch_failed_indices_enhanced(index_code: str, index_name: str, days: int = 250) -> tuple:
    """
    é’ˆå¯¹å¤±è´¥æŒ‡æ•°çš„å¢å¼ºæ•°æ®è·å–å‡½æ•°ï¼Œå°è¯•å¤šä¸ªakshareæ¥å£
    Args:
        index_code: æŒ‡æ•°ä»£ç 
        index_name: æŒ‡æ•°åç§°
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        tuple: (DataFrame, å®é™…ä½¿ç”¨çš„æ•°æ®æº)
    """
    logger.info(f"å¼€å§‹å¢å¼ºè·å–å¤±è´¥æŒ‡æ•° {index_name}({index_code}) çš„æ•°æ®")
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date_dt = datetime.now()
    start_date_dt = end_date_dt - timedelta(days=days)
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    # æ ¹æ®æŒ‡æ•°ç±»å‹å°è¯•ä¸åŒçš„æ¥å£
    if index_code in ["883418", "932000", "899050"]:  # Aè‚¡å¤±è´¥æŒ‡æ•°
        logger.info(f"ä¸ºAè‚¡å¤±è´¥æŒ‡æ•° {index_name}({index_code}) å°è¯•å¤šä¸ªakshareæ¥å£")
        
        # æ¥å£1: å°è¯•stock_zh_index_daily
        try:
            if index_code.startswith(('00', '88', '93')):
                market_code = f"sh{index_code}"
            elif index_code.startswith('399'):
                market_code = f"sz{index_code}"
            elif index_code.startswith('899'):
                market_code = f"bj{index_code}"
            else:
                market_code = index_code
                
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ stock_zh_index_daily")
            logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={market_code}")
                
            df = ak.stock_zh_index_daily(symbol=market_code)
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare stock_zh_index_dailyå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape}")
            if not df.empty:
                logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                
            if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                logger.info(f"âœ… é€šè¿‡stock_zh_index_dailyæˆåŠŸè·å– {index_name} æ•°æ®")
                df = df.reset_index()
                df = df.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜', 
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜',
                    'volume': 'æˆäº¤é‡'
                })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df, "akshare_stock_zh_index_daily"
        except Exception as e:
            logger.warning(f"æ¥å£stock_zh_index_dailyå¤±è´¥: {str(e)}")
        
        # æ¥å£2: å°è¯•index_zh_a_hist
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ index_zh_a_hist")
            logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}, period=daily, start_date={start_date}, end_date={end_date}")
            
            df = ak.index_zh_a_hist(
                symbol=index_code,
                period="daily",
                start_date=start_date,
                end_date=end_date
            )
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare index_zh_a_histå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
            if not df.empty:
                logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                
            if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                logger.info(f"âœ… é€šè¿‡index_zh_a_histæˆåŠŸè·å– {index_name} æ•°æ®")
                df = df.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜',
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜',
                    'volume': 'æˆäº¤é‡',
                    'amount': 'æˆäº¤é¢'
                })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df, "akshare_index_zh_a_hist"
        except Exception as e:
            logger.warning(f"æ¥å£index_zh_a_histå¤±è´¥: {str(e)}")
        
        # æ¥å£3: å°è¯•index_csindex_all (ä¸­è¯æŒ‡æ•°)
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ index_csindex_all")
            
            df_all = ak.index_csindex_all()
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare index_csindex_allå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df_all.shape}")
            if not df_all.empty:
                logger.info(f"ğŸ“Š åŒ…å«çš„æŒ‡æ•°æ•°é‡: {len(df_all)}")
                
            if not df_all.empty and 'æŒ‡æ•°ä»£ç ' in df_all.columns:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æŒ‡æ•°
                if index_code in df_all['æŒ‡æ•°ä»£ç '].values:
                    logger.info(f"âœ… åœ¨index_csindex_allä¸­æ‰¾åˆ° {index_name}ï¼Œä½†éœ€è¦å•ç‹¬è·å–å†å²æ•°æ®")
                    # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†è·å–å…·ä½“æŒ‡æ•°çš„å†å²æ•°æ®
        except Exception as e:
            logger.warning(f"æ¥å£index_csindex_allå¤±è´¥: {str(e)}")
        
        # æ¥å£4: å°è¯•index_stock_info
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ index_stock_info")
            
            df_info = ak.index_stock_info()
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare index_stock_infoå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df_info.shape}")
            if not df_info.empty:
                logger.info(f"ğŸ“Š åŒ…å«çš„æŒ‡æ•°æ•°é‡: {len(df_info)}")
                
            if not df_info.empty and 'æŒ‡æ•°ä»£ç ' in df_info.columns:
                if index_code in df_info['æŒ‡æ•°ä»£ç '].values:
                    logger.info(f"âœ… åœ¨index_stock_infoä¸­æ‰¾åˆ° {index_name}ï¼Œä½†éœ€è¦å•ç‹¬è·å–å†å²æ•°æ®")
        except Exception as e:
            logger.warning(f"æ¥å£index_stock_infoå¤±è´¥: {str(e)}")
    
    elif index_code in ["HSTECH", "HSCEI", "HSI"]:  # æ¸¯è‚¡å¤±è´¥æŒ‡æ•°
        logger.info(f"ä¸ºæ¸¯è‚¡å¤±è´¥æŒ‡æ•° {index_name}({index_code}) å°è¯•å¤šä¸ªakshareæ¥å£")
        
        # æ¥å£1: å°è¯•stock_hk_index_daily_em
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ stock_hk_index_daily_em")
            logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}, start_date={start_date}, end_date={end_date}")
            
            df = ak.stock_hk_index_daily_em(symbol=index_code)
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare stock_hk_index_daily_emå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
            if not df.empty:
                logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                
            if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                logger.info(f"âœ… é€šè¿‡stock_hk_index_daily_emæˆåŠŸè·å– {index_name} æ•°æ®")
                df = df.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜',
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜',
                    'volume': 'æˆäº¤é‡'
                })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df, "akshare_stock_hk_index_daily_em"
        except Exception as e:
            logger.warning(f"æ¥å£stock_hk_index_daily_emå¤±è´¥: {str(e)}")
        
        # æ¥å£2: å°è¯•index_global_hist_em
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ index_global_hist_em")
            logger.info(f"ğŸ“‹ è°ƒç”¨å‚æ•°: symbol={index_code}")
            
            df = ak.index_global_hist_em(symbol=index_code)
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare index_global_hist_emå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'N/A'}")
            if not df.empty:
                logger.info(f"ğŸ“Š ç¬¬ä¸€æ¡è¿”å›æ•°æ®: {df.iloc[0].to_dict() if hasattr(df, 'iloc') else 'N/A'}")
                
            if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                logger.info(f"âœ… é€šè¿‡index_global_hist_emæˆåŠŸè·å– {index_name} æ•°æ®")
                df = df.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜',
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜'
                })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df, "akshare_index_global_hist_em"
        except Exception as e:
            logger.warning(f"æ¥å£index_global_hist_emå¤±è´¥: {str(e)}")
        
        # æ¥å£3: å°è¯•index_global_spot_emè·å–å®æ—¶æ•°æ®
        try:
            # è®°å½•è¯¦ç»†çš„æ¥å£è°ƒç”¨ä¿¡æ¯
            logger.info(f"ğŸ“¡ è°ƒç”¨akshareå¢å¼ºæ¥å£ index_global_spot_em")
            
            df_spot = ak.index_global_spot_em()
            
            # è®°å½•è¯¦ç»†çš„æ•°æ®è¿”å›ä¿¡æ¯
            logger.info(f"âœ… akshare index_global_spot_emå¢å¼ºæ¥å£è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®å½¢çŠ¶: {df_spot.shape}")
            if not df_spot.empty:
                logger.info(f"ğŸ“Š åŒ…å«çš„æŒ‡æ•°æ•°é‡: {len(df_spot)}")
                
            if not df_spot.empty and 'åç§°' in df_spot.columns:
                # æŸ¥æ‰¾ç›®æ ‡æŒ‡æ•°
                target_row = df_spot[df_spot['åç§°'].str.contains(index_code, na=False)]
                if not target_row.empty:
                    logger.info(f"âœ… åœ¨index_global_spot_emä¸­æ‰¾åˆ° {index_name} å®æ—¶æ•°æ®")
                    # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†è·å–å†å²æ•°æ®
        except Exception as e:
            logger.warning(f"æ¥å£index_global_spot_emå¤±è´¥: {str(e)}")
    
    # æ‰€æœ‰æ¥å£éƒ½å¤±è´¥
    logger.warning(f"æ‰€æœ‰å¢å¼ºæ¥å£éƒ½æ— æ³•è·å– {index_name}({index_code}) æ•°æ®")
    return pd.DataFrame(), "æ‰€æœ‰å¢å¼ºæ¥å£å‡å¤±è´¥"

def fetch_index_data_smart(index_info: dict, days: int = 250) -> tuple:
    """
    æ™ºèƒ½æ•°æ®è·å–å‡½æ•°ï¼Œå½“é¦–é€‰æ•°æ®æºå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
    Args:
        index_info: æŒ‡æ•°ä¿¡æ¯å­—å…¸ï¼ˆåŒ…å«code, name, sourceç­‰ï¼‰
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
    Returns:
        tuple: (DataFrame, å®é™…ä½¿ç”¨çš„æ•°æ®æº)
    """
    code = index_info["code"]
    name = index_info["name"]
    preferred_source = index_info["source"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æˆåŠŸçš„ä¼˜åŒ–æ•°æ®æº
    optimized_source = get_optimized_data_source(code)
    if optimized_source:
        logger.info(f"ä¸ºæŒ‡æ•° {name}({code}) ä½¿ç”¨æµ‹è¯•æˆåŠŸçš„ä¼˜åŒ–æ•°æ®æº: {optimized_source}")
        
        try:
            if optimized_source["primary"] == "akshare" and optimized_source["interface"] == "stock_zh_index_daily":
                df = fetch_akshare_stock_zh_index_daily(optimized_source["code"], days)
                if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                    return df, f"akshare_stock_zh_index_daily({optimized_source['code']})"
            
            elif optimized_source["primary"] == "yfinance":
                df = fetch_yfinance_data(optimized_source["code"], days)
                if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                    return df, f"yfinance({optimized_source['code']})"
                    
        except Exception as e:
            logger.warning(f"ä¼˜åŒ–æ•°æ®æºè·å–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘: {str(e)}")
    
    # åŸå§‹çš„æ•°æ®æºè·å–é€»è¾‘
    data_sources = ["baostock", "akshare", "yfinance"]
    
    # å¦‚æœé¦–é€‰æ•°æ®æºä¸åœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­ï¼Œå°†å…¶æ·»åŠ åˆ°æœ€å‰é¢
    if preferred_source in data_sources:
        # å°†é¦–é€‰æ•°æ®æºç§»åˆ°æœ€å‰é¢
        data_sources.remove(preferred_source)
        data_sources.insert(0, preferred_source)
    else:
        # å¦‚æœé¦–é€‰æ•°æ®æºä¸åœ¨å·²çŸ¥åˆ—è¡¨ä¸­ï¼Œå°†å…¶æ·»åŠ åˆ°æœ€å‰é¢
        data_sources.insert(0, preferred_source)
    
    logger.info(f"ä¸ºæŒ‡æ•° {name}({code}) å°è¯•æ•°æ®æºé¡ºåº: {data_sources}")
    
    # å°è¯•æ¯ä¸ªæ•°æ®æºï¼Œç›´åˆ°æˆåŠŸè·å–æ•°æ®
    for source in data_sources:
        logger.info(f"å°è¯•ä½¿ç”¨ {source} æ•°æ®æºè·å– {name}({code}) æ•°æ®")
        
        try:
            if source == "baostock":
                # è½¬æ¢ä»£ç ä¸ºbaostockæ ¼å¼
                baostock_code = convert_index_code_to_baostock_format(code)
                df = fetch_baostock_data_simplified(baostock_code, days)
            elif source == "yfinance":
                df = fetch_yfinance_data(code, days)
            elif source == "akshare":
                df = fetch_akshare_data(code, days)
            else:
                logger.warning(f"æœªçŸ¥æ•°æ®æº: {source}")
                continue
            
            # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®
            if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
                logger.info(f"âœ… æˆåŠŸé€šè¿‡ {source} è·å–åˆ° {name} æ•°æ®")
                return df, source
            
            # å¦‚æœæ•°æ®é‡ä¸è¶³ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ•°æ®æº
            if not df.empty and len(df) < CRITICAL_VALUE_DAYS:
                logger.warning(f"é€šè¿‡ {source} è·å–çš„ {name} æ•°æ®é‡ä¸è¶³ï¼Œç»§ç»­å°è¯•å…¶ä»–æ•°æ®æº")
                continue
                
        except Exception as e:
            logger.error(f"é€šè¿‡ {source} è·å– {name} æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            continue
    
    # =============== æ–°å¢ï¼šå¦‚æœå¸¸è§„æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•å¢å¼ºæ•°æ®è·å– ===============
    logger.info(f"å¸¸è§„æ•°æ®æºå…¨éƒ¨å¤±è´¥ï¼Œå¼€å§‹å¢å¼ºæ•°æ®è·å–: {name}({code})")
    df, enhanced_source = fetch_failed_indices_enhanced(code, name, days)
    if not df.empty and len(df) >= CRITICAL_VALUE_DAYS:
        logger.info(f"âœ… é€šè¿‡å¢å¼ºæ¥å£æˆåŠŸè·å– {name} æ•°æ®")
        return df, f"enhanced_{enhanced_source}"
    
    # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
    logger.error(f"æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è·å– {name}({code}) çš„æœ‰æ•ˆæ•°æ®")
    return pd.DataFrame(), "æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥"

def calculate_critical_value(df: pd.DataFrame) -> float:
    """è®¡ç®—ä¸´ç•Œå€¼ï¼ˆ20æ—¥å‡çº¿ï¼‰"""
    if len(df) < CRITICAL_VALUE_DAYS:
        logger.warning(f"æ•°æ®ä¸è¶³{CRITICAL_VALUE_DAYS}å¤©ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—ä¸´ç•Œå€¼")
        # åªè®¡ç®—éNaNå€¼çš„å‡å€¼
        valid_data = df["æ”¶ç›˜"].dropna()
        return valid_data.mean() if not valid_data.empty else 0.0
    # è®¡ç®—æ»šåŠ¨å‡å€¼ï¼Œå¿½ç•¥NaNå€¼
    ma = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS, min_periods=1).mean()
    # è¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆå€¼
    for i in range(len(ma)-1, -1, -1):
        if not np.isnan(ma.iloc[i]):
            return ma.iloc[i]
    return df["æ”¶ç›˜"].dropna().mean()

def calculate_deviation(current: float, critical: float) -> float:
    """è®¡ç®—åç¦»ç‡"""
    return (current - critical) / critical * 100

def calculate_consecutive_days_above(df: pd.DataFrame, critical_value: float) -> int:
    """è®¡ç®—è¿ç»­ç«™ä¸Šå‡çº¿çš„å¤©æ•°"""
    if len(df) < 2:
        return 0
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    # è®¡ç®—å‡çº¿ï¼ˆä½¿ç”¨ä¸ä¸»è®¡ç®—ç›¸åŒçš„é€»è¾‘ï¼‰
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS, min_periods=1).mean().values
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥
    consecutive_days = 0
    for i in range(len(close_prices)-1, -1, -1):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1:
            # ä½¿ç”¨å½“å‰è®¡ç®—çš„å‡çº¿å€¼
            if i < len(ma_values) and not np.isnan(ma_values[i]):
                if close_prices[i] >= ma_values[i]:
                    consecutive_days += 1
                else:
                    break
        else:
            # ä½¿ç”¨è®¡ç®—å‡ºçš„å‡çº¿å€¼
            if not np.isnan(close_prices[i]) and not np.isnan(ma_values[i]) and close_prices[i] >= ma_values[i]:
                consecutive_days += 1
            else:
                break
    return consecutive_days

def calculate_consecutive_days_below(df: pd.DataFrame, critical_value: float) -> int:
    """è®¡ç®—è¿ç»­è·Œç ´å‡çº¿çš„å¤©æ•°"""
    if len(df) < 2:
        return 0
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    # è®¡ç®—å‡çº¿ï¼ˆä½¿ç”¨ä¸ä¸»è®¡ç®—ç›¸åŒçš„é€»è¾‘ï¼‰
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS, min_periods=1).mean().values
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥
    consecutive_days = 0
    for i in range(len(close_prices)-1, -1, -1):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1:
            # ä½¿ç”¨å½“å‰è®¡ç®—çš„å‡çº¿å€¼
            if i < len(ma_values) and not np.isnan(ma_values[i]):
                if close_prices[i] < ma_values[i]:
                    consecutive_days += 1
                else:
                    break
        else:
            # ä½¿ç”¨è®¡ç®—å‡ºçš„å‡çº¿å€¼
            if not np.isnan(close_prices[i]) and not np.isnan(ma_values[i]) and close_prices[i] < ma_values[i]:
                consecutive_days += 1
            else:
                break
    return consecutive_days

def calculate_volume_change(df: pd.DataFrame) -> float:
    """
    è®¡ç®—æˆäº¤é‡å˜åŒ–ç‡
    Args:
        df: ETFæ—¥çº¿æ•°æ®
    Returns:
        float: æˆäº¤é‡å˜åŒ–ç‡
    """
    try:
        if len(df) < 2:
            logger.warning("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æˆäº¤é‡å˜åŒ–")
            return 0.0
        # è·å–æœ€æ–°ä¸¤ä¸ªäº¤æ˜“æ—¥çš„æˆäº¤é‡
        current_volume = df['æˆäº¤é‡'].values[-1]
        previous_volume = df['æˆäº¤é‡'].values[-2]
        # ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
        if not isinstance(current_volume, (int, float)) or not isinstance(previous_volume, (int, float)):
            try:
                current_volume = float(current_volume)
                previous_volume = float(previous_volume)
            except:
                logger.warning("æˆäº¤é‡æ•°æ®æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
                return 0.0
        # æ£€æŸ¥NaN
        if np.isnan(current_volume) or np.isnan(previous_volume) or previous_volume <= 0:
            return 0.0
        # è®¡ç®—å˜åŒ–ç‡
        volume_change = (current_volume - previous_volume) / previous_volume
        return volume_change
    except Exception as e:
        logger.error(f"è®¡ç®—æˆäº¤é‡å˜åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def calculate_loss_percentage(df: pd.DataFrame) -> float:
    """è®¡ç®—å½“å‰äºæŸæ¯”ä¾‹ï¼ˆç›¸å¯¹äºæœ€è¿‘ä¸€æ¬¡ä¹°å…¥ç‚¹ï¼‰"""
    if len(df) < 2:
        return 0.0
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    # è®¡ç®—å‡çº¿ï¼ˆä½¿ç”¨ä¸ä¸»è®¡ç®—ç›¸åŒçš„é€»è¾‘ï¼‰
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS, min_periods=1).mean().values
    # ä»æœ€æ–°æ—¥æœŸå¼€å§‹å‘å‰æ£€æŸ¥ï¼Œæ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ç«™ä¸Šå‡çº¿çš„ç‚¹
    buy_index = -1
    for i in range(len(close_prices)-1, -1, -1):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1:
            # ä½¿ç”¨å½“å‰è®¡ç®—çš„å‡çº¿å€¼
            if i < len(ma_values) and not np.isnan(ma_values[i]) and close_prices[i] >= ma_values[i]:
                buy_index = i
                break
        else:
            # ä½¿ç”¨è®¡ç®—å‡ºçš„å‡çº¿å€¼
            if not np.isnan(close_prices[i]) and not np.isnan(ma_values[i]) and close_prices[i] >= ma_values[i]:
                buy_index = i
                break
    # å¦‚æœæ‰¾ä¸åˆ°ä¹°å…¥ç‚¹ï¼Œä½¿ç”¨30å¤©å‰ä½œä¸ºå‚è€ƒ
    if buy_index == -1:
        buy_index = max(0, len(close_prices) - 30)
    current_price = close_prices[-1]
    buy_price = close_prices[buy_index]
    # ç¡®ä¿æ˜¯æœ‰æ•ˆæ•°å€¼
    if np.isnan(current_price) or np.isnan(buy_price) or buy_price <= 0:
        return 0.0
    loss_percentage = (current_price - buy_price) / buy_price * 100
    return loss_percentage

def is_in_volatile_market(df: pd.DataFrame) -> tuple:
    """åˆ¤æ–­æ˜¯å¦å¤„äºéœ‡è¡å¸‚"""
    if len(df) < 10:
        return False, 0, (0, 0)  # ä¸­æ–‡åç§°é€šå¸¸2-4ä¸ªå­—
    # è·å–æ”¶ç›˜ä»·å’Œå‡çº¿åºåˆ—
    close_prices = df["æ”¶ç›˜"].values
    # è®¡ç®—å‡çº¿ï¼ˆä½¿ç”¨ä¸ä¸»è®¡ç®—ç›¸åŒçš„é€»è¾‘ï¼‰
    ma_values = df["æ”¶ç›˜"].rolling(window=CRITICAL_VALUE_DAYS, min_periods=1).mean().values
    # æ£€æŸ¥æ˜¯å¦è¿ç»­10å¤©åœ¨å‡çº¿é™„è¿‘æ³¢åŠ¨ï¼ˆ-5%~+5%ï¼‰
    last_10_days = df.tail(10)
    deviations = []
    for i in range(len(last_10_days)):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1:
            # ä½¿ç”¨å½“å‰è®¡ç®—çš„å‡çº¿å€¼
            if i < len(ma_values) and not np.isnan(ma_values[i]):
                deviation = (close_prices[i] - ma_values[i]) / ma_values[i] * 100
                if not np.isnan(deviation) and abs(deviation) <= 5.0:
                    deviations.append(deviation)
                else:
                    return False, 0, (0, 0)
        else:
            # ä½¿ç”¨è®¡ç®—å‡ºçš„å‡çº¿å€¼
            if not np.isnan(close_prices[i]) and not np.isnan(ma_values[i]):
                deviation = (close_prices[i] - ma_values[i]) / ma_values[i] * 100
                if not np.isnan(deviation) and abs(deviation) <= 5.0:
                    deviations.append(deviation)
                else:
                    return False, 0, (0, 0)
    # æ£€æŸ¥ä»·æ ¼æ˜¯å¦åå¤ç©¿è¶Šå‡çº¿
    cross_count = 0
    for i in range(len(close_prices)-10, len(close_prices)-1):
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—å‡çº¿
        if i < CRITICAL_VALUE_DAYS - 1:
            # ä½¿ç”¨å½“å‰è®¡ç®—çš„å‡çº¿å€¼
            if i < len(ma_values) and i+1 < len(ma_values) and not np.isnan(ma_values[i]) and not np.isnan(ma_values[i+1]):
                if (close_prices[i] >= ma_values[i] and close_prices[i+1] < ma_values[i+1]) or \
                   (close_prices[i] < ma_values[i] and close_prices[i+1] >= ma_values[i+1]):
                    cross_count += 1
            else:
                continue
        else:
            # ä½¿ç”¨è®¡ç®—å‡ºçš„å‡çº¿å€¼
            if not np.isnan(close_prices[i]) and not np.isnan(close_prices[i+1]) and \
               not np.isnan(ma_values[i]) and not np.isnan(ma_values[i+1]):
                if (close_prices[i] >= ma_values[i] and close_prices[i+1] < ma_values[i+1]) or \
                   (close_prices[i] < ma_values[i] and close_prices[i+1] >= ma_values[i+1]):
                    cross_count += 1
            else:
                continue
    # è‡³å°‘éœ€è¦5æ¬¡ç©¿è¶Šæ‰è®¤å®šä¸ºéœ‡è¡å¸‚
    min_cross_count = 5
    is_volatile = cross_count >= min_cross_count
    # è®¡ç®—æœ€è¿‘10å¤©åç¦»ç‡èŒƒå›´
    if deviations:
        min_deviation = min(deviations)
        max_deviation = max(deviations)
    else:
        min_deviation = 0
        max_deviation = 0
    return is_volatile, cross_count, (min_deviation, max_deviation)

def detect_head_and_shoulders(df: pd.DataFrame) -> dict:
    """æ£€æµ‹Må¤´å’Œå¤´è‚©é¡¶å½¢æ€"""
    if len(df) < 20:  # éœ€è¦è¶³å¤Ÿæ•°æ®
        return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": []}
    # è·å–æ”¶ç›˜ä»·
    close_prices = df["æ”¶ç›˜"].values
    # å¯»æ‰¾å±€éƒ¨é«˜ç‚¹
    peaks = []
    for i in range(5, len(close_prices)-5):
        # ç¡®ä¿æ˜¯æœ‰æ•ˆæ•°å€¼
        if np.isnan(close_prices[i]) or i - 5 < 0 or i + 6 > len(close_prices):
            continue
        # æ£€æŸ¥æ˜¯å¦ä¸ºå±€éƒ¨é«˜ç‚¹
        is_peak = True
        for j in range(i-5, i):
            if j < 0 or np.isnan(close_prices[j]):
                continue
            if close_prices[i] <= close_prices[j]:
                is_peak = False
                break
        if not is_peak:
            continue
        for j in range(i+1, i+6):
            if j >= len(close_prices) or np.isnan(close_prices[j]):
                continue
            if close_prices[i] <= close_prices[j]:
                is_peak = False
                break
        if is_peak:
            peaks.append((i, close_prices[i]))
    # å¦‚æœæ‰¾åˆ°çš„é«˜ç‚¹å°‘äº3ä¸ªï¼Œæ— æ³•å½¢æˆå¤´è‚©é¡¶
    if len(peaks) < 3:
        return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
    # æ£€æµ‹Må¤´ï¼ˆä¸¤ä¸ªé«˜ç‚¹ï¼‰
    m_top_detected = False
    m_top_confidence = 0.0
    if len(peaks) >= 2:
        # ä¸¤ä¸ªé«˜ç‚¹ï¼Œç¬¬äºŒä¸ªç•¥ä½äºç¬¬ä¸€ä¸ªï¼Œä¸­é—´æœ‰æ˜æ˜¾ä½ç‚¹
        peak1_idx, peak1_price = peaks[-2]
        peak2_idx, peak2_price = peaks[-1]
        # æ£€æŸ¥ç¬¬äºŒä¸ªé«˜ç‚¹æ˜¯å¦ä½äºç¬¬ä¸€ä¸ª
        if peak2_price < peak1_price and peak2_price > peak1_price * 0.95:
            # æ£€æŸ¥ä¸­é—´æ˜¯å¦æœ‰æ˜æ˜¾ä½ç‚¹
            if peak1_idx >= len(close_prices) or peak2_idx >= len(close_prices):
                return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
            trough_idx = peak1_idx + np.argmin(close_prices[peak1_idx:peak2_idx])
            if trough_idx >= len(close_prices):
                return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
            trough_price = close_prices[trough_idx]
            # æ£€æŸ¥ä½ç‚¹æ˜¯å¦æ˜æ˜¾
            if trough_price < peak1_price * 0.97 and trough_price < peak2_price * 0.97:
                m_top_detected = True
                # è®¡ç®—ç½®ä¿¡åº¦
                price_diff = (peak1_price - peak2_price) / peak1_price
                trough_depth = (peak1_price - trough_price) / peak1_price
                m_top_confidence = 0.5 + 0.5 * min(price_diff / 0.05, 1) + 0.5 * min(trough_depth / 0.05, 1)
                m_top_confidence = min(m_top_confidence, 1.0)
    # æ£€æµ‹å¤´è‚©é¡¶ï¼ˆä¸‰ä¸ªé«˜ç‚¹ï¼‰
    head_and_shoulders_confidence = 0.0
    head_and_shoulders_detected = False
    if len(peaks) >= 3:
        # ä¸‰ä¸ªé«˜ç‚¹ï¼Œä¸­é—´æœ€é«˜ï¼Œä¸¤ä¾§è¾ƒä½
        shoulder1_idx, shoulder1_price = peaks[-3]
        head_idx, head_price = peaks[-2]
        shoulder2_idx, shoulder2_price = peaks[-1]
        # æ£€æŸ¥ä¸­é—´æ˜¯å¦ä¸ºæœ€é«˜ç‚¹
        if head_price > shoulder1_price and head_price > shoulder2_price:
            # æ£€æŸ¥ä¸¤ä¾§è‚©è†€æ˜¯å¦å¤§è‡´å¯¹ç§°
            shoulder_similarity = min(shoulder1_price, shoulder2_price) / max(shoulder1_price, shoulder2_price)
            # æ£€æŸ¥ä¸­é—´ä½ç‚¹
            if shoulder1_idx >= len(close_prices) or head_idx >= len(close_prices) or shoulder2_idx >= len(close_prices):
                return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
            trough1_idx = shoulder1_idx + np.argmin(close_prices[shoulder1_idx:head_idx])
            trough2_idx = head_idx + np.argmin(close_prices[head_idx:shoulder2_idx])
            if trough1_idx >= len(close_prices) or trough2_idx >= len(close_prices):
                return {"pattern_type": "æ— ", "detected": False, "confidence": 0, "peaks": peaks}
            neckline_price = (close_prices[trough1_idx] + close_prices[trough2_idx]) / 2
            # æ£€æŸ¥å¤´è‚©æ¯”ä¾‹æ˜¯å¦åˆç†
            if shoulder_similarity > 0.85 and head_price > neckline_price * 1.1:
                head_and_shoulders_detected = True
                # è®¡ç®—ç½®ä¿¡åº¦
                shoulder_diff = 1 - shoulder_similarity
                head_height = (head_price - neckline_price) / neckline_price
                head_and_shoulders_confidence = 0.5 + 0.3 * min(shoulder_diff / 0.15, 1) + 0.2 * min(head_height / 0.15, 1)
                head_and_shoulders_confidence = min(head_and_shoulders_confidence, 1.0)
    # ç¡®å®šä¸»è¦æ£€æµ‹ç»“æœ
    if head_and_shoulders_detected and head_and_shoulders_confidence > m_top_confidence:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿confidenceæ˜¯æ ‡é‡å€¼
        return {
            "pattern_type": "å¤´è‚©é¡¶",
            "detected": True,
            "confidence": float(head_and_shoulders_confidence),
            "peaks": peaks[-3:] if len(peaks) >= 3 else peaks
        }
    elif m_top_detected:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿confidenceæ˜¯æ ‡é‡å€¼
        return {
            "pattern_type": "Må¤´",
            "detected": True,
            "confidence": float(m_top_confidence),
            "peaks": peaks[-2:]
        }
    else:
        return {
            "pattern_type": "æ— ",
            "detected": False,
            "confidence": 0.0,
            "peaks": peaks[-3:] if len(peaks) >= 3 else peaks
        }

def generate_signal_message(index_info: dict, df: pd.DataFrame, current: float, critical: float, deviation: float) -> str:
    """ç”Ÿæˆç­–ç•¥ä¿¡å·æ¶ˆæ¯"""
    # è®¡ç®—è¿ç»­ç«™ä¸Š/è·Œç ´å‡çº¿çš„å¤©æ•°
    consecutive_above = calculate_consecutive_days_above(df, critical)
    consecutive_below = calculate_consecutive_days_below(df, critical)
    # è®¡ç®—æˆäº¤é‡å˜åŒ–
    volume_change = calculate_volume_change(df)
    # æ£€æµ‹Må¤´/å¤´è‚©é¡¶å½¢æ€
    pattern_detection = detect_head_and_shoulders(df)
    # 3. éœ‡è¡å¸‚åˆ¤æ–­ - ä¼˜å…ˆçº§æœ€é«˜
    is_volatile, cross_count, (min_dev, max_dev) = is_in_volatile_market(df)
    if is_volatile:
        # è®¡ç®—ä¸Šè½¨å’Œä¸‹è½¨ä»·æ ¼
        upper_band = critical * (1 + max_dev/100)
        lower_band = critical * (1 + min_dev/100)
        return (
            f"ã€éœ‡è¡å¸‚ã€‘è¿ç»­10æ—¥ä»·æ ¼åå¤ç©¿å‡çº¿ï¼ˆç©¿è¶Š{cross_count}æ¬¡ï¼‰ï¼Œåç¦»ç‡èŒƒå›´[{min_dev:.2f}%~{max_dev:.2f}%]\n"
            f"âœ… æ“ä½œå»ºè®®ï¼š\n"
            f"  â€¢ ä¸Šæ²¿æ“ä½œï¼ˆä»·æ ¼â‰ˆ{upper_band:.2f}ï¼‰ï¼šå°å¹…å‡ä»“10%-20%\n"
            f"  â€¢ ä¸‹æ²¿æ“ä½œï¼ˆä»·æ ¼â‰ˆ{lower_band:.2f}ï¼‰ï¼šå°å¹…åŠ ä»“10%-20%\n"
            f"  â€¢ æ€»ä»“ä½ä¸¥æ ¼æ§åˆ¶åœ¨â‰¤50%\n"
            f"âš ï¸ é¿å…é¢‘ç¹äº¤æ˜“ï¼Œç­‰å¾…è¶‹åŠ¿æ˜æœ—"
        )
    # 1. YESä¿¡å·ï¼šå½“å‰ä»·æ ¼ â‰¥ 20æ—¥å‡çº¿
    if current >= critical:
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡çªç ´ï¼ˆä»·æ ¼åˆšç«™ä¸Šå‡çº¿ï¼Œè¿ç»­2-3æ—¥ç«™ç¨³+æˆäº¤é‡æ”¾å¤§20%+ï¼‰
        if consecutive_above == 1 and volume_change > 0.2:
            return "\n".join(SCENARIO_MESSAGES_DICT["YES"]["initial_breakout"]).format(
                consecutive=consecutive_above,
                volume=volume_change*100,
                etf_code=index_info['etfs'][0]['code'],
                target_price=current * 0.99
            )
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡çªç ´ï¼ˆä»·æ ¼åˆšç«™ä¸Šå‡çº¿ï¼Œè¿ç»­2-3æ—¥ç«™ç¨³+æˆäº¤é‡æ”¾å¤§20%+ï¼‰
        elif 2 <= consecutive_above <= 3 and volume_change > 0.2:
            return "\n".join(SCENARIO_MESSAGES_DICT["YES"]["confirmed_breakout"]).format(
                consecutive=consecutive_above,
                volume=volume_change*100,
                etf_code=index_info['etfs'][0]['code'],
                target_price=current * 0.99
            )
        # å­æ¡ä»¶2ï¼šæŒç»­ç«™ç¨³ï¼ˆä»·æ ¼ç»´æŒåœ¨å‡çº¿ä¸Šï¼‰
        else:
            # åœºæ™¯Aï¼šåç¦»ç‡â‰¤+5%ï¼ˆè¶‹åŠ¿ç¨³å¥ï¼‰
            if deviation <= 5.0:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿confidenceæ˜¯æ ‡é‡å€¼
                    confidence = float(confidence) if isinstance(confidence, (np.ndarray, np.float32)) else confidence
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“10%-15%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“5%-10%"
                return "\n".join(SCENARIO_MESSAGES_DICT["YES"]["trend_stable"]).format(
                    consecutive=consecutive_above,
                    deviation=deviation,
                    target_price=current * 0.99,
                    etf_code=index_info['etfs'][0]['code'],
                    pattern_msg=pattern_msg
                )
            # åœºæ™¯Bï¼š+5%ï¼œåç¦»ç‡â‰¤+10%ï¼ˆè¶‹åŠ¿è¾ƒå¼ºï¼‰
            elif 5.0 < deviation <= 10.0:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿confidenceæ˜¯æ ‡é‡å€¼
                    confidence = float(confidence) if isinstance(confidence, (np.ndarray, np.float32)) else confidence
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œç«‹å³å‡ä»“10%-15%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“5%-10%"
                return "\n".join(SCENARIO_MESSAGES_DICT["YES"]["trend_strong"]).format(
                    consecutive=consecutive_above,
                    deviation=deviation,
                    target_price=current * 0.99,
                    etf_code=index_info['etfs'][0]['code'],
                    pattern_msg=pattern_msg
                )
            # åœºæ™¯Cï¼šåç¦»ç‡ï¼+10%ï¼ˆè¶…ä¹°é£é™©ï¼‰
            else:
                # æ·»åŠ Må¤´/å¤´è‚©é¡¶å½¢æ€æ£€æµ‹
                pattern_msg = ""
                if pattern_detection["detected"]:
                    pattern_name = pattern_detection["pattern_type"]
                    confidence = pattern_detection["confidence"]
                    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿confidenceæ˜¯æ ‡é‡å€¼
                    confidence = float(confidence) if isinstance(confidence, (np.ndarray, np.float32)) else confidence
                    if confidence >= PATTERN_CONFIDENCE_THRESHOLD:
                        pattern_msg = f"ã€é‡è¦ã€‘{pattern_name}å½¢æ€å·²ç¡®è®¤ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œç«‹å³å‡ä»“20%-30%"
                    elif confidence >= 0.5:
                        pattern_msg = f"ã€è­¦å‘Šã€‘ç–‘ä¼¼{pattern_name}å½¢æ€ï¼ˆç½®ä¿¡åº¦{confidence:.0%}ï¼‰ï¼Œå»ºè®®å‡ä»“15%-25%"
                return "\n".join(SCENARIO_MESSAGES_DICT["YES"]["overbought"]).format(
                    consecutive=consecutive_above,
                    deviation=deviation,
                    target_price=critical * 1.05,
                    etf_code=index_info['etfs'][0]['code'],
                    pattern_msg=pattern_msg
                )
    # 2. NOä¿¡å·ï¼šå½“å‰ä»·æ ¼ ï¼œ 20æ—¥å‡çº¿
    else:
        # è®¡ç®—äºæŸæ¯”ä¾‹
        loss_percentage = calculate_loss_percentage(df)
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡è·Œç ´ï¼ˆä»·æ ¼åˆšè·Œç©¿å‡çº¿ï¼Œè¿ç»­1-2æ—¥æœªæ”¶å›+æˆäº¤é‡æ”¾å¤§ï¼‰
        if consecutive_below == 1 and volume_change > 0.2:
            if loss_percentage > -15.0:  # äºæŸ<15%
                return "\n".join(SCENARIO_MESSAGES_DICT["NO"]["initial_breakdown"]).format(
                    consecutive=consecutive_below,
                    volume=volume_change*100,
                    etf_code=index_info['etfs'][0]['code'],
                    target_price=critical * 1.05
                )
            else:  # äºæŸâ‰¥15%
                return (
                    f"ã€é¦–æ¬¡è·Œç ´-ä¸¥é‡äºæŸã€‘è¿ç»­{consecutive_below}å¤©è·Œç ´20æ—¥å‡çº¿ï¼Œæˆäº¤é‡æ”¾å¤§{volume_change*100:.1f}%ï¼ŒäºæŸ{loss_percentage:.2f}%\n"
                    f"âœ… æ“ä½œå»ºè®®ï¼š\n"
                    f"  â€¢ æ ¸å¿ƒå®½åŸºETFï¼ˆ{index_info['etfs'][0]['code']}ï¼‰ç«‹å³æ¸…ä»“\n"
                    f"  â€¢ å«æ˜Ÿè¡Œä¸šETFä¿ç•™20%-30%åº•ä»“è§‚å¯Ÿ\n"
                    f"  â€¢ ä¸¥æ ¼æ­¢æŸï¼šæ”¶ç›˜ä»·ç«™ä¸Š20æ—¥å‡çº¿æ‰è€ƒè™‘å›è¡¥\n"
                    f"âš ï¸ é‡å¤§äºæŸä¿¡å·ï¼Œé¿å…ç›²ç›®æŠ„åº•"
                )
        # å­æ¡ä»¶1ï¼šé¦–æ¬¡è·Œç ´ï¼ˆä»·æ ¼åˆšè·Œç©¿å‡çº¿ï¼Œè¿ç»­1-2æ—¥æœªæ”¶å›+æˆäº¤é‡æ”¾å¤§ï¼‰
        elif consecutive_below == 2 and volume_change > 0.2:
            return "\n".join(SCENARIO_MESSAGES_DICT["NO"]["confirmed_breakdown"]).format(
                consecutive=consecutive_below,
                volume=volume_change*100,
                etf_code=index_info['etfs'][0]['code'],
                target_price=critical * 0.95
            )
        # å­æ¡ä»¶2ï¼šæŒç»­è·Œç ´ï¼ˆä»·æ ¼ç»´æŒåœ¨å‡çº¿ä¸‹ï¼‰
        else:
            # åœºæ™¯Aï¼šåç¦»ç‡â‰¥-5%ï¼ˆä¸‹è·ŒåˆæœŸï¼‰
            if deviation >= -5.0:
                return "\n".join(SCENARIO_MESSAGES_DICT["NO"]["decline_initial"]).format(
                    consecutive=consecutive_below,
                    deviation=deviation,
                    target_price=critical
                )
            # åœºæ™¯Bï¼š-10%â‰¤åç¦»ç‡ï¼œ-5%ï¼ˆä¸‹è·Œä¸­æœŸï¼‰
            elif -10.0 <= deviation < -5.0:
                return "\n".join(SCENARIO_MESSAGES_DICT["NO"]["decline_medium"]).format(
                    consecutive=consecutive_below,
                    deviation=deviation,
                    etf_code=index_info['etfs'][0]['code']
                )
            # åœºæ™¯Cï¼šåç¦»ç‡ï¼œ-10%ï¼ˆè¶…å–æœºä¼šï¼‰
            else:
                return "\n".join(SCENARIO_MESSAGES_DICT["NO"]["oversold"]).format(
                    consecutive=consecutive_below,
                    deviation=deviation,
                    target_price=critical * 0.95,
                    etf_code=index_info['etfs'][0]['code']
                )

def generate_report():
    """ç”Ÿæˆç­–ç•¥æŠ¥å‘Šå¹¶æ¨é€å¾®ä¿¡"""
    try:
        # ç»Ÿè®¡ä¿¡æ¯
        total_indices = len(INDICES)
        disabled_indices = sum(1 for idx in INDICES if idx.get("switch", 1) == 2)
        enabled_indices = total_indices - disabled_indices
        logger.info(f"å…±è®¾è®¡{total_indices}ä¸ªæŒ‡æ•°ï¼Œå…¶ä¸­{disabled_indices}ä¸ªæŒ‡æ•°æš‚åœè®¡ç®—ï¼Œæœ¬æ¬¡è®¡ç®—{enabled_indices}ä¸ªæŒ‡æ•°")
        
        # ç™»å½•baostockï¼ˆä¸€æ¬¡ï¼‰
        login_result = bs.login()
        if login_result.error_code != '0':
            logger.error(f"baostockç™»å½•å¤±è´¥: {login_result.error_msg}")
        else:
            logger.info("baostockç™»å½•æˆåŠŸ")
        
        beijing_time = get_beijing_time()
        summary_lines = []
        valid_indices_count = 0
        disabled_messages = []
        
        # æŒ‰é…ç½®é¡ºåºå¤„ç†
        for idx in INDICES:
            code = idx["code"]
            name = idx["name"]
            preferred_source = idx["source"]
            
            # å¤„ç†å¼€å…³ä¸º2çš„æŒ‡æ•°
            if idx.get("switch", 1) == 2:
                logger.info(f"è·³è¿‡å¼€å…³ä¸º2çš„æŒ‡æ•°: {name}({code})")
                etf_list = [f"{etf['code']}({etf['description']})" for etf in idx["etfs"]]
                etf_str = "ï¼Œ".join(etf_list)
                disabled_message = f"{name} \nã€{code}ï¼›ETFï¼š{etf_str}ã€‘\næœ¬æŒ‡æ•°å·²æš‚æ—¶å±è”½ï¼Œä¸ä½œä»»ä½•YES/NOè®¡ç®—"
                disabled_messages.append(disabled_message)
                # å‘é€å•ç‹¬çš„å±è”½æ¶ˆæ¯
                send_wechat_message(disabled_message)
                time.sleep(1)
                continue
                
            # ä½¿ç”¨æ™ºèƒ½æ•°æ®è·å–å‡½æ•°
            logger.info(f"ä¸ºæŒ‡æ•° {name}({code}) å°è¯•é¦–é€‰æ•°æ®æº: {preferred_source}")
            df, actual_source = fetch_index_data_smart(idx)
            
            if df.empty:
                logger.warning(f"æ— æ•°æ®: {name}({code})")
                # æ•°æ®è·å–å¤±è´¥çš„æ¶ˆæ¯
                etf_list = [f"{etf['code']}({etf['description']})" for etf in idx["etfs"]]
                etf_str = "ï¼Œ".join(etf_list)
                message_lines = [
                    f"{name}ã€{code}ã€‘\n",
                    f"ETFæ ‡çš„ï¼š{etf_str}\n",
                    f"ğŸ“Š å½“å‰ï¼šæ•°æ®è·å–å¤±è´¥ | ä¸´ç•Œï¼šN/A | åç¦»ï¼šN/A\n",
                    f"æ•°æ®è·å–å¤±è´¥\n",
                    "â”â”â”â”â”â”â”â”â”â”\n"
                ]
                message = "\n".join(message_lines)
                logger.info(f"æ¨é€ {name} Yes/noä¿¡å·ï¼ˆæ•°æ®è·å–å¤±è´¥ï¼‰")
                send_wechat_message(message)
                time.sleep(1)
                continue

            # æ•°æ®é‡æ£€æŸ¥
            if len(df) < CRITICAL_VALUE_DAYS:
                logger.warning(f"æŒ‡æ•° {name}({code}) æ—¥çº¿å¤©æ•°ä¸è¶³{CRITICAL_VALUE_DAYS}å¤©ï¼Œè·³è¿‡è®¡ç®—")
                etf_list = [f"{etf['code']}({etf['description']})" for etf in idx["etfs"]]
                etf_str = "ï¼Œ".join(etf_list)
                message_lines = [
                    f"{name}ã€{code}ã€‘\n",
                    f"ETFæ ‡çš„ï¼š{etf_str}\n",
                    f"ğŸ“Š å½“å‰ï¼šæ•°æ®ä¸è¶³ | ä¸´ç•Œï¼šN/A | åç¦»ï¼šN/A\n",
                    f"âš ï¸ è¯¥æŒ‡æ•°çš„æ—¥çº¿æ•°æ®ä¸è¶³\n",
                    "â”â”â”â”â”â”â”â”â”â”\n",
                    f"âš ï¸ éœ€è¦è‡³å°‘{CRITICAL_VALUE_DAYS}å¤©æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œå½“å‰åªæœ‰{len(df)}å¤©\n",
                    "â”â”â”â”â”â”â”â”â”â”\n"
                ]
                message = "\n".join(message_lines)
                logger.info(f"æ¨é€ {name} Yes/noä¿¡å·ï¼ˆæ—¥çº¿å¤©æ•°ä¸è¶³ï¼‰")
                send_wechat_message(message)
                time.sleep(2)
                continue

            # æ­£å¸¸è®¡ç®—é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
            close_price = df['æ”¶ç›˜'].values[-1]
            critical_value = calculate_critical_value(df)
            
            # ç¡®ä¿æ˜¯æ ‡é‡å€¼
            if isinstance(critical_value, pd.Series):
                critical_value = critical_value.values[-1]
            elif isinstance(critical_value, pd.DataFrame):
                critical_value = critical_value.iloc[-1, 0]
                
            try:
                close_price = float(close_price)
                critical_value = float(critical_value)
            except (TypeError, ValueError) as e:
                logger.error(f"è½¬æ¢ä»·æ ¼å€¼å¤±è´¥: {str(e)}")
                continue

            # è®¡ç®—åç¦»ç‡å’Œä¿¡å·
            deviation = calculate_deviation(close_price, critical_value)
            status = "YES" if close_price >= critical_value else "NO"
            signal_message = generate_signal_message(idx, df, close_price, critical_value, deviation)
            
            # æ„å»ºæ¨é€æ¶ˆæ¯
            etf_list = [f"{etf['code']}({etf['description']})" for etf in idx["etfs"]]
            etf_str = "ï¼Œ".join(etf_list)
            signal_symbol = "âœ…" if status == "YES" else "âŒ"
            
            message_lines = [
                f"{name}ã€{code}ã€‘",
                f"ETFæ ‡çš„ï¼š{etf_str}",
                f"ğŸ“Š å½“å‰ä»·ï¼š{close_price:.2f}",
                f"ğŸ“Š ä¸´ç•Œå€¼ï¼š{critical_value:.2f}",
                f"ğŸ“Š åç¦»ç‡ï¼š{deviation:.2f}%",
                f"ä¿¡å·ï¼š{signal_symbol} {status}",
                f"{signal_message}"
            ]
            message = "\n".join(message_lines)
            
            logger.info(f"æ¨é€ {name} æŒ‡æ•°yes/noä¿¡å·ï¼ˆä½¿ç”¨æ•°æ®æº: {actual_source}ï¼‰")
            send_wechat_message(message)
            
            # æ·»åŠ åˆ°æ€»ç»“
            name_padding = 10 if len(name) <= 4 else 8
            name_with_padding = f"{name}{' ' * (name_padding - len(name))}"
            summary_line = f"{name_with_padding}ã€{code}ã€‘\nETFæ ‡çš„ï¼š{etf_str}\nä¿¡å·ï¼š{signal_symbol} {status}\nå½“å‰ä»·ï¼š{close_price:.2f}\nä¸´ç•Œå€¼ï¼š{critical_value:.2f}\nåç¦»ç‡ï¼š{deviation:.2f}%\nâ”â”â”â”â”â”â”â”â”â”\n"
            summary_lines.append(summary_line)
            valid_indices_count += 1
            time.sleep(1)

        # é€€å‡ºbaostock
        bs.logout()
        logger.info("baostockå·²é€€å‡º")
        

        # æ„å»ºæ€»ç»“æ¶ˆæ¯ - æŒ‰åºå·èŒƒå›´åˆ†ç»„
        def extract_index_number(line):
            """ä»summary_lineä¸­æå–åºå·"""
            # lineçš„ç¬¬ä¸€è¡Œæ ¼å¼ä¸º"1ã€ä¼¦æ•¦é‡‘ç°(XAU)ã€GC=Fã€‘"
            first_line = line.split('\n')[0]
            # æå–åºå·ï¼Œå¦‚"1"
            import re
            match = re.search(r'^(\d+)', first_line)
            if match:
                return int(match.group(1))
            return 0

        # åˆ†ç»„è§„åˆ™ï¼š1-19, 20-30, 31-41, ...
        def get_group_number(index_num):
            if index_num <= 19:
                return 1
            else:
                return (index_num - 20) // 11 + 2

        # æå–æ¯ä¸ªæŒ‡æ•°çš„åºå·å¹¶åˆ†ç»„
        summary_groups = {}
        for line in summary_lines:
            index_num = extract_index_number(line)
            if index_num == 0:
                continue  # æ— æ³•æå–åºå·ï¼Œè·³è¿‡
            group_num = get_group_number(index_num)
            if group_num not in summary_groups:
                summary_groups[group_num] = []
            summary_groups[group_num].append(line)

        # å‘é€åˆ†ç»„æ¶ˆæ¯
        for group_num, group_lines in sorted(summary_groups.items()):
            # ç¡®å®šåºå·èŒƒå›´
            if group_num == 1:
                range_str = "1-19"
            else:
                start = 20 + (group_num - 2) * 11
                end = start + 10
                range_str = f"{start}-{end}"
    
            # æ„å»ºåˆ†ç»„æ¶ˆæ¯
            group_message_lines = []
            group_message_lines.append(f"\n====æŒ‡æ•°ä¿¡å·æ€»ç»“ ({range_str})====\n")
            group_message_lines.extend(group_lines)
    
            group_message = "".join(group_message_lines)
            logger.info(f"æ¨é€æ€»ç»“æ¶ˆæ¯ - {range_str}ç»„")
            send_wechat_message(group_message)
            time.sleep(1)
        
        # æ„å»ºæ€»ç»“æ¶ˆæ¯
        #final_summary_lines = []
        
        # æ·»åŠ å±è”½æŒ‡æ•°çš„ä¿¡æ¯
        #if disabled_messages:
        #    final_summary_lines.append("ã€å·²å±è”½æŒ‡æ•°ã€‘\n")
        #    for msg in disabled_messages:
        #        final_summary_lines.append(f"ğŸ”‡ {msg}\n")
        #    final_summary_lines.append("\n")
        
        # æ·»åŠ æ­£å¸¸è®¡ç®—çš„æŒ‡æ•°ä¿¡æ¯
        #if summary_lines:
        #    final_summary_lines.append("\n====æ‰€æœ‰æŒ‡æ•°ä¿¡å·æ€»ç»“====\n\n")
        #    final_summary_lines.extend(summary_lines)
        
        # å¦‚æœæœ‰ä»»ä½•æŒ‡æ•°ä¿¡æ¯ï¼Œå‘é€æ€»ç»“æ¶ˆæ¯
        #if final_summary_lines:
        #    summary_message = "".join(final_summary_lines)
        #    logger.info("æ¨é€æ€»ç»“æ¶ˆæ¯")
        #    send_wechat_message(summary_message)
        #    time.sleep(1)
            
        logger.info(f"æ‰€æœ‰æŒ‡æ•°ç­–ç•¥æŠ¥å‘Šå·²æˆåŠŸå‘é€è‡³ä¼ä¸šå¾®ä¿¡ï¼ˆå…±{valid_indices_count}ä¸ªæœ‰æ•ˆæŒ‡æ•°ï¼Œ{len(disabled_messages)}ä¸ªå±è”½æŒ‡æ•°ï¼‰")
        
    except Exception as e:
        logger.error(f"ç­–ç•¥æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # ä¿®æ­£ï¼šé”™è¯¯æ¶ˆæ¯ä¸æ­£å¸¸ä¿¡å·æ¶ˆæ¯åˆ†ç¦»
        try:
            send_wechat_message(f"ğŸš¨ ã€é”™è¯¯é€šçŸ¥ã€‘ç­–ç•¥æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        except Exception as wechat_error:
            logger.error(f"å‘é€å¾®ä¿¡æ¶ˆæ¯å¤±è´¥: {str(wechat_error)}", exc_info=True)

if __name__ == "__main__":
    logger.info("==== å¼€å§‹æ‰§è¡Œ æŒ‡æ•°Yes/Noç­–ç•¥ ====")
    # æ·»åŠ å»¶æ—¶
    time.sleep(30)
    generate_report()
    logger.info("=== æŒ‡æ•°Yes/Noç­–ç•¥æ‰§è¡Œå®Œæˆ ===")
