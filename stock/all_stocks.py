#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨åˆ—è¡¨æ›´æ–°æ¨¡å— - Baostock æ•°æ®æºï¼ˆä¸¥æ ¼å•æ¥å£å®ç°ï¼‰

ã€è¯¦ç»†è¿‡æ»¤æ¡ä»¶ã€‘
1. åŸºç¡€è¿‡æ»¤ï¼š
   - ç§»é™¤STå’Œ*STè‚¡ç¥¨
   - ç§»é™¤åç§°ä»¥"N"å¼€å¤´çš„æ–°ä¸Šå¸‚è‚¡ç¥¨
   - ç§»é™¤åç§°åŒ…å«"é€€å¸‚"çš„è‚¡ç¥¨
   - ç§»é™¤æŒ‡æ•°è‚¡ç¥¨ï¼ˆåœ¨æ•°æ®è·å–é˜¶æ®µå®Œæˆï¼‰
2. è´¨æŠ¼æ•°æ®è¿‡æ»¤ï¼š
   - ç§»é™¤è´¨æŠ¼è‚¡æ•°è¶…è¿‡é˜ˆå€¼çš„è‚¡ç¥¨

æ³¨æ„ï¼šä¸å†åŒ…å«å¸‚ç›ˆç‡è¿‡æ»¤ï¼Œå› ä¸ºæ–°CSVç»“æ„å·²ç§»é™¤è¯¥å­—æ®µ
"""

import os
import logging
import pandas as pd
import baostock as bs
import time
import random
import traceback
from datetime import datetime
from config import Config
from utils.date_utils import get_beijing_time
from utils.git_utils import commit_files_in_batches
import akshare as ak  # æ–°å¢ï¼šç”¨äºè·å–è´¨æŠ¼æ•°æ®

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "daily")
STOCK_DIR = os.path.join(DATA_DIR, "stock")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_stocks.csv")
TOP_500_FILE = os.path.join(STOCK_DIR, "top500stock.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(STOCK_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ä¸“ä¸šçº§é‡è¯•é…ç½®
MAX_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
BASE_RETRY_DELAY = 2  # åŸºç¡€é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_RANDOM_DELAY = 8  # æœ€å¤§éšæœºå»¶æ—¶ï¼ˆç§’ï¼‰

# è´¨æŠ¼è¿‡æ»¤å‚æ•°é…ç½®
PLEDGE_FILTER = {
    "enabled": True,
    "threshold": 100,  # é»˜è®¤ä¸ºä¸€ç™¾ï¼ˆä¸‡è‚¡â€”ï¼‰ï¼Œè¡¨ç¤ºç§»é™¤æ‰€æœ‰è´¨æŠ¼è‚¡æ•°>100ä¸‡çš„è‚¡ç¥¨
    "column": "è´¨æŠ¼è‚¡æ•°",
    "condition": "<= {threshold}ï¼ˆæ’é™¤è´¨æŠ¼è‚¡æ•°è¶…è¿‡é˜ˆå€¼çš„è‚¡ç¥¨ï¼‰"
}

def format_stock_code(code):
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½å­—ç¬¦ä¸²æ ¼å¼
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¯èƒ½åŒ…å«å‰ç¼€æˆ–é6ä½ï¼‰
    Returns:
        str: è§„èŒƒåŒ–çš„6ä½è‚¡ç¥¨ä»£ç 
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    code_str = str(code).strip().lower()
    
    # ç§»é™¤å¯èƒ½çš„å¸‚åœºå‰ç¼€
    if code_str.startswith(('sh', 'sz', 'hk', 'bj')):
        code_str = code_str[2:]
    
    # ç§»é™¤å¯èƒ½çš„ç‚¹å·ï¼ˆå¦‚"0.600022"ï¼‰
    if '.' in code_str:
        code_str = code_str.split('.')[1] if code_str.startswith('0.') else code_str
    
    # ç¡®ä¿æ˜¯6ä½æ•°å­—
    code_str = code_str.zfill(6)
    
    # éªŒè¯æ ¼å¼
    if not code_str.isdigit() or len(code_str) != 6:
        logger.warning(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {code_str}")
        return None
    
    return code_str

def is_index_stock(row):
    """
    æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦ä¸ºæŒ‡æ•°
    Args:
        row: è‚¡ç¥¨æ•°æ®è¡Œ
    Returns:
        bool: æ˜¯å¦æ˜¯æŒ‡æ•°
    """
    # æ–¹æ³•1: æ£€æŸ¥åç§°æ˜¯å¦åŒ…å«æŒ‡æ•°ç›¸å…³å…³é”®è¯
    index_keywords = ["æŒ‡æ•°", "ETF", "LOF", "åŸºé‡‘", "å€ºåˆ¸", "å›½å€º", "ä¿¡ç”¨å€º", "å¯è½¬å€º", "æœŸè´§", "æœŸæƒ", "ç†è´¢", "ç¥¨æ®"]
    if "åç§°" in row and any(keyword in str(row["åç§°"]) for keyword in index_keywords):
        return True
    
    # æ–¹æ³•2: æ£€æŸ¥è‚¡ç¥¨ç±»å‹
    if "è¯åˆ¸ç±»å‹" in row:
        # åªä¿ç•™æ™®é€šè‚¡ï¼ˆAè‚¡ï¼‰ï¼Œå…¶ä»–ç±»å‹ï¼ˆå¦‚æŒ‡æ•°ã€åŸºé‡‘ç­‰ï¼‰éƒ½è¿‡æ»¤æ‰
        # æ ¹æ®Baostockæ–‡æ¡£ï¼Œæ™®é€šè‚¡çš„typeä¸º1
        return str(row["è¯åˆ¸ç±»å‹"]) != "1"
    
    # æ–¹æ³•3: æ£€æŸ¥å¸‚åœºä»£ç 
    if "ä»£ç " in row:
        code = str(row["ä»£ç "])
        # æ’é™¤ä»¥000å¼€å¤´çš„æŒ‡æ•°ä»£ç ï¼ˆé™¤äº†000001ä¸Šè¯æŒ‡æ•°å¤–ï¼Œå…¶ä»–000å¼€å¤´çš„ä»£ç å¯èƒ½æ˜¯è‚¡ç¥¨ï¼‰
        if code.startswith("000") and code != "000001":
            # 000å¼€å¤´ä¸”ä¸æ˜¯000001çš„å¯èƒ½æ˜¯æŒ‡æ•°
            return True
    
    return False

def get_stock_section(stock_code: str) -> str:
    """
    è·å–è‚¡ç¥¨æ‰€å±æ¿å—
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå·²æ ¼å¼åŒ–ä¸º6ä½ï¼‰
    
    Returns:
        str: æ¿å—åç§°
    """
    # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½
    stock_code = format_stock_code(stock_code)
    if not stock_code:
        return "æ ¼å¼é”™è¯¯"
    
    # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€åˆ¤æ–­æ¿å—
    if stock_code.startswith('60'):
        return "æ²ªå¸‚ä¸»æ¿"
    elif stock_code.startswith('00'):
        return "æ·±å¸‚ä¸»æ¿"
    elif stock_code.startswith('30'):
        return "åˆ›ä¸šæ¿"
    elif stock_code.startswith('688'):
        return "ç§‘åˆ›æ¿"
    elif stock_code.startswith('8'):
        return "åŒ—äº¤æ‰€"
    elif stock_code.startswith('4') or stock_code.startswith('8'):
        return "ä¸‰æ¿å¸‚åœº"
    else:
        return "å…¶ä»–æ¿å—"

def save_top_500_stock_data(stock_data):
    """
    ä¿å­˜å‰500æ¡è‚¡ç¥¨æ•°æ®ç”¨äºéªŒè¯
    Args:
        stock_data: è‚¡ç¥¨æ•°æ®DataFrame
    """
    try:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿åªä¿å­˜å‰500æ¡æ•°æ®
        top_500 = stock_data.head(500).copy()
        
        # ã€å…³é”®ä¿®å¤ã€‘ä¿å­˜ä¸ºCSVæ–‡ä»¶
        top_500.to_csv(TOP_500_FILE, index=False)
        
        # ã€å…³é”®ä¿®å¤ã€‘æäº¤åˆ°Gitä»“åº“
        commit_files_in_batches(TOP_500_FILE, "ä¿å­˜å‰500æ¡è‚¡ç¥¨æ•°æ®ç”¨äºéªŒè¯")
        
        logger.info(f"å·²æˆåŠŸä¿å­˜å‰500æ¡è‚¡ç¥¨æ•°æ®åˆ° {TOP_500_FILE}")
        
        # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥"åç§°"åˆ—æ˜¯å¦å­˜åœ¨
        if "åç§°" in top_500.columns:
            logger.info(f"å‰500æ¡æ•°æ®ä¸­åŒ…å«çš„åˆ—: {', '.join(top_500.columns)}")
            logger.info(f"å‰500æ¡æ•°æ®ä¸­STè‚¡ç¥¨æ•°é‡: {top_500['åç§°'].str.contains('ST', na=False).sum()}")
            logger.info(f"å‰500æ¡æ•°æ®ä¸­é€€å¸‚è‚¡ç¥¨æ•°é‡: {top_500['åç§°'].str.contains('é€€å¸‚', na=False).sum()}")
            logger.info(f"å‰500æ¡æ•°æ®ä¸­Nå¼€å¤´è‚¡ç¥¨æ•°é‡: {top_500['åç§°'].str.startswith('N').sum()}")
        else:
            logger.warning("åˆ—'åç§°'ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒST/é€€å¸‚/Nå¼€å¤´è‚¡ç¥¨ç»Ÿè®¡")
            logger.info(f"å®é™…å¯ç”¨åˆ—: {', '.join(top_500.columns)}")
    except Exception as e:
        logger.error(f"ä¿å­˜å‰500æ¡è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def get_stock_list_data():
    """
    è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®ï¼ˆä½¿ç”¨baostockæ¥å£ï¼‰
    
    Returns:
        pd.DataFrame: è‚¡ç¥¨åˆ—è¡¨æ•°æ®
    """
    # ç™»å½•Baostock
    login_result = bs.login()
    if login_result.error_code != '0':
        logger.error(f"Baostockç™»å½•å¤±è´¥: {login_result.error_msg}")
        return pd.DataFrame()
    
    try:
        for retry in range(MAX_RETRIES):
            try:
                # ã€å…³é”®ä¿®å¤ã€‘å¤§å¹…å¢åŠ éšæœºå»¶æ—¶ï¼ˆ2.0-8.0ç§’ï¼‰- é¿å…è¢«å°
                delay = random.uniform(2.0, 8.0)
                logger.info(f"è·å–è‚¡ç¥¨åˆ—è¡¨å‰ç­‰å¾… {delay:.2f} ç§’ï¼ˆå°è¯• {retry+1}/{MAX_RETRIES}ï¼‰...")
                time.sleep(delay)
                
                logger.info("æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®...")
                
                # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨query_stock_basicæ¥å£è·å–æ•°æ®
                rs = bs.query_stock_basic()
                
                # æ£€æŸ¥è¿”å›ç»“æœ
                if rs.error_code != '0':
                    logger.error(f"APIè¿”å›é”™è¯¯: {rs.error_msg}")
                    if retry < MAX_RETRIES - 1:
                        extra_delay = retry * 10
                        total_delay = BASE_RETRY_DELAY + extra_delay
                        logger.warning(f"å°†åœ¨ {total_delay:.1f} ç§’åé‡è¯• ({retry+1}/{MAX_RETRIES})")
                        time.sleep(total_delay)
                        continue
                    return pd.DataFrame()
                
                # æ”¶é›†æ•°æ®
                data_list = []
                while rs.next():
                    data_list.append(rs.get_row_data())
                
                if not data_list:
                    logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®å¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®")
                    if retry < MAX_RETRIES - 1:
                        extra_delay = retry * 10
                        total_delay = BASE_RETRY_DELAY + extra_delay
                        logger.warning(f"å°†åœ¨ {total_delay:.1f} ç§’åé‡è¯• ({retry+1}/{MAX_RETRIES})")
                        time.sleep(total_delay)
                        continue
                    return pd.DataFrame()
                
                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(data_list, columns=rs.fields)
                
                # ã€å…³é”®ä¿®æ”¹ã€‘æ‰“å°å®é™…è¿”å›çš„å­—æ®µï¼Œç”¨äºè°ƒè¯•
                logger.info(f"Baostock query_stock_basic è¿”å›çš„å­—æ®µ: {', '.join(rs.fields)}")
                
                # ã€å…³é”®ä¿®å¤ã€‘é‡å‘½ååˆ—å
                # Baostockè¿”å›çš„åˆ—åä¸ä»£ç æœŸæœ›çš„åˆ—åä¸åŒ
                column_mapping = {
                    'code': 'ä»£ç ',
                    'code_name': 'åç§°',
                    'ipoDate': 'ä¸Šå¸‚æ—¥æœŸ',
                    'outDate': 'é€€å¸‚æ—¥æœŸ',
                    'type': 'è¯åˆ¸ç±»å‹',
                    'status': 'ä¸Šå¸‚çŠ¶æ€'
                }
                
                # ä»…ä¿ç•™å­˜åœ¨çš„åˆ—
                existing_columns = {k: v for k, v in column_mapping.items() if k in df.columns}
                df = df.rename(columns=existing_columns)
                
                # ç¡®ä¿æœ‰"åç§°"åˆ—
                if "åç§°" not in df.columns:
                    logger.error("è¿”å›æ•°æ®ä¸­ç¼ºå°‘'åç§°'åˆ—ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                    return pd.DataFrame()
                
                # ã€å…³é”®ä¿®æ”¹ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼ç»Ÿä¸€ä¸º6ä½
                if "ä»£ç " in df.columns:
                    # ä» Baostock æ ¼å¼è½¬æ¢ä¸ºçº¯æ•°å­—ä»£ç 
                    df['ä»£ç '] = df['ä»£ç '].apply(lambda x: x[3:] if x.startswith(('sh.', 'sz.')) else x)
                    df['ä»£ç '] = df['ä»£ç '].apply(format_stock_code)
                    df = df[df['ä»£ç '].notna()]
                
                # ã€å…³é”®ä¿®æ”¹ã€‘æ·»åŠ æ‰€å±æ¿å—åˆ—
                if "ä»£ç " in df.columns:
                    df['æ‰€å±æ¿å—'] = df['ä»£ç '].apply(get_stock_section)
                else:
                    df['æ‰€å±æ¿å—'] = "æœªçŸ¥æ¿å—"
                    logger.warning("ä»£ç åˆ—ä¸å­˜åœ¨ï¼Œæ‰€å±æ¿å—åˆ—å·²è®¾ä¸º'æœªçŸ¥æ¿å—'")
                
                # ã€å…³é”®ä¿®æ”¹ã€‘æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆæ ¹æ®è¦æ±‚è®¾ä¸º0æˆ–é»˜è®¤å€¼ï¼‰
                # æ³¨æ„ï¼šè¿™äº›åˆ—åœ¨Baostockæ¥å£ä¸­ä¸å­˜åœ¨ï¼Œæ ¹æ®è¦æ±‚è®¾ä¸º0
                df['æµé€šå¸‚å€¼'] = 0.0
                df['æ€»å¸‚å€¼'] = 0.0
                df['åŠ¨æ€å¸‚ç›ˆç‡'] = 0.0
                
                # ã€å…³é”®ä¿®æ”¹ã€‘ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
                required_columns = ["ä»£ç ", "åç§°", "æ‰€å±æ¿å—", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "ä¸Šå¸‚çŠ¶æ€", "åŠ¨æ€å¸‚ç›ˆç‡"]
                for col in required_columns:
                    if col not in df.columns:
                        if col in ["æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "åŠ¨æ€å¸‚ç›ˆç‡"]:
                            df[col] = 0.0
                            logger.warning(f"åˆ— '{col}' ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ 0.0")
                        else:
                            df[col] = ""
                            logger.warning(f"åˆ— '{col}' ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ç©ºå­—ç¬¦ä¸²")
                
                # ã€å…³é”®ä¿®æ”¹ã€‘ç§»é™¤æ‰€æœ‰æŒ‡æ•°è‚¡ç¥¨ï¼ˆåœ¨åŸºç¡€è¿‡æ»¤å‰ï¼‰
                original_count = len(df)
                df = df[~df.apply(is_index_stock, axis=1)]
                logger.info(f"æŒ‡æ•°è¿‡æ»¤ï¼šä» {original_count} æ¡ä¸­ç§»é™¤äº† {original_count - len(df)} æ¡æŒ‡æ•°/ETF/åŸºé‡‘/å€ºåˆ¸è‚¡ç¥¨")
                
                logger.info(f"æˆåŠŸè·å– {len(df)} æ¡è‚¡ç¥¨åˆ—è¡¨æ•°æ®ï¼ˆå·²è¿‡æ»¤æŒ‡æ•°ï¼‰")
                
                # ã€å…³é”®ä¿®å¤ã€‘ä¿å­˜å‰500æ¡æ•°æ®ç”¨äºéªŒè¯
                save_top_500_stock_data(df)
                
                return df
            
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®å¤±è´¥ (å°è¯• {retry+1}/{MAX_RETRIES}): {str(e)}", exc_info=True)
                logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                if retry < MAX_RETRIES - 1:
                    extra_delay = retry * 10
                    total_delay = BASE_RETRY_DELAY + extra_delay
                    logger.warning(f"å°†åœ¨ {total_delay:.1f} ç§’åé‡è¯• ({retry+1}/{MAX_RETRIES})")
                    time.sleep(total_delay)
    
        logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return pd.DataFrame()
    
    finally:
        # ç¡®ä¿ç™»å‡º
        bs.logout()

def apply_basic_filters(stock_data):
    """
    åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
    
    Args:
        stock_data: è‚¡ç¥¨åˆ—è¡¨DataFrame
    
    Returns:
        pd.DataFrame: åº”ç”¨åŸºç¡€è¿‡æ»¤åçš„è‚¡ç¥¨æ•°æ®
    """
    # åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    stock_info = stock_data.copy()
    
    # ã€å…³é”®ä¿®å¤ã€‘è®°å½•åˆå§‹è‚¡ç¥¨æ•°é‡
    initial_count = len(stock_info)
    logger.info(f"å¼€å§‹åº”ç”¨åŸºç¡€è¿‡æ»¤ï¼Œåˆå§‹è‚¡ç¥¨æ•°é‡: {initial_count}")
    
    # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦åˆ—
    if "åç§°" not in stock_info.columns:
        logger.error("æ•°æ®ä¸­ç¼ºå°‘'åç§°'åˆ—ï¼Œæ— æ³•åº”ç”¨è¿‡æ»¤æ¡ä»¶")
        return stock_info
    
    # ã€å…³é”®ä¿®å¤ã€‘åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
    # 1. ç§»é™¤STå’Œ*STè‚¡ç¥¨
    before = len(stock_info)
    stock_info = stock_info[~stock_info["åç§°"].str.contains("ST", na=False, regex=False)]
    removed = before - len(stock_info)
    if removed > 0:
        logger.info(f"æ’é™¤ {removed} åªSTè‚¡ç¥¨ï¼ˆåŸºç¡€è¿‡æ»¤ï¼‰")
    
    # 2. ç§»é™¤åç§°ä»¥"N"å¼€å¤´çš„æ–°ä¸Šå¸‚è‚¡ç¥¨
    before = len(stock_info)
    stock_info = stock_info[~stock_info["åç§°"].str.startswith("N")]
    removed = before - len(stock_info)
    if removed > 0:
        logger.info(f"æ’é™¤ {removed} åªæ–°ä¸Šå¸‚è‚¡ç¥¨ï¼ˆåŸºç¡€è¿‡æ»¤ï¼‰")
    
    # 3. ç§»é™¤åç§°åŒ…å«"é€€å¸‚"çš„è‚¡ç¥¨
    before = len(stock_info)
    stock_info = stock_info[~stock_info["åç§°"].str.contains("é€€å¸‚", na=False, regex=False)]
    removed = before - len(stock_info)
    if removed > 0:
        logger.info(f"æ’é™¤ {removed} åªé€€å¸‚è‚¡ç¥¨ï¼ˆåŸºç¡€è¿‡æ»¤ï¼‰")
    
    # 4. ç§»é™¤å·²é€€å¸‚è‚¡ç¥¨
    if "ä¸Šå¸‚çŠ¶æ€" in stock_info.columns:
        before = len(stock_info)
        stock_info = stock_info[stock_info["ä¸Šå¸‚çŠ¶æ€"] == "1"]
        removed = before - len(stock_info)
        if removed > 0:
            logger.info(f"æ’é™¤ {removed} åªå·²é€€å¸‚è‚¡ç¥¨ï¼ˆåŸºç¡€è¿‡æ»¤ï¼‰")
    
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç å”¯ä¸€ - ç§»é™¤é‡å¤é¡¹
    if "ä»£ç " in stock_info.columns:
        stock_info = stock_info.drop_duplicates(subset=['ä»£ç '], keep='first')
    
    # ã€å…³é”®ä¿®å¤ã€‘è®°å½•åŸºç¡€è¿‡æ»¤åè‚¡ç¥¨æ•°é‡
    logger.info(f"åŸºç¡€è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(stock_info)} æ¡è®°å½•ï¼ˆåˆå§‹: {initial_count}ï¼‰")
    
    return stock_info

def get_pledge_data():
    """
    è·å–è‚¡ç¥¨è´¨æŠ¼æ•°æ®
    è¿”å›:
        pd.DataFrame: åŒ…å«è´¨æŠ¼æ•°æ®çš„DataFrame
    """
    try:
        logger.info("æ­£åœ¨è·å–è‚¡ç¥¨è´¨æŠ¼æ•°æ®...")
        df = ak.stock_gpzy_pledge_ratio_em()
        
        if df.empty:
            logger.error("è·å–è‚¡ç¥¨è´¨æŠ¼æ•°æ®å¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®")
            return pd.DataFrame()
        
        # æ‰“å°å®é™…è¿”å›çš„åˆ—å
        logger.info(f"è´¨æŠ¼æ•°æ®å®é™…åˆ—å: {', '.join(df.columns)}")
        
        # ç¡®ä¿åˆ—åæ­£ç¡®
        required_columns = ['è‚¡ç¥¨ä»£ç ', 'è´¨æŠ¼è‚¡æ•°', 'æ— é™å”®è‚¡è´¨æŠ¼æ•°']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"è´¨æŠ¼æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {col}")
                return pd.DataFrame()
        
        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼æ­£ç¡®
        df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].apply(lambda x: str(x).zfill(6))
        
        # ç­›é€‰æœ‰æ•ˆæ•°æ®
        df = df[df['è‚¡ç¥¨ä»£ç '].apply(lambda x: len(x) == 6)]
        
        # é‡å‘½ååˆ—ï¼Œç¡®ä¿ä¸ä¸»æ•°æ®åŒ¹é…
        df = df.rename(columns={
            'è‚¡ç¥¨ä»£ç ': 'ä»£ç ',
            'è´¨æŠ¼è‚¡æ•°': 'è´¨æŠ¼è‚¡æ•°',
            'æ— é™å”®è‚¡è´¨æŠ¼æ•°': 'æ— é™å”®è‚¡è´¨æŠ¼æ•°'
        })
        
        # é€‰æ‹©éœ€è¦çš„åˆ—
        df = df[['ä»£ç ', 'è´¨æŠ¼è‚¡æ•°', 'æ— é™å”®è‚¡è´¨æŠ¼æ•°']]
        
        # å¡«å……ç¼ºå¤±å€¼
        df['è´¨æŠ¼è‚¡æ•°'] = df['è´¨æŠ¼è‚¡æ•°'].fillna(0)
        df['æ— é™å”®è‚¡è´¨æŠ¼æ•°'] = df['æ— é™å”®è‚¡è´¨æŠ¼æ•°'].fillna(0)
        
        logger.info(f"æˆåŠŸè·å– {len(df)} æ¡è‚¡ç¥¨è´¨æŠ¼æ•°æ®")
        return df
    
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨è´¨æŠ¼æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def apply_pledge_filter(stock_data):
    """
    åº”ç”¨è´¨æŠ¼æ•°æ®è¿‡æ»¤æ¡ä»¶
    
    Args:
        stock_data: åŸºç¡€è‚¡ç¥¨åˆ—è¡¨DataFrame
    
    Returns:
        pd.DataFrame: åº”ç”¨è´¨æŠ¼è¿‡æ»¤åçš„è‚¡ç¥¨æ•°æ®
    """
    # è·å–è´¨æŠ¼æ•°æ®
    pledge_data = get_pledge_data()
    if pledge_data.empty:
        logger.warning("è´¨æŠ¼æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡è´¨æŠ¼è¿‡æ»¤")
        return stock_data
    
    # åˆ›å»ºå‰¯æœ¬é¿å…SettingWithCopyWarning
    stock_info = stock_data.copy()
    
    # ä»…åœ¨æœ‰è´¨æŠ¼æ•°æ®æ—¶æ·»åŠ è´¨æŠ¼è‚¡æ•°åˆ—
    if 'è´¨æŠ¼è‚¡æ•°' not in stock_info.columns:
        # æ·»åŠ è´¨æŠ¼è‚¡æ•°åˆ—ï¼Œåˆå§‹å€¼ä¸º0
        stock_info['è´¨æŠ¼è‚¡æ•°'] = 0
    
    # ä»…åœ¨æœ‰æ— é™å”®è‚¡è´¨æŠ¼æ•°æ®æ—¶æ·»åŠ æ— é™å”®è‚¡è´¨æŠ¼æ•°åˆ—
    if 'æ— é™å”®è‚¡è´¨æŠ¼æ•°' not in stock_info.columns:
        # æ·»åŠ æ— é™å”®è‚¡è´¨æŠ¼æ•°åˆ—ï¼Œåˆå§‹å€¼ä¸º0
        stock_info['æ— é™å”®è‚¡è´¨æŠ¼æ•°'] = 0
    
    # åˆå¹¶è´¨æŠ¼æ•°æ®
    merged_data = pd.merge(stock_info, pledge_data, on='ä»£ç ', how='left', suffixes=('', '_new'))
    
    # æ›´æ–°è´¨æŠ¼è‚¡æ•°åˆ—
    if 'è´¨æŠ¼è‚¡æ•°_new' in merged_data.columns:
        # ç”¨æ–°æ•°æ®æ›¿æ¢æ—§æ•°æ®
        merged_data['è´¨æŠ¼è‚¡æ•°'] = merged_data['è´¨æŠ¼è‚¡æ•°_new'].fillna(0)
        # ç§»é™¤ä¸´æ—¶åˆ—
        merged_data = merged_data.drop(columns=['è´¨æŠ¼è‚¡æ•°_new'])
    else:
        # å¦‚æœæ–°æ•°æ®ä¸­æ²¡æœ‰è´¨æŠ¼è‚¡æ•°åˆ—ï¼Œä¿æŒåŸå€¼
        logger.warning("è´¨æŠ¼æ•°æ®ä¸­æ²¡æœ‰'è´¨æŠ¼è‚¡æ•°'åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
        merged_data['è´¨æŠ¼è‚¡æ•°'] = 0
    
    # æ›´æ–°æ— é™å”®è‚¡è´¨æŠ¼æ•°åˆ—
    if 'æ— é™å”®è‚¡è´¨æŠ¼æ•°_new' in merged_data.columns:
        # ç”¨æ–°æ•°æ®æ›¿æ¢æ—§æ•°æ®
        merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°'] = merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°_new'].fillna(0)
        # ç§»é™¤ä¸´æ—¶åˆ—
        merged_data = merged_data.drop(columns=['æ— é™å”®è‚¡è´¨æŠ¼æ•°_new'])
    else:
        # å¦‚æœæ–°æ•°æ®ä¸­æ²¡æœ‰æ— é™å”®è‚¡è´¨æŠ¼æ•°åˆ—ï¼Œä¿æŒåŸå€¼
        logger.warning("è´¨æŠ¼æ•°æ®ä¸­æ²¡æœ‰'æ— é™å”®è‚¡è´¨æŠ¼æ•°'åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
        merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°'] = 0
    
    # è®°å½•è¿‡æ»¤å‰çš„è‚¡ç¥¨æ•°é‡
    initial_count = len(merged_data)
    logger.info(f"å¼€å§‹åº”ç”¨è´¨æŠ¼è¿‡æ»¤ï¼Œåˆå§‹è‚¡ç¥¨æ•°é‡: {initial_count}")
    
    # æ·»åŠ è¯¦ç»†çš„è´¨æŠ¼æ•°æ®ç»Ÿè®¡
    logger.info(f"è´¨æŠ¼æ•°æ®ç»Ÿè®¡: æœ€å°å€¼={merged_data['è´¨æŠ¼è‚¡æ•°'].min()}, æœ€å¤§å€¼={merged_data['è´¨æŠ¼è‚¡æ•°'].max()}, å¹³å‡å€¼={merged_data['è´¨æŠ¼è‚¡æ•°'].mean():.2f}")
    logger.info(f"æ— é™å”®è‚¡è´¨æŠ¼æ•°ç»Ÿè®¡: æœ€å°å€¼={merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°'].min()}, æœ€å¤§å€¼={merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°'].max()}, å¹³å‡å€¼={merged_data['æ— é™å”®è‚¡è´¨æŠ¼æ•°'].mean():.2f}")
    
    # æŒ‰è´¨æŠ¼è‚¡æ•°æ’åºï¼Œæ‰“å°æœ€å¤§çš„å‰ä¸¤è¡Œæ•°æ®
    top_pledge = merged_data.sort_values('è´¨æŠ¼è‚¡æ•°', ascending=False).head(2)
    logger.info("è´¨æŠ¼è‚¡æ•°æœ€å¤§çš„å‰ä¸¤è¡Œæ•°æ®:")
    for i, row in top_pledge.iterrows():
        logger.info(f"{row['ä»£ç ']}: {row['åç§°']} - è´¨æŠ¼è‚¡æ•°: {row['è´¨æŠ¼è‚¡æ•°']}")
    
    # æŒ‰æ— é™å”®è‚¡è´¨æŠ¼æ•°æ’åºï¼Œæ‰“å°æœ€å¤§çš„å‰ä¸¤è¡Œæ•°æ®
    top_unrestricted = merged_data.sort_values('æ— é™å”®è‚¡è´¨æŠ¼æ•°', ascending=False).head(2)
    logger.info("æ— é™å”®è‚¡è´¨æŠ¼æ•°æœ€å¤§çš„å‰ä¸¤è¡Œæ•°æ®:")
    for i, row in top_unrestricted.iterrows():
        logger.info(f"{row['ä»£ç ']}: {row['åç§°']} - æ— é™å”®è‚¡è´¨æŠ¼æ•°: {row['æ— é™å”®è‚¡è´¨æŠ¼æ•°']}")
    
    # åº”ç”¨è´¨æŠ¼è¿‡æ»¤æ¡ä»¶
    if PLEDGE_FILTER["enabled"]:
        threshold = PLEDGE_FILTER["threshold"]
        before = len(merged_data)
        # è®°å½•è¢«è¿‡æ»¤çš„è‚¡ç¥¨ä»£ç 
        filtered_stocks = merged_data[merged_data['è´¨æŠ¼è‚¡æ•°'] > threshold]
        filtered_codes = filtered_stocks['ä»£ç '].head(50).tolist()
        
        # åº”ç”¨è¿‡æ»¤
        merged_data = merged_data[merged_data['è´¨æŠ¼è‚¡æ•°'] <= threshold]
        removed = before - len(merged_data)
        
        if removed > 0:
            logger.info(f"æ’é™¤ {removed} åªè´¨æŠ¼è‚¡æ•°è¶…è¿‡é˜ˆå€¼({threshold})çš„è‚¡ç¥¨ï¼ˆè´¨æŠ¼è¿‡æ»¤ï¼‰")
            # è®°å½•å‰50ä¸ªè¢«è¿‡æ»¤çš„è‚¡ç¥¨ä»£ç 
            if filtered_codes:
                logger.info(f"å‰50ä¸ªè¢«è¿‡æ»¤çš„è‚¡ç¥¨ä»£ç : {', '.join(filtered_codes)}")
        else:
            logger.info(f"æ‰€æœ‰è‚¡ç¥¨è´¨æŠ¼è‚¡æ•°å‡æœªè¶…è¿‡é˜ˆå€¼({threshold})")
    
    # è®°å½•è´¨æŠ¼è¿‡æ»¤åè‚¡ç¥¨æ•°é‡
    logger.info(f"è´¨æŠ¼è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(merged_data)} æ¡è®°å½•ï¼ˆåˆå§‹: {initial_count}ï¼‰")
    
    return merged_data

def save_base_stock_info(stock_info, include_pledge=False):
    """
    ä¿å­˜åŸºç¡€è‚¡ç¥¨åˆ—è¡¨åˆ°æ–‡ä»¶
    ç¡®ä¿æ–‡ä»¶ç»“æ„: ä»£ç ,åç§°,æ‰€å±æ¿å—,æµé€šå¸‚å€¼,æ€»å¸‚å€¼,æ•°æ®çŠ¶æ€,åŠ¨æ€å¸‚ç›ˆç‡,filter,next_crawl_index[,è´¨æŠ¼è‚¡æ•°]
    
    Args:
        stock_info: åŸºç¡€è‚¡ç¥¨åˆ—è¡¨DataFrame
        include_pledge: æ˜¯å¦åŒ…å«è´¨æŠ¼è‚¡æ•°åˆ—
    """
    try:
        # åˆ›å»ºå‰¯æœ¬é¿å…SettingWithCopyWarning
        stock_info = stock_info.copy()
        
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿åˆ—åæ­£ç¡®
        # ç¡®ä¿æµé€šå¸‚å€¼å’Œæ€»å¸‚å€¼æ˜¯æ•°å€¼ç±»å‹
        if "æµé€šå¸‚å€¼" in stock_info.columns:
            stock_info["æµé€šå¸‚å€¼"] = pd.to_numeric(stock_info["æµé€šå¸‚å€¼"], errors='coerce')
        else:
            stock_info["æµé€šå¸‚å€¼"] = 0.0
            logger.warning("æµé€šå¸‚å€¼åˆ—ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼0.0")
            
        if "æ€»å¸‚å€¼" in stock_info.columns:
            stock_info["æ€»å¸‚å€¼"] = pd.to_numeric(stock_info["æ€»å¸‚å€¼"], errors='coerce')
        else:
            stock_info["æ€»å¸‚å€¼"] = 0.0
            logger.warning("æ€»å¸‚å€¼åˆ—ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼0.0")
        
        if "åŠ¨æ€å¸‚ç›ˆç‡" in stock_info.columns:
            stock_info["åŠ¨æ€å¸‚ç›ˆç‡"] = pd.to_numeric(stock_info["åŠ¨æ€å¸‚ç›ˆç‡"], errors='coerce')
        else:
            stock_info["åŠ¨æ€å¸‚ç›ˆç‡"] = 0.0
            logger.warning("åŠ¨æ€å¸‚ç›ˆç‡åˆ—ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼0.0")
        
        # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ å¿…éœ€åˆ—
        stock_info["æ•°æ®çŠ¶æ€"] = "åŸºç¡€æ•°æ®å·²è·å–"
        stock_info["filter"] = False  # æ·»åŠ filteråˆ—å¹¶è®¾ç½®é»˜è®¤å€¼ä¸ºFalse
        stock_info["next_crawl_index"] = 0
        
        # å®šä¹‰åŸºç¡€åˆ—ï¼ˆä¸åŒ…å«è´¨æŠ¼è‚¡æ•°ï¼‰
        basic_columns = ["ä»£ç ", "åç§°", "æ‰€å±æ¿å—", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "æ•°æ®çŠ¶æ€", "åŠ¨æ€å¸‚ç›ˆç‡", "filter", "next_crawl_index"]
        
        # å¦‚æœéœ€è¦åŒ…å«è´¨æŠ¼è‚¡æ•°åˆ—
        if include_pledge:
            # ç¡®ä¿"è´¨æŠ¼è‚¡æ•°"åˆ—å­˜åœ¨
            if 'è´¨æŠ¼è‚¡æ•°' not in stock_info.columns:
                logger.warning("è´¨æŠ¼è‚¡æ•°åˆ—ä¸å­˜åœ¨ï¼Œæ·»åŠ é»˜è®¤å€¼0")
                stock_info['è´¨æŠ¼è‚¡æ•°'] = 0
            else:
                # ç¡®ä¿"è´¨æŠ¼è‚¡æ•°"æ˜¯æ•°å€¼ç±»å‹
                stock_info['è´¨æŠ¼è‚¡æ•°'] = pd.to_numeric(stock_info['è´¨æŠ¼è‚¡æ•°'], errors='coerce').fillna(0)
            
            # å®šä¹‰å®Œæ•´åˆ—
            final_columns = basic_columns + ["è´¨æŠ¼è‚¡æ•°"]
        else:
            # åªä½¿ç”¨åŸºç¡€åˆ—
            final_columns = basic_columns
        
        # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in final_columns:
            if col not in stock_info.columns:
                if col == "filter":
                    stock_info[col] = False
                    logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ False")
                elif col == "next_crawl_index":
                    stock_info[col] = 0
                    logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ 0")
                elif col == "è´¨æŠ¼è‚¡æ•°":
                    stock_info[col] = 0
                    logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ 0")
                elif col in ["æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "åŠ¨æ€å¸‚ç›ˆç‡"]:
                    stock_info[col] = 0.0
                    logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ 0.0")
                else:
                    stock_info[col] = ""
                    logger.warning(f"åˆ— {col} ä¸å­˜åœ¨ï¼Œå·²æ·»åŠ é»˜è®¤å€¼ç©ºå­—ç¬¦ä¸²")
        
        # é€‰æ‹©æ­£ç¡®çš„åˆ—å¹¶æ’åº
        stock_info = stock_info[final_columns]
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        stock_info.to_csv(BASIC_INFO_FILE, index=False, float_format='%.2f')
        
        # æäº¤åˆ°Gitä»“åº“
        commit_files_in_batches(BASIC_INFO_FILE, "æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆåŸºç¡€è¿‡æ»¤åï¼‰")
        
        logger.info(f"åŸºç¡€è‚¡ç¥¨åˆ—è¡¨å·²æˆåŠŸæ›´æ–°ï¼Œå…± {len(stock_info)} æ¡è®°å½•")
    except Exception as e:
        logger.error(f"ä¿å­˜åŸºç¡€è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)

def apply_market_value_and_pe_filters():
    """
    è¯»å–æœ€æ–°çš„ all_stocks.csvï¼Œè¡¥å……ã€æµé€šå¸‚å€¼ã€æ€»å¸‚å€¼ã€åŠ¨æ€å¸‚ç›ˆç‡ã€‘ï¼Œ
    å¹¶åº”ç”¨ä»¥ä¸‹ä¸¤ä¸ªè¿‡æ»¤æ¡ä»¶ï¼š
        1. åŠ¨æ€å¸‚ç›ˆç‡ >= 0
        2. æµé€šå¸‚å€¼ / æ€»å¸‚å€¼ > 90%
    
    æœ€åå°†ç»“æœä¿å­˜å› all_stocks.csvã€‚
    """
    try:
        logger.info("å¼€å§‹è¡¥å……æµé€šå¸‚å€¼ã€æ€»å¸‚å€¼ã€åŠ¨æ€å¸‚ç›ˆç‡å¹¶åº”ç”¨æ–°è¿‡æ»¤æ¡ä»¶...")

        # 1. è¯»å–åˆšåˆšä¿å­˜çš„ all_stocks.csv
        latest_stock_df = pd.read_csv(BASIC_INFO_FILE)
        logger.info(f"ä» {BASIC_INFO_FILE} è¯»å–åˆ° {len(latest_stock_df)} æ¡è‚¡ç¥¨æ•°æ®ç”¨äºè¡¥å……æŒ‡æ ‡")

        if latest_stock_df.empty:
            logger.error("è¯»å–çš„è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¡¥å……æŒ‡æ ‡")
            return False

        # 2. è·å–å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆå«æµé€šå¸‚å€¼ã€æ€»å¸‚å€¼ã€åŠ¨æ€å¸‚ç›ˆç‡ï¼‰â€”â€” åˆ†æ‰¹æ¬¡å¤„ç†
        try:
            # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šå…ˆè·å–å…¨é‡æ•°æ®ï¼ˆè¿™æ˜¯ ak çš„é™åˆ¶ï¼Œæˆ‘ä»¬åªèƒ½è¿™ä¹ˆå¹²ï¼‰
            logger.info("æ­£åœ¨è·å–å…¨é‡å®æ—¶è¡Œæƒ…æ•°æ®...")
            spot_all_df = ak.stock_zh_a_spot_em()
            if spot_all_df.empty:
                logger.error("è·å–å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®")
                return False

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æˆ‘ä»¬çš„éœ€æ±‚
            spot_all_df.rename(columns={
                'ä»£ç ': 'ä»£ç ',
                'åç§°': 'åç§°',
                'æ€»å¸‚å€¼': 'æ€»å¸‚å€¼',
                'æµé€šå¸‚å€¼': 'æµé€šå¸‚å€¼',
                'å¸‚ç›ˆç‡-åŠ¨æ€': 'åŠ¨æ€å¸‚ç›ˆç‡'
            }, inplace=True)

            # åªä¿ç•™æˆ‘ä»¬éœ€è¦çš„åˆ—
            required_cols = ['ä»£ç ', 'åç§°', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'åŠ¨æ€å¸‚ç›ˆç‡']
            spot_all_df = spot_all_df[required_cols]

            # è½¬æ¢ä¸ºæ•°å€¼å‹ï¼ˆé¿å…å­—ç¬¦ä¸²å¯¼è‡´è®¡ç®—é”™è¯¯ï¼‰
            for col in ['æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'åŠ¨æ€å¸‚ç›ˆç‡']:
                spot_all_df[col] = pd.to_numeric(spot_all_df[col], errors='coerce')

            logger.info(f"æˆåŠŸè·å– {len(spot_all_df)} æ¡å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆå«æµé€šå¸‚å€¼/æ€»å¸‚å€¼/åŠ¨æ€å¸‚ç›ˆç‡ï¼‰")

        except Exception as e:
            logger.error(f"è·å–å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
            return False

        # 3. åˆ†æ‰¹æ¬¡å¤„ç†ï¼šæŠŠè‚¡ç¥¨ä»£ç åˆ†æˆå°æ‰¹æ¬¡ï¼Œæ¯æ¬¡å¤„ç† 50 åª
        stock_codes = latest_stock_df['ä»£ç '].tolist()
        batch_size = 50
        all_batch_data = []

        for i in range(0, len(stock_codes), batch_size):
            batch_codes = stock_codes[i:i+batch_size]
            logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ¬¡ï¼ˆ{len(batch_codes)} åªè‚¡ç¥¨ï¼‰...")

            # ä»å…¨é‡æ•°æ®ä¸­ç­›é€‰å‡ºæœ¬æ‰¹æ¬¡çš„è‚¡ç¥¨
            batch_df = spot_all_df[spot_all_df['ä»£ç '].isin(batch_codes)]

            # å¦‚æœæœ¬æ‰¹æ¬¡æ²¡æœ‰æ•°æ®ï¼Œè®°å½•æ—¥å¿—
            if batch_df.empty:
                logger.warning(f"ç¬¬ {i//batch_size + 1} æ‰¹æ¬¡æ— å¯¹åº”è¡Œæƒ…æ•°æ®")
            else:
                logger.info(f"âœ… ç¬¬ {i//batch_size + 1} æ‰¹æ¬¡è·å–åˆ° {len(batch_df)} æ¡æ•°æ®")

            all_batch_data.append(batch_df)

            # æ¯æ‰¹ä¹‹é—´åŠ å»¶æ—¶ï¼ˆé¿å…è¢«å°ï¼‰
            time.sleep(random.uniform(1.0, 3.0))

        # 4. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®
        if all_batch_data:
            spot_df = pd.concat(all_batch_data, ignore_index=True)
        else:
            spot_df = pd.DataFrame()

        # 5. åˆå¹¶æ•°æ®ï¼šåŸºäºâ€œä»£ç â€å·¦è¿æ¥ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹è‚¡ç¥¨ï¼Œç¼ºå¤±å€¼è®¾ä¸º NaN
        merged_df = latest_stock_df.merge(
            spot_df[['ä»£ç ', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼', 'åŠ¨æ€å¸‚ç›ˆç‡']],
            on='ä»£ç ',
            how='left',
            suffixes=('', '_new')
        )

        # 6. è®°å½•è¡¥å……å‰çŠ¶æ€
        initial_count = len(merged_df)
        logger.info(f"è¡¥å……æŒ‡æ ‡å‰è‚¡ç¥¨æ•°é‡: {initial_count}")

        # 7. åº”ç”¨æ–°è¿‡æ»¤æ¡ä»¶
        # æ¡ä»¶1ï¼šåŠ¨æ€å¸‚ç›ˆç‡ >= 0
        before_pe = len(merged_df)
        merged_df = merged_df.dropna(subset=['åŠ¨æ€å¸‚ç›ˆç‡'])  # å…ˆæ’é™¤NaN
        merged_df = merged_df[merged_df['åŠ¨æ€å¸‚ç›ˆç‡'] >= 0]
        removed_pe = before_pe - len(merged_df)
        logger.info(f"æ’é™¤ {removed_pe} åªåŠ¨æ€å¸‚ç›ˆç‡ < 0 çš„è‚¡ç¥¨ï¼ˆPEè¿‡æ»¤ï¼‰")

        # æ¡ä»¶2ï¼šæµé€šå¸‚å€¼ / æ€»å¸‚å€¼ > 90%
        before_ratio = len(merged_df)
        merged_df = merged_df.dropna(subset=['æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼'])
        merged_df = merged_df[merged_df['æ€»å¸‚å€¼'] > 0]
        merged_df['æµé€šå¸‚å€¼å æ¯”'] = merged_df['æµé€šå¸‚å€¼'] / merged_df['æ€»å¸‚å€¼']
        merged_df = merged_df[merged_df['æµé€šå¸‚å€¼å æ¯”'] > 0.9]
        removed_ratio = before_ratio - len(merged_df)
        logger.info(f"æ’é™¤ {removed_ratio} åªæµé€šå¸‚å€¼å æ¯” <= 90% çš„è‚¡ç¥¨ï¼ˆå¸‚å€¼ç»“æ„è¿‡æ»¤ï¼‰")

        # 8. æ¸…ç†ä¸´æ—¶åˆ—
        if 'æµé€šå¸‚å€¼å æ¯”' in merged_df.columns:
            merged_df = merged_df.drop(columns=['æµé€šå¸‚å€¼å æ¯”'])

        # 9. é‡æ–°æ•´ç†åˆ—é¡ºåºï¼ˆç¡®ä¿ä¸åŸç»“æ„ä¸€è‡´ï¼‰
        target_columns = [
            "ä»£ç ", "åç§°", "æ‰€å±æ¿å—", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "æ•°æ®çŠ¶æ€", 
            "åŠ¨æ€å¸‚ç›ˆç‡", "filter", "next_crawl_index", "è´¨æŠ¼è‚¡æ•°"
        ]
        # è¡¥å……ç¼ºå¤±åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for col in target_columns:
            if col not in merged_df.columns:
                if col == "filter":
                    merged_df[col] = False
                elif col == "next_crawl_index":
                    merged_df[col] = 0
                elif col in ["æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "åŠ¨æ€å¸‚ç›ˆç‡"]:
                    merged_df[col] = 0.0
                elif col == "è´¨æŠ¼è‚¡æ•°":
                    merged_df[col] = 0
                else:
                    merged_df[col] = ""

        # é€‰æ‹©ç›®æ ‡åˆ—å¹¶æ’åº
        merged_df = merged_df[target_columns]

        # 10. ä¿å­˜æœ€ç»ˆç»“æœ
        merged_df.to_csv(BASIC_INFO_FILE, index=False, float_format='%.2f')
        commit_files_in_batches(BASIC_INFO_FILE, "æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆè¡¥å……æµé€šå¸‚å€¼/æ€»å¸‚å€¼/åŠ¨æ€å¸‚ç›ˆç‡å¹¶è¿‡æ»¤ï¼‰")
        logger.info(f"è‚¡ç¥¨åˆ—è¡¨å·²æˆåŠŸè¡¥å……è´¢åŠ¡æŒ‡æ ‡å¹¶å®Œæˆæœ€ç»ˆè¿‡æ»¤ï¼Œå…± {len(merged_df)} æ¡è®°å½•")

        return True

    except Exception as e:
        logger.error(f"åº”ç”¨å¸‚å€¼å’ŒPEè¿‡æ»¤å¤±è´¥: {str(e)}", exc_info=True)
        return False
       
def update_stock_list():
    """
    æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼Œä¿å­˜åˆ°all_stocks.csv
    """
    try:
        logger.info("å¼€å§‹æ›´æ–°è‚¡ç¥¨åˆ—è¡¨...")
        
        # ã€å…³é”®ä¿®å¤ã€‘ç›´æ¥è·å–è‚¡ç¥¨åˆ—è¡¨æ•°æ®
        stock_data = get_stock_list_data()
        
        if stock_data.empty:
            logger.error("è·å–çš„è‚¡ç¥¨åˆ—è¡¨æ•°æ®ä¸ºç©º")
            return False
        
        # ã€å…³é”®ä¿®å¤ã€‘åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
        filtered_data = apply_basic_filters(stock_data)
        
        if filtered_data.empty:
            logger.error("åŸºç¡€è¿‡æ»¤åè‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return False
        
        # ã€å…³é”®ä¿®å¤ã€‘åˆæ¬¡ä¿å­˜æ—¶ï¼Œä¸åŒ…å«è´¨æŠ¼è‚¡æ•°åˆ—
        save_base_stock_info(filtered_data, include_pledge=False)
        
        # ã€æ–°å¢ã€‘åº”ç”¨è´¨æŠ¼æ•°æ®è¿‡æ»¤
        logger.info("å¼€å§‹åº”ç”¨è´¨æŠ¼æ•°æ®è¿‡æ»¤...")
        pledge_filtered_data = apply_pledge_filter(filtered_data)
        
        # ã€ä¿®å¤ã€‘ä¿å­˜è¿‡æ»¤åçš„è‚¡ç¥¨åˆ—è¡¨
        if not pledge_filtered_data.empty:
            # ç›´æ¥ä¿å­˜è´¨æŠ¼è¿‡æ»¤åçš„æ•°æ®ï¼ˆå·²ç»åŒ…å«è´¨æŠ¼ä¿¡æ¯ï¼‰
            save_base_stock_info(pledge_filtered_data, include_pledge=True)
            logger.info(f"è‚¡ç¥¨åˆ—è¡¨å·²æˆåŠŸåº”ç”¨è´¨æŠ¼è¿‡æ»¤å¹¶æ›´æ–°")
        else:
            logger.warning("è´¨æŠ¼è¿‡æ»¤åæ— è‚¡ç¥¨æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
            return False
        
        # âœ… æ–°å¢ï¼šè°ƒç”¨ç‹¬ç«‹å‡½æ•°å¤„ç†å¸‚å€¼/PEè¡¥å……ä¸è¿‡æ»¤
        logger.info("å¼€å§‹åº”ç”¨å¸‚å€¼ä¸PEè¿‡æ»¤...")
        if not apply_market_value_and_pe_filters():
            logger.error("å¸‚å€¼ä¸PEè¿‡æ»¤é˜¶æ®µå¤±è´¥ï¼Œç»ˆæ­¢æ›´æ–°æµç¨‹")
            return False

        logger.info("è‚¡ç¥¨åˆ—è¡¨æ›´æ–°æµç¨‹å…¨éƒ¨å®Œæˆ âœ…")
        return True

    except Exception as e:
        logger.error(f"æ›´æ–°è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return False

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                        handlers=[
                            logging.StreamHandler(),
                            logging.FileHandler(os.path.join(LOG_DIR, "all_stocks.log"))
                        ])
    
    # æ›´æ–°è‚¡ç¥¨åˆ—è¡¨
    update_stock_list()
