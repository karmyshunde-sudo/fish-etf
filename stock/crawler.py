#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®çˆ¬å–æ¨¡å— - ä¸¥æ ¼ç¡®ä¿è‚¡ç¥¨ä»£ç ä¸º6ä½æ ¼å¼ï¼Œæ—¥æœŸå¤„ç†é€»è¾‘å®Œå–„
ã€æœ€ç»ˆä¿®å¤ç‰ˆã€‘
- å½»åº•ä¿®å¤è‚¡ç¥¨ä»£ç æ ¼å¼é—®é¢˜ï¼Œç¡®ä¿æ‰€æœ‰åœ°æ–¹éƒ½ä¿å­˜ä¸º6ä½ä»£ç 
- å½»åº•ä¿®å¤æ—¥æœŸç±»å‹é—®é¢˜ï¼Œç¡®ä¿æ‰€æœ‰æ—¥æœŸæ¯”è¾ƒéƒ½ä½¿ç”¨ç›¸åŒç±»å‹
- ä¸¥æ ¼ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´ï¼Œä¸å¤„ç†æœªæ¥æ—¥æœŸ
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
- æ–°å¢è¡¥å…¨ç¼ºå¤±æ—¥çº¿æ•°æ®çš„åŠŸèƒ½
"""

import os
import logging
import pandas as pd
import akshare as ak
import time
import random
import json
from datetime import datetime, timedelta, date
from config import Config
from utils.date_utils import is_trading_day, get_last_trading_day, get_beijing_time
from utils.git_utils import commit_files_in_batches

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_stocks.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

def ensure_directory_exists():
    """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
    if not os.path.exists(DAILY_DIR):
        os.makedirs(DAILY_DIR)
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

def format_stock_code(code):
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½å­—ç¬¦ä¸²æ ¼å¼
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¯èƒ½åŒ…å«å‰ç¼€æˆ–é6ä½ï¼‰
    Returns:
        str: è§„èŒƒåŒ–çš„6ä½è‚¡ç¥¨ä»£ç 
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    code_str = str(code).strip().lower()
    
    # ç§»é™¤å¯èƒ½çš„å¸‚åœºå‰ç¼€
    if code_str.startswith(('sh', 'sz', 'hk', 'bj')):
        code_str = code_str[2:]
    
    # ç§»é™¤å¯èƒ½çš„ç‚¹å·ï¼ˆå¦‚"0.600022"ï¼‰
    if '.' in code_str:
        code_str = code_str.split('.')[1] if code_str.startswith('0.') else code_str
    
    # ç¡®ä¿æ˜¯6ä½æ•°å­—
    code_str = code_str.zfill(6)
    
    # éªŒè¯æ ¼å¼
    if not code_str.isdigit() or len(code_str) != 6:
        logger.warning(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {code_str}")
        return None
    
    return code_str

def get_stock_section(stock_code: str) -> str:
    """
    è·å–è‚¡ç¥¨æ‰€å±æ¿å—
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå·²æ ¼å¼åŒ–ä¸º6ä½ï¼‰
    
    Returns:
        str: æ¿å—åç§°
    """
    # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½
    stock_code = format_stock_code(stock_code)
    if not stock_code:
        return "æ ¼å¼é”™è¯¯"
    
    # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€åˆ¤æ–­æ¿å—
    if stock_code.startswith('60'):
        return "æ²ªå¸‚ä¸»æ¿"
    elif stock_code.startswith('00'):
        return "æ·±å¸‚ä¸»æ¿"
    elif stock_code.startswith('30'):
        return "åˆ›ä¸šæ¿"
    elif stock_code.startswith('688'):
        return "ç§‘åˆ›æ¿"
    elif stock_code.startswith('8'):
        return "åŒ—äº¤æ‰€"
    elif stock_code.startswith('4') or stock_code.startswith('8'):
        return "ä¸‰æ¿å¸‚åœº"
    else:
        return "å…¶ä»–æ¿å—"

def to_naive_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºnaive datetimeï¼ˆæ— æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: naive datetime
    """
    if dt is None:
        return None
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

def to_aware_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºaware datetimeï¼ˆæœ‰æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: aware datetimeï¼ˆåŒ—äº¬æ—¶åŒºï¼‰
    """
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=Config.BEIJING_TIMEZONE)
    return dt

def get_valid_trading_date_range(start_date, end_date):
    """
    è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´ï¼Œç¡®ä¿åªåŒ…å«å†å²äº¤æ˜“æ—¥
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
    
    Returns:
        tuple: (valid_start_date, valid_end_date) - æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´
    """
    # ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    
    if start_date is None or end_date is None:
        logger.error("æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
        return None, None
    
    # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
    now = get_beijing_time()
    # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡ç±»å‹ä¸€è‡´
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(now)
    
    if end_date > now:
        end_date = now
        logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    days_back = 0
    while days_back < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_end_date.date()):
            break
        valid_end_date -= timedelta(days=1)
        days_back += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼Œè¿”å›ç©ºèŒƒå›´
    if days_back >= 30:
        logger.warning(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼ˆä» {end_date.strftime('%Y-%m-%d')} å¼€å§‹ï¼‰")
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    days_forward = 0
    while days_forward < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_start_date.date()):
            break
        valid_start_date += timedelta(days=1)
        days_forward += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ç»“æŸäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹æ—¥
    if days_forward >= 30:
        valid_start_date = valid_end_date
    
    # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰ç±»å‹ä¸€è‡´
    start_naive = to_naive_datetime(valid_start_date)
    end_naive = to_naive_datetime(valid_end_date)
    
    if start_naive > end_naive:
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def to_datetime(date_input):
    """
    ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    Args:
        date_input: æ—¥æœŸè¾“å…¥ï¼Œå¯ä»¥æ˜¯strã€dateã€datetimeç­‰ç±»å‹
    Returns:
        datetime.datetime: ç»Ÿä¸€çš„datetimeç±»å‹
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try:
                return datetime.strptime(date_input, fmt)
            except:
                continue
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_input}")
        return None
    return None

def fetch_stock_daily_data(stock_code: str) -> pd.DataFrame:
    """è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å"""
    try:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½ï¼ˆå‰é¢è¡¥é›¶ï¼‰
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            logger.error(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {stock_code}")
            return pd.DataFrame()
        
        # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰è¯¥è‚¡ç¥¨çš„æ—¥çº¿æ•°æ®æ–‡ä»¶
        local_file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
        existing_data = None
        last_date = None
        
        if os.path.exists(local_file_path):
            try:
                # è¯»å–å·²æœ‰çš„æ•°æ®
                existing_data = pd.read_csv(local_file_path)
                if not existing_data.empty and 'æ—¥æœŸ' in existing_data.columns:
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                    existing_data['æ—¥æœŸ'] = pd.to_datetime(existing_data['æ—¥æœŸ'], errors='coerce')
                    # è·å–æœ€åä¸€æ¡æ•°æ®çš„æ—¥æœŸ
                    last_date = existing_data['æ—¥æœŸ'].max()
                    if pd.notna(last_date):
                        logger.info(f"è‚¡ç¥¨ {stock_code} æœ¬åœ°å·²æœ‰æ•°æ®ï¼Œæœ€åæ—¥æœŸ: {last_date.strftime('%Y-%m-%d')}")
                    else:
                        last_date = None
            except Exception as e:
                logger.warning(f"è¯»å–è‚¡ç¥¨ {stock_code} æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")
                existing_data = None
                last_date = None
        
        # ===== å…³é”®ä¿®å¤ï¼šç¡®ä¿åªå¤„ç†å†å²äº¤æ˜“æ—¥ =====
        # 1. ç¡®å®šçˆ¬å–çš„æ—¥æœŸèŒƒå›´
        if last_date is not None:
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹ç‚¹
            current_date = last_date + timedelta(days=1)
            start_date = None
            
            # æœ€å¤šæŸ¥æ‰¾30å¤©ï¼Œé¿å…æ— é™å¾ªç¯
            for i in range(30):
                if is_trading_day(current_date.date()):
                    start_date = current_date
                    break
                current_date += timedelta(days=1)
            
            if not start_date:
                # å¦‚æœæ‰¾ä¸åˆ°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
                last_trading_date = get_last_trading_day()
                if last_trading_date:
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿last_trading_dateæ˜¯datetimeç±»å‹
                    if not isinstance(last_trading_date, datetime):
                        last_trading_date = datetime.combine(last_trading_date, datetime.min.time())
                    start_date = last_trading_date
                    logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥: {start_date.strftime('%Y%m%d')}")
                else:
                    logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æœ‰æ•ˆäº¤æ˜“æ—¥ï¼Œè·³è¿‡çˆ¬å–")
                    return pd.DataFrame()
            
            # è·å–å½“å‰æ—¥æœŸå‰çš„æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
            end_date = get_last_trading_day()
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿end_dateæ˜¯datetimeç±»å‹
            if not isinstance(end_date, datetime):
                end_date = datetime.combine(end_date, datetime.min.time())
            
            # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
            now = get_beijing_time()
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            now_naive = to_naive_datetime(now)
            end_date_naive = to_naive_datetime(end_date)
            
            if end_date_naive > now_naive:
                end_date = now
                logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
            
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ—¥æœŸç±»å‹ä¸€è‡´
            if not isinstance(start_date, datetime):
                start_date = to_datetime(start_date)
            if not isinstance(end_date, datetime):
                end_date = to_datetime(end_date)
            
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            # è½¬æ¢ä¸ºnaive datetimeè¿›è¡Œæ¯”è¾ƒ
            start_date_naive = to_naive_datetime(start_date)
            end_date_naive = to_naive_datetime(end_date)
            
            # ä¸¥æ ¼æ£€æŸ¥æ—¥æœŸ
            # å¼€å§‹æ—¥æœŸ >= ç»“æŸæ—¥æœŸï¼Œä»£è¡¨æ•°æ®å·²æœ€æ–°
            if start_date_naive >= end_date_naive:
                logger.info(f"è‚¡ç¥¨ {stock_code} æ²¡æœ‰æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆå¼€å§‹æ—¥æœŸ: {start_date.strftime('%Y%m%d')} >= ç»“æŸæ—¥æœŸ: {end_date.strftime('%Y%m%d')}ï¼‰")
                return pd.DataFrame()
            
            logger.info(f"è‚¡ç¥¨ {stock_code} å¢é‡çˆ¬å–ï¼Œä» {start_date.strftime('%Y%m%d')} åˆ° {end_date.strftime('%Y%m%d')}")
        else:
            # æ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œçˆ¬å–æœ€è¿‘ä¸€å¹´çš„æ•°æ®
            now = get_beijing_time()
            start_date = now - timedelta(days=365)
            end_date = get_last_trading_day()
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿end_dateæ˜¯datetimeç±»å‹
            if not isinstance(end_date, datetime):
                end_date = datetime.combine(end_date, datetime.min.time())
            
            # ç¡®ä¿èµ·å§‹æ—¥æœŸæ˜¯äº¤æ˜“æ—¥
            current_date = start_date
            start_date = None
            for i in range(30):
                if is_trading_day(current_date.date()):
                    start_date = current_date
                    break
                current_date += timedelta(days=1)
            
            if not start_date:
                start_date = end_date
            
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            # è½¬æ¢ä¸ºnaive datetimeè¿›è¡Œæ¯”è¾ƒ
            start_date_naive = to_naive_datetime(start_date)
            end_date_naive = to_naive_datetime(end_date)
            
            # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
            if start_date_naive > end_date_naive:
                start_date = end_date
            
            logger.info(f"è‚¡ç¥¨ {stock_code} é¦–æ¬¡çˆ¬å–ï¼Œè·å–ä» {start_date.strftime('%Y%m%d')} åˆ° {end_date.strftime('%Y%m%d')} çš„æ•°æ®")
        
        # ã€å…³é”®ä¿®å¤ã€‘ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        start_date_str = start_date.strftime("%Y%m%d")
        end_date_str = end_date.strftime("%Y%m%d")
        
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æµ‹è¯•æˆåŠŸçš„è°ƒç”¨æ–¹å¼ï¼šä¸å¸¦å¸‚åœºå‰ç¼€ï¼
        logger.debug(f"æ­£åœ¨è·å–è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ® (ä»£ç : {stock_code}, å¤æƒå‚æ•°: qfq)")
        
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æµ‹è¯•æˆåŠŸçš„å‚æ•°è¿›è¡Œå¢é‡çˆ¬å–
        try:
            df = ak.stock_zh_a_hist(
                symbol=stock_code,      # ä¸å¸¦å¸‚åœºå‰ç¼€ï¼
                period="daily",
                start_date=start_date_str,
                end_date=end_date_str,
                adjust="qfq"
            )
        except Exception as e:
            logger.warning(f"è·å–è‚¡ç¥¨ {stock_code} çš„å¢é‡æ•°æ®å¤±è´¥ï¼Œå°è¯•è·å–30å¤©æ•°æ®: {str(e)}")
            try:
                # å°è¯•è·å–30å¤©æ•°æ®ï¼ˆé€‚ç”¨äºæ–°ä¸Šå¸‚è‚¡ç¥¨ï¼‰
                df = ak.stock_zh_a_hist(
                    symbol=stock_code,      # ä¸å¸¦å¸‚åœºå‰ç¼€ï¼
                    period="daily",
                    start_date=(datetime.now() - timedelta(days=30)).strftime("%Y%m%d"),
                    end_date=datetime.now().strftime("%Y%m%d"),
                    adjust="qfq"
                )
            except Exception as e:
                logger.warning(f"è·å–è‚¡ç¥¨ {stock_code} çš„30å¤©æ•°æ®å¤±è´¥ï¼Œå°è¯•è·å–ä¸å¤æƒæ•°æ®: {str(e)}")
                # å°è¯•ä¸å¤æƒæ•°æ®
                df = ak.stock_zh_a_hist(
                    symbol=stock_code,      # ä¸å¸¦å¸‚åœºå‰ç¼€ï¼
                    period="daily",
                    start_date=(datetime.now() - timedelta(days=30)).strftime("%Y%m%d"),
                    end_date=datetime.now().strftime("%Y%m%d"),
                    adjust=""
                )
        
        # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ è¯¦ç»†çš„APIå“åº”æ£€æŸ¥
        if df is None or df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # æ·»åŠ åˆ—åæ£€æŸ¥æ—¥å¿—
        logger.debug(f"è‚¡ç¥¨ {stock_code} è·å–åˆ°çš„åˆ—å: {df.columns.tolist()}")
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"è‚¡ç¥¨ {stock_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return pd.DataFrame()
        
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        # ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å€¼ç±»å‹
        numeric_columns = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç§»é™¤NaNå€¼
        df = df.dropna(subset=['æ”¶ç›˜', 'æˆäº¤é‡'])
        
        # ã€å…³é”®ä¿®å¤ã€‘åˆå¹¶æ–°æ•°æ®ä¸å·²æœ‰æ•°æ®
        if existing_data is not None and not existing_data.empty:
            # åˆå¹¶æ•°æ®å¹¶å»é‡
            combined_df = pd.concat([existing_data, df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
            # æŒ‰æ—¥æœŸæ’åº
            combined_df = combined_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            # ã€å…³é”®ä¿®å¤ã€‘åªä¿ç•™æœ€è¿‘ä¸€å¹´çš„æ•°æ®ï¼ˆçº¦250ä¸ªäº¤æ˜“æ—¥ï¼‰
            if len(combined_df) > 250:
                combined_df = combined_df.tail(250)
            
            df = combined_df
            logger.info(f"è‚¡ç¥¨ {stock_code} åˆå¹¶åå…±æœ‰ {len(df)} æ¡è®°å½•ï¼ˆæ–°å¢ {len(df) - len(existing_data)} æ¡ï¼‰")
        else:
            logger.info(f"è‚¡ç¥¨ {stock_code} æˆåŠŸè·å– {len(df)} æ¡æ—¥çº¿æ•°æ®")
        
        return df
    
    except Exception as e:
        # æ·»åŠ è¯¦ç»†çš„å¼‚å¸¸æ—¥å¿—
        logger.error(f"è·å–è‚¡ç¥¨ {stock_code} æ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸:", exc_info=True)
        logger.error(f"akshare ç‰ˆæœ¬: {ak.__version__}")
        logger.error(f"akshare æ¨¡å—è·¯å¾„: {ak.__file__}")
        return pd.DataFrame()

def save_stock_daily_data(stock_code: str, df: pd.DataFrame):
    """ä¿å­˜è‚¡ç¥¨æ—¥çº¿æ•°æ®åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å"""
    if df.empty:
        return
    
    try:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½ï¼ˆå‰é¢è¡¥é›¶ï¼‰
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            logger.error(f"æ— æ³•ä¿å­˜ï¼šè‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥")
            return
        
        file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ä¿å­˜å‰å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if 'æ—¥æœŸ' in df.columns:
            df_save = df.copy()
            df_save['æ—¥æœŸ'] = df_save['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
        else:
            df_save = df
        
        # ä¿å­˜æ•°æ®
        df_save.to_csv(file_path, index=False)
        
        logger.debug(f"å·²ä¿å­˜è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®åˆ° {file_path}")
        
        # ã€å…³é”®ä¿®å¤ã€‘åªéœ€ç®€å•è°ƒç”¨ï¼Œæ— éœ€ä»»ä½•é¢å¤–é€»è¾‘
        commit_files_in_batches(file_path)
        logger.debug(f"å·²æäº¤è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®åˆ°ä»“åº“")
    except Exception as e:
        logger.error(f"ä¿å­˜è‚¡ç¥¨ {stock_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

def complete_missing_stock_data():
    """
    è¡¥å…¨ç¼ºå¤±çš„è‚¡ç¥¨æ—¥çº¿æ•°æ®
    1. æ¯”å¯¹è‚¡ç¥¨åˆ—è¡¨ä¸æ—¥çº¿æ•°æ®ç›®å½•
    2. ä¸ºç¼ºå¤±çš„è‚¡ç¥¨è°ƒç”¨æ­£å¸¸çˆ¬å–æµç¨‹
    """
    logger.info("å¼€å§‹æ£€æŸ¥å¹¶è¡¥å…¨ç¼ºå¤±çš„è‚¡ç¥¨æ—¥çº¿æ•°æ®...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_directory_exists()
    
    # æ£€æŸ¥åŸºç¡€ä¿¡æ¯æ–‡ä»¶
    if not os.path.exists(BASIC_INFO_FILE):
        logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œç¼ºå¤±æ•°æ®è¡¥å…¨")
        return False
    
    try:
        # åŠ è½½åŸºç¡€ä¿¡æ¯
        basic_info_df = pd.read_csv(BASIC_INFO_FILE)
        if basic_info_df.empty:
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œç¼ºå¤±æ•°æ®è¡¥å…¨")
            return False
        
        # ç¡®ä¿"ä»£ç "åˆ—æ˜¯6ä½æ ¼å¼
        basic_info_df["ä»£ç "] = basic_info_df["ä»£ç "].apply(format_stock_code)
        # ç§»é™¤æ— æ•ˆè‚¡ç¥¨
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].notna()]
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].str.len() == 6]
        basic_info_df = basic_info_df.reset_index(drop=True)
        
        # ç»Ÿè®¡æœ‰æ•ˆè‚¡ç¥¨æ•°é‡
        total_stocks = len(basic_info_df)
        logger.info(f"åŸºç¡€ä¿¡æ¯ä¸­åŒ…å« {total_stocks} åªè‚¡ç¥¨")
        
        # æ£€æŸ¥å“ªäº›è‚¡ç¥¨ç¼ºå¤±æ—¥çº¿æ•°æ®
        missing_stocks = []
        for _, row in basic_info_df.iterrows():
            stock_code = format_stock_code(row["ä»£ç "])
            if not stock_code:
                continue
                
            file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
            if not os.path.exists(file_path):
                missing_stocks.append(stock_code)
        
        # æ²¡æœ‰ç¼ºå¤±æ•°æ®ï¼Œç›´æ¥è¿”å›
        if not missing_stocks:
            logger.info("æ‰€æœ‰è‚¡ç¥¨æ—¥çº¿æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return True
        
        logger.info(f"å‘ç° {len(missing_stocks)} åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ç¼ºå¤±ï¼Œå¼€å§‹è¡¥å…¨...")
        
        # æŒ‰é¡ºåºå¤„ç†ç¼ºå¤±è‚¡ç¥¨
        for i, stock_code in enumerate(missing_stocks):
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1.5, 2.5))
            
            logger.info(f"è¡¥å…¨ç¬¬ {i+1}/{len(missing_stocks)} åªç¼ºå¤±è‚¡ç¥¨: {stock_code}")
            df = fetch_stock_daily_data(stock_code)
            
            if not df.empty:
                save_stock_daily_data(stock_code, df)
                logger.info(f"æˆåŠŸè¡¥å…¨è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®")
            else:
                logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®è¡¥å…¨å¤±è´¥")
        
        # æ£€æŸ¥è¡¥å…¨ç»“æœ
        still_missing = []
        for stock_code in missing_stocks:
            file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
            if not os.path.exists(file_path):
                still_missing.append(stock_code)
        
        if still_missing:
            logger.warning(f"è¡¥å…¨åä»æœ‰ {len(still_missing)} åªè‚¡ç¥¨ç¼ºå¤±æ—¥çº¿æ•°æ®: {still_missing}")
        else:
            logger.info(f"æ‰€æœ‰ç¼ºå¤±è‚¡ç¥¨æ•°æ®å·²æˆåŠŸè¡¥å…¨")
        
        return len(still_missing) == 0
    
    except Exception as e:
        logger.error(f"è¡¥å…¨ç¼ºå¤±è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return False

def update_all_stocks_daily_data():
    """
    æ›´æ–°æ‰€æœ‰è‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å
    """
    try:
        logger.info("=== å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        # åˆå§‹åŒ–ç›®å½•
        Config.init_dirs()
        stock_daily_dir = os.path.join(Config.DATA_DIR, "etf_daily")
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {stock_daily_dir}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = get_all_stocks()
        total_count = len(stock_list)
        logger.info(f"å¾…çˆ¬å–è‚¡ç¥¨æ€»æ•°ï¼š{total_count}åª")
        
        # åŠ è½½è¿›åº¦
        progress = load_progress()
        next_index = progress["next_index"]
        
        # ç¡®å®šå¤„ç†èŒƒå›´
        batch_size = 100
        start_idx = next_index
        end_idx = min(start_idx + batch_size, len(stock_list))
        
        # å…³é”®ä¿®å¤ï¼šå½“ç´¢å¼•åˆ°è¾¾æ€»æ•°æ—¶ï¼Œé‡ç½®ç´¢å¼•
        if start_idx >= len(stock_list):
            logger.info("å·²çˆ¬å–å®Œæ‰€æœ‰è‚¡ç¥¨ï¼Œé‡ç½®çˆ¬å–çŠ¶æ€")
            start_idx = 0
            end_idx = min(150, len(stock_list))
        
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ ETF ({end_idx - start_idx}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
        
        # å·²å®Œæˆåˆ—è¡¨è·¯å¾„
        completed_file = os.path.join(stock_daily_dir, "etf_daily_completed.txt")
        
        # åŠ è½½å·²å®Œæˆåˆ—è¡¨
        completed_codes = set()
        if os.path.exists(completed_file):
            try:
                with open(completed_file, "r", encoding="utf-8") as f:
                    completed_codes = set(line.strip() for line in f if line.strip())
                logger.info(f"è¿›åº¦è®°å½•ä¸­å·²å®Œæˆçˆ¬å–çš„ETFæ•°é‡ï¼š{len(completed_codes)}")
            except Exception as e:
                logger.error(f"è¯»å–è¿›åº¦è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                completed_codes = set()
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡
        processed_count = 0
        last_processed_code = None
        for i in range(start_idx, end_idx):
            stock_code = stock_list[i]
            stock_name = get_stock_name(stock_code)
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(stock_code)
            if start_date is None or end_date is None:
                logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"è‚¡ç¥¨ä»£ç ï¼š{stock_code}| åç§°ï¼š{stock_name}")
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            
            df = crawl_etf_daily_data(stock_code, start_date, end_date)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if df.empty:
                logger.info(f"è‚¡ç¥¨ä»£ç ï¼š{stock_code}| åç§°ï¼š{stock_name}")
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                # è®°å½•å¤±è´¥æ—¥å¿—
                with open(os.path.join(stock_daily_dir, "failed_etfs.txt"), "a", encoding="utf-8") as f:
                    f.write(f"{stock_code},{stock_name},æœªè·å–åˆ°æ•°æ®\n")
                continue
            
            # å¤„ç†å·²æœ‰æ•°æ®çš„è¿½åŠ é€»è¾‘
            save_path = os.path.join(stock_daily_dir, f"{stock_code}.csv")
            if os.path.exists(save_path):
                try:
                    existing_df = pd.read_csv(save_path)
                    
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                    if "æ—¥æœŸ" in existing_df.columns:
                        existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors='coerce')
                    
                    # åˆå¹¶æ•°æ®å¹¶å»é‡
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                    combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=False)
                    
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                    combined_df.to_csv(temp_file.name, index=False)
                    # åŸå­æ›¿æ¢
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²è¿½åŠ è‡³: {save_path} (åˆå¹¶åå…±{len(combined_df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            else:
                # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­æ“ä½œ
                temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig')
                try:
                    df.to_csv(temp_file.name, index=False)
                    # åŸå­æ›¿æ¢
                    shutil.move(temp_file.name, save_path)
                    logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")
                finally:
                    if os.path.exists(temp_file.name):
                        os.unlink(temp_file.name)
            
            # æ ‡è®°ä¸ºå·²å®Œæˆ
            with open(completed_file, "a", encoding="utf-8") as f:
                f.write(f"{stock_code}\n")
            
            # æ¯10åªETFæäº¤ä¸€æ¬¡
            processed_count += 1
            if processed_count % 10 == 0 or processed_count == (end_idx - start_idx):
                logger.info(f"å·²å¤„ç† {processed_count} åªETFï¼Œæ‰§è¡Œæäº¤æ“ä½œ...")
                try:
                    from utils.git_utils import commit_final
                    commit_final()
                    logger.info(f"å·²æäº¤å‰ {processed_count} åªETFçš„æ•°æ®åˆ°ä»“åº“")
                except Exception as e:
                    logger.error(f"æäº¤æ–‡ä»¶æ—¶å‡ºé”™ï¼Œç»§ç»­æ‰§è¡Œ: {str(e)}")
            
            # æ›´æ–°è¿›åº¦
            last_processed_code = stock_code
            save_progress(stock_code, start_idx + processed_count, total_count, i + 1)
            
            # è®°å½•è¿›åº¦
            logger.info(f"è¿›åº¦: {start_idx + processed_count}/{total_count} ({(start_idx + processed_count)/total_count*100:.1f}%)")
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿›åº¦æ–‡ä»¶è¢«æ­£ç¡®ä¿å­˜
        # å³ä½¿æ²¡æœ‰ETFéœ€è¦å¤„ç†ï¼Œä¹Ÿè¦æ›´æ–°è¿›åº¦
        if processed_count == 0:
            logger.info("æœ¬æ‰¹æ¬¡æ— æ–°æ•°æ®éœ€è¦çˆ¬å–")
            # ä¿å­˜è¿›åº¦
            save_progress(last_processed_code, start_idx + processed_count, total_count, end_idx)
        
        # çˆ¬å–å®Œæœ¬æ‰¹æ¬¡åï¼Œç›´æ¥é€€å‡ºï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è°ƒç”¨
        logger.info(f"æœ¬æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªETF")
        logger.info("ç¨‹åºå°†é€€å‡ºï¼Œç­‰å¾…å·¥ä½œæµå†æ¬¡è°ƒç”¨")
        
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # ä¿å­˜è¿›åº¦ï¼ˆå¦‚æœå¤±è´¥ï¼‰
        try:
            save_progress(None, next_index, total_count, next_index)
        except:
            pass
        raise

def get_all_stocks() -> list:
    """
    è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
    """
    try:
        # è¿™é‡Œåº”è¯¥æœ‰è·å–è‚¡ç¥¨ä»£ç çš„å®ç°
        # ä¸ºç®€åŒ–ç¤ºä¾‹ï¼Œè¿”å›ä¸€ä¸ªç¤ºä¾‹åˆ—è¡¨
        return [f"00000{i:02d}" for i in range(3000)]
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

def get_stock_name(stock_code: str) -> str:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç è·å–è‚¡ç¥¨åç§°
    """
    try:
        # è¿™é‡Œåº”è¯¥æœ‰è·å–è‚¡ç¥¨åç§°çš„å®ç°
        return f"è‚¡ç¥¨{stock_code}"
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åç§°å¤±è´¥: {str(e)}", exc_info=True)
        return ""

def get_incremental_date_range(stock_code: str) -> tuple:
    """
    è·å–å¢é‡æ—¥æœŸèŒƒå›´
    """
    try:
        # è¿™é‡Œåº”è¯¥æœ‰è·å–å¢é‡æ—¥æœŸèŒƒå›´çš„å®ç°
        return (datetime.now() - timedelta(days=30), datetime.now())
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        return (None, None)

def load_progress() -> dict:
    """
    åŠ è½½çˆ¬å–è¿›åº¦
    """
    # è¿™é‡Œåº”è¯¥æœ‰åŠ è½½è¿›åº¦çš„å®ç°
    return {"next_index": 0}

def save_progress(etf_code: str, processed_count: int, total_count: int, next_index: int):
    """
    ä¿å­˜çˆ¬å–è¿›åº¦
    """
    # è¿™é‡Œåº”è¯¥æœ‰ä¿å­˜è¿›åº¦çš„å®ç°
    pass

if __name__ == "__main__":
    try:
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(os.path.join(Config.LOG_DIR, "stock_crawler.log"))
            ]
        )
        
        logger.info("===== å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼šcrawl_stock_daily =====")
        logger.info(f"UTCæ—¶é—´ï¼š{datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')}")
        
        crawl_all_etfs_daily_data()
        
        logger.info("===== ä»»åŠ¡æ‰§è¡Œç»“æŸï¼šsuccess =====")
    except Exception as e:
        logger.error(f"ETFæ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)
