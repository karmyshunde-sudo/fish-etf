#=====5æ•°æ®æºcrawler-QW11.py=====
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å–æ¨¡å— - å®Œå…¨éµå¾ªETFçˆ¬å–é€»è¾‘
ã€2025-11-15ï¼šå½»åº•è§£å†³Gitæäº¤é—®é¢˜ã€‘
- æ¯10åªè‚¡ç¥¨æ•°æ®ç¼“å­˜å¹¶æäº¤
- ä¸¥æ ¼éµå¾ªETFçˆ¬å–é€»è¾‘ï¼šæ•°æ®ç¼“å­˜ -> å°æ‰¹æ¬¡æäº¤ -> å…œåº•æäº¤ -> æœ€åæ›´æ–°è¿›åº¦
- ç¡®ä¿æ•°æ®æ–‡ä»¶å…ˆæäº¤ï¼Œè¿›åº¦æ–‡ä»¶åæ›´æ–°
"""

import os
import logging
import pandas as pd
import akshare as ak
import time
import random
import tempfile
import shutil
from datetime import datetime, timedelta
from config import Config
from utils.date_utils import is_trading_day, get_last_trading_day, get_beijing_time
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files, _verify_git_file_content
from stock.all_stocks import update_stock_list
from stock.stock_source import get_stock_daily_data_from_sources

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_stocks.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# æ‰¹æ¬¡å‚æ•°
MINOR_BATCH_SIZE = 10  # æ¯10åªè‚¡ç¥¨æäº¤ä¸€æ¬¡
BATCH_SIZE = 1         # å•æ¬¡è¿è¡Œå¤„ç†8åªè‚¡ç¥¨

def format_stock_code(code):
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½å­—ç¬¦ä¸²æ ¼å¼
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¯èƒ½åŒ…å«å‰ç¼€æˆ–é6ä½ï¼‰
    Returns:
        str: è§„èŒƒåŒ–çš„6ä½è‚¡ç¥¨ä»£ç 
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    code_str = str(code).strip().lower()
    
    # ç§»é™¤å¯èƒ½çš„å¸‚åœºå‰ç¼€
    if code_str.startswith(('sh', 'sz', 'hk', 'bj')):
        code_str = code_str[2:]
    
    # ç§»é™¤å¯èƒ½çš„ç‚¹å·ï¼ˆå¦‚"0.600022"ï¼‰
    if '.' in code_str:
        code_str = code_str.split('.')[1] if code_str.startswith('0.') else code_str
    
    # ç¡®ä¿æ˜¯6ä½æ•°å­—
    code_str = code_str.zfill(6)
    
    # éªŒè¯æ ¼å¼
    if not code_str.isdigit() or len(code_str) != 6:
        logger.warning(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {code_str}")
        return None
    
    return code_str

def get_next_crawl_index() -> int:
    """è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„è‚¡ç¥¨ç´¢å¼•"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return 0
        
        if not _verify_git_file_content(BASIC_INFO_FILE):
            logger.warning("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ è½½")
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•è·å–è¿›åº¦")
            return 0
        
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
            basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
            if not _verify_git_file_content(BASIC_INFO_FILE):
                logger.warning("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶å†…å®¹ä¸Gitä»“åº“ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°æäº¤")
            logger.info("å·²æ·»åŠ next_crawl_indexåˆ—å¹¶åˆå§‹åŒ–ä¸º0")
        
        next_index = int(basic_info_df["next_crawl_index"].iloc[0])
        logger.info(f"å½“å‰è¿›åº¦ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
        return next_index
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨è¿›åº¦ç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
        return 0

def save_crawl_progress(next_index: int):
    """ä¿å­˜è‚¡ç¥¨çˆ¬å–è¿›åº¦ - ä»…ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸æäº¤"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
            return
        
        if "next_crawl_index" not in basic_info_df.columns:
            basic_info_df["next_crawl_index"] = 0
        
        basic_info_df["next_crawl_index"] = next_index
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        logger.info(f"âœ… è¿›åº¦å·²ä¿å­˜ï¼šä¸‹ä¸€ä¸ªç´¢å¼•ä½ç½®: {next_index}/{len(basic_info_df)}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜è‚¡ç¥¨è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)

def commit_crawl_progress():
    """æäº¤è¿›åº¦æ–‡ä»¶åˆ°Gitä»“åº“"""
    try:
        commit_message = f"feat: æ›´æ–°è‚¡ç¥¨çˆ¬å–è¿›åº¦ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        success = commit_files_in_batches(BASIC_INFO_FILE, commit_message)
        if success:
            logger.info("âœ… è¿›åº¦æ–‡ä»¶å·²æäº¤åˆ°Gitä»“åº“")
        else:
            logger.error("âŒ è¿›åº¦æ–‡ä»¶æäº¤å¤±è´¥")
        return success
    except Exception as e:
        logger.error(f"âŒ æäº¤è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_all_stock_codes() -> list:
    """è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç """
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.info("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            if not update_stock_list():
                logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶åˆ›å»ºå¤±è´¥")
                return []
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return []
        
        if "ä»£ç " not in basic_info_df.columns:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ç¼ºå°‘'ä»£ç 'åˆ—")
            return []
        
        stock_codes = basic_info_df["ä»£ç "].tolist()
        logger.info(f"è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨ä»£ç ")
        return stock_codes
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return []

def normalize_stock_df(df: pd.DataFrame, stock_code: str) -> pd.DataFrame:
    """
    è§„èŒƒè‚¡ç¥¨æ—¥çº¿æ•°æ®ç»“æ„ä¸ç²¾åº¦
    """
    expected_columns = [
        "æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢",
        "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡", "è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°"
    ]

    # ç¼ºå°‘åˆ—è‡ªåŠ¨è¡¥0
    for col in expected_columns:
        if col not in df.columns:
            df[col] = 0

    # ç²¾åº¦å¤„ç†
    four_decimals = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
    for col in four_decimals:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").round(4)

    if "æˆäº¤é‡" in df.columns:
        df["æˆäº¤é‡"] = pd.to_numeric(df["æˆäº¤é‡"], errors="coerce").fillna(0).astype(int)

    df["è‚¡ç¥¨ä»£ç "] = stock_code
    df["è‚¡ç¥¨åç§°"] = get_stock_name(stock_code)
    
    df = df[expected_columns]
    df = df.sort_values(by="æ—¥æœŸ", ascending=True)
    return df

def get_stock_name(stock_code):
    """è·å–è‚¡ç¥¨åç§°"""
    try:
        if not os.path.exists(BASIC_INFO_FILE):
            logger.warning(f"è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {BASIC_INFO_FILE}")
            return stock_code
        
        basic_info_df = pd.read_csv(
            BASIC_INFO_FILE,
            dtype={"ä»£ç ": str}
        )
        
        if basic_info_df.empty:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸ºç©º")
            return stock_code
        
        if "ä»£ç " not in basic_info_df.columns or "åç§°" not in basic_info_df.columns:
            logger.error("è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            return stock_code
        
        stock_code_str = str(stock_code).strip()
        stock_row = basic_info_df[basic_info_df["ä»£ç "] == stock_code_str]
        
        if not stock_row.empty:
            return stock_row["åç§°"].values[0]
        
        logger.warning(f"è‚¡ç¥¨ {stock_code_str} ä¸åœ¨åˆ—è¡¨ä¸­")
        return stock_code
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åç§°å¤±è´¥: {str(e)}", exc_info=True)
        return stock_code

def get_incremental_date_range(stock_code: str) -> (datetime, datetime):
    """è·å–å¢é‡çˆ¬å–çš„æ—¥æœŸèŒƒå›´"""
    try:
        last_trading_day = get_last_trading_day()
        if not isinstance(last_trading_day, datetime):
            last_trading_day = datetime.now()
        
        if last_trading_day.tzinfo is None:
            last_trading_day = last_trading_day.replace(tzinfo=Config.BEIJING_TIMEZONE)
        
        end_date = last_trading_day
        current_time = get_beijing_time()
        
        if end_date > current_time:
            end_date = current_time
        
        while not is_trading_day(end_date.date()):
            end_date -= timedelta(days=1)
            if (last_trading_day - end_date).days > 30:
                logger.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥")
                return None, None
        
        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)
        
        save_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
        
        if os.path.exists(save_path):
            try:
                df = pd.read_csv(save_path)
                
                if "æ—¥æœŸ" not in df.columns:
                    logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®æ–‡ä»¶ç¼ºå°‘'æ—¥æœŸ'åˆ—")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
                valid_dates = df["æ—¥æœŸ"].dropna()
                if valid_dates.empty:
                    logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®æ–‡ä»¶ä¸­æ—¥æœŸåˆ—å…¨ä¸ºNaN")
                    start_date = last_trading_day - timedelta(days=365)
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    return start_date, end_date
                
                latest_date = valid_dates.max()
                if not isinstance(latest_date, datetime):
                    latest_date = pd.to_datetime(latest_date)
                
                if latest_date.tzinfo is None:
                    latest_date = latest_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                
                latest_date_date = latest_date.date()
                end_date_date = end_date.date()
                
                logger.debug(f"è‚¡ç¥¨ {stock_code} æ—¥æœŸæ¯”è¾ƒ: æœ€æ–°æ—¥æœŸ={latest_date_date}, ç»“æŸæ—¥æœŸ={end_date_date}")
                
                if latest_date_date < end_date_date:
                    start_date = latest_date + timedelta(days=1)
                    
                    while not is_trading_day(start_date.date()):
                        start_date += timedelta(days=1)
                    
                    if start_date.tzinfo is None:
                        start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                    
                    if start_date > end_date:
                        logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®å·²æœ€æ–°ï¼ˆæœ€æ–°æ—¥æœŸ={latest_date_date}ï¼Œç»“æŸæ—¥æœŸ={end_date_date}ï¼‰")
                        return None, None
                    
                    logger.info(f"è‚¡ç¥¨ {stock_code} éœ€è¦æ›´æ–°æ•°æ®: æœ€æ–°æ—¥æœŸ {latest_date_date} < ç»“æŸæ—¥æœŸ {end_date_date}")
                    logger.info(f"è‚¡ç¥¨ {stock_code} å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
                    return start_date, end_date
                else:
                    logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®å·²æœ€æ–°: æœ€æ–°æ—¥æœŸ {latest_date_date} >= ç»“æŸæ—¥æœŸ {end_date_date}")
                    return None, None
            
            except Exception as e:
                logger.error(f"è¯»å–è‚¡ç¥¨ {stock_code} æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
                start_date = last_trading_day - timedelta(days=365)
                if start_date.tzinfo is None:
                    start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
                return start_date, end_date
        else:
            logger.info(f"è‚¡ç¥¨ {stock_code} æ— å†å²æ•°æ®ï¼Œå°†è·å–ä¸€å¹´å†å²æ•°æ®")
            start_date = last_trading_day - timedelta(days=365)
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
            return start_date, end_date
    
    except Exception as e:
        logger.error(f"è·å–å¢é‡æ—¥æœŸèŒƒå›´å¤±è´¥: {str(e)}", exc_info=True)
        last_trading_day = get_last_trading_day()
        start_date = last_trading_day - timedelta(days=365)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=Config.BEIJING_TIMEZONE)
        end_date = last_trading_day.replace(hour=23, minute=59, second=59, microsecond=0)
        return start_date, end_date

def save_stock_data_batch(stock_data_dict: dict) -> int:
    """
    æ‰¹é‡ä¿å­˜è‚¡ç¥¨æ—¥çº¿æ•°æ®
    """
    if not stock_data_dict:
        return 0

    os.makedirs(DAILY_DIR, exist_ok=True)
    saved_count = 0

    for stock_code, df in stock_data_dict.items():
        if df.empty:
            continue

        save_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")

        # ä¿å­˜å‰è§„èŒƒåŒ–æ•°æ®ç»“æ„ä¸ç²¾åº¦
        stock_name = df["è‚¡ç¥¨åç§°"].iloc[0] if "è‚¡ç¥¨åç§°" in df.columns else get_stock_name(stock_code)
        df = normalize_stock_df(df, stock_code)

        try:
            # =============================
            # Step 1: è¯»å–å·²æœ‰æ•°æ®ï¼ˆå¦‚å­˜åœ¨ï¼‰
            # =============================
            if os.path.exists(save_path):
                existing_df = pd.read_csv(save_path)

                # ç¡®ä¿æ—§æ•°æ®çš„"æ—¥æœŸ"åˆ—ç»Ÿä¸€ä¸ºdatetimeæ ¼å¼
                if "æ—¥æœŸ" in existing_df.columns:
                    existing_df["æ—¥æœŸ"] = pd.to_datetime(existing_df["æ—¥æœŸ"], errors="coerce")

                # ç¡®ä¿æ–°æ•°æ®çš„"æ—¥æœŸ"åˆ—ä¹Ÿæ˜¯datetimeæ ¼å¼
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")

                # =============================
                # Step 2: åˆå¹¶æ•°æ®
                # =============================
                combined_df = pd.concat([existing_df, df], ignore_index=True)

                # å†æ¬¡ç»Ÿä¸€æ—¥æœŸåˆ—ç±»å‹
                combined_df["æ—¥æœŸ"] = pd.to_datetime(combined_df["æ—¥æœŸ"], errors="coerce")

                # ä¸¢å¼ƒæ— æ•ˆæ—¥æœŸ
                invalid_dates = combined_df["æ—¥æœŸ"].isna().sum()
                if invalid_dates > 0:
                    logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} åˆå¹¶åå‘ç° {invalid_dates} æ¡æ— æ•ˆæ—¥æœŸè®°å½•ï¼Œå·²è¿‡æ»¤")
                    combined_df = combined_df.dropna(subset=["æ—¥æœŸ"])

                # =============================
                # Step 3: å»é‡ + æ’åº
                # =============================
                combined_df = combined_df.drop_duplicates(subset=["æ—¥æœŸ"], keep="last")
                combined_df = combined_df.sort_values("æ—¥æœŸ", ascending=True).reset_index(drop=True)

                # =============================
                # Step 4: æ ¼å¼åŒ–æ—¥æœŸåˆ—ä¸ºå­—ç¬¦ä¸²ä¿å­˜
                # =============================
                combined_df["æ—¥æœŸ"] = combined_df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")

                # =============================
                # Step 5: ä¸´æ—¶æ–‡ä»¶å®‰å…¨å†™å…¥
                # =============================
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    combined_df.to_csv(temp_file.name, index=False)

                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²åˆå¹¶è‡³: {save_path} (å…±{len(combined_df)}æ¡)")

            else:
                # =============================
                # æ— æ—§æ•°æ®ï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®
                # =============================
                if "æ—¥æœŸ" in df.columns:
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                    df = df.dropna(subset=["æ—¥æœŸ"])
                    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")

                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8-sig') as temp_file:
                    df.to_csv(temp_file.name, index=False)

                shutil.move(temp_file.name, save_path)
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜è‡³: {save_path} ({len(df)}æ¡)")

            saved_count += 1

        except Exception as e:
            logger.error(f"ä¿å­˜è‚¡ç¥¨ {stock_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)

    return saved_count

def crawl_all_stocks_daily_data():
    """çˆ¬å–æ‰€æœ‰è‚¡ç¥¨æ—¥çº¿æ•°æ®"""
    try:
        logger.info("=== å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å– ===")
        beijing_time = get_beijing_time()
        logger.info(f"åŒ—äº¬æ—¶é—´ï¼š{beijing_time.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆUTC+8ï¼‰")
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(DAILY_DIR, exist_ok=True)
        logger.info(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {DATA_DIR}")
        
        stock_codes = get_all_stock_codes()
        total_count = len(stock_codes)
        
        if total_count == 0:
            logger.error("è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œçˆ¬å–")
            return
        
        logger.info(f"å¾…çˆ¬å–è‚¡ç¥¨æ€»æ•°ï¼š{total_count}åªï¼ˆå…¨å¸‚åœºè‚¡ç¥¨ï¼‰")
        
        next_index = get_next_crawl_index()
        total_to_process = min(BATCH_SIZE, total_count - next_index)
        
        if total_to_process <= 0:
            logger.info("æ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†")
            return
            
        logger.info(f"æœ¬æ¬¡å°†å¤„ç† {total_to_process} åªè‚¡ç¥¨ï¼ˆç›®æ ‡ï¼š{BATCH_SIZE}åªï¼‰")
        
        start_idx = next_index % total_count
        stock_data_dict = {}  # å°æ‰¹æ¬¡æ•°æ®ç¼“å­˜
        processed_count = 0
        successful_count = 0
        failed_stocks = []
        
        # å¤„ç†æ‰€æœ‰è‚¡ç¥¨
        for i in range(total_to_process):
            current_index = (start_idx + i) % total_count
            stock_code = stock_codes[current_index]
            stock_name = get_stock_name(stock_code)
            logger.info(f"è‚¡ç¥¨ä»£ç ï¼š{stock_code}| åç§°ï¼š{stock_name}")
            
            # è·å–å¢é‡æ—¥æœŸèŒƒå›´
            start_date, end_date = get_incremental_date_range(stock_code)
            if start_date is None or end_date is None:
                logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®å·²æœ€æ–°ï¼Œè·³è¿‡çˆ¬å–")
                processed_count += 1
                continue
            
            # çˆ¬å–æ•°æ®
            logger.info(f"ğŸ“… å¢é‡çˆ¬å–æ—¥æœŸèŒƒå›´ï¼š{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            df = get_stock_daily_data_from_sources(
                stock_code=stock_code,
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°æ•°æ®")
                failed_stocks.append(f"{stock_code},{stock_name},æœªè·å–åˆ°æ•°æ®")
                processed_count += 1
                continue
            
            # ç¼“å­˜åˆ°å°æ‰¹æ¬¡
            stock_data_dict[stock_code] = df
            successful_count += 1
            processed_count += 1
            
            current_progress = f"{current_index}/{total_count} ({(current_index)/total_count*100:.1f}%)"
            logger.info(f"è¿›åº¦: {current_progress} - æ•°æ®å·²ç¼“å­˜")
            
            # æ¯10åªè‚¡ç¥¨æäº¤ä¸€æ¬¡ï¼ˆåŒ…æ‹¬æœ€åä¸€åªï¼‰
            if (i + 1) % MINOR_BATCH_SIZE == 0 or i == total_to_process - 1:
                # ä¿å­˜å½“å‰å°æ‰¹æ¬¡æ•°æ®
                if stock_data_dict:
                    logger.info(f"å¼€å§‹ä¿å­˜å°æ‰¹æ¬¡æ•°æ®ï¼ˆ{len(stock_data_dict)}åªè‚¡ç¥¨ï¼‰...")
                    saved_count = save_stock_data_batch(stock_data_dict)
                    logger.info(f"âœ… å°æ‰¹æ¬¡æ•°æ®ä¿å­˜å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {saved_count} ä¸ªè‚¡ç¥¨æ•°æ®æ–‡ä»¶")
                    
                    # é‡æ–°æ·»åŠ ï¼šç¡®ä¿æ•°æ®æ–‡ä»¶è¢«æ·»åŠ åˆ°Gitæš‚å­˜åŒº
                    os.system("git add data/daily/*.csv")
                    
                    # æ„å»ºè¦æäº¤çš„æ–‡ä»¶åˆ—è¡¨
                    file_list = [os.path.join(DAILY_DIR, f"{code}.csv") for code in stock_data_dict.keys()]
                    # æäº¤æ•°æ®æ–‡ä»¶
                    commit_msg = f"feat: æ‰¹é‡æäº¤{len(stock_data_dict)}åªè‚¡ç¥¨æ—¥çº¿æ•°æ® [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
                    logger.info(f"æäº¤æ•°æ®æ–‡ä»¶: {commit_msg}")
                    commit_success = commit_files_in_batches(file_list, commit_msg)
                    
                    if commit_success:
                        logger.info(f"âœ… å°æ‰¹æ¬¡æ•°æ®æ–‡ä»¶æäº¤æˆåŠŸï¼š{len(stock_data_dict)}åª")
                    else:
                        logger.error("âŒ å°æ‰¹æ¬¡æ•°æ®æ–‡ä»¶æäº¤å¤±è´¥")
                    
                    # æ›´æ–°è¿›åº¦ï¼ˆå½“å‰å·²å¤„ç†æ•°é‡ï¼‰
                    new_index = start_idx + i + 1
                    new_index = new_index % total_count
                    save_crawl_progress(new_index)
                    logger.info(f"âœ… è¿›åº¦å·²æ›´æ–°ä¸º {new_index}/{total_count}")
                    
                    # æäº¤è¿›åº¦æ–‡ä»¶
                    progress_commit_success = commit_crawl_progress()
                    if progress_commit_success:
                        logger.info(f"âœ… è¿›åº¦æ–‡ä»¶æäº¤æˆåŠŸï¼Œè¿›åº¦æ›´æ–°ä¸º {new_index}/{total_count}")
                    else:
                        logger.error("âŒ è¿›åº¦æ–‡ä»¶æäº¤å¤±è´¥")
                    
                    # æ¸…ç©ºå°æ‰¹æ¬¡ç¼“å­˜
                    stock_data_dict = {}
                else:
                    logger.info("å½“å‰å°æ‰¹æ¬¡æ²¡æœ‰æ–°æ•°æ®ï¼Œè·³è¿‡æäº¤")
            
            # æ¯åªè‚¡ç¥¨ä¹‹é—´éšæœºç­‰å¾…
            time.sleep(random.uniform(1.2, 4.6))
        
        # å¤„ç†ç»“æŸåè®°å½•å¤±è´¥è‚¡ç¥¨
        if failed_stocks:
            failed_file = os.path.join(DAILY_DIR, "failed_stocks.txt")
            with open(failed_file, "w", encoding="utf-8") as f:
                f.write("\n".join(failed_stocks))
            logger.info(f"è®°å½•äº† {len(failed_stocks)} åªå¤±è´¥çš„è‚¡ç¥¨")
        
        # è®¡ç®—å‰©ä½™è‚¡ç¥¨æ•°é‡
        remaining_stocks = total_count - (start_idx + total_to_process)
        if remaining_stocks < 0:
            remaining_stocks = total_count + remaining_stocks
            
        logger.info(f"æœ¬æ¬¡çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} åªè‚¡ç¥¨ï¼ŒæˆåŠŸ {successful_count} åªï¼Œå¤±è´¥ {len(failed_stocks)} åª")
        logger.info(f"è¿˜æœ‰ {remaining_stocks} åªè‚¡ç¥¨å¾…çˆ¬å–")
        
    except Exception as e:
        logger.error(f"è‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        # å¼‚å¸¸æƒ…å†µä¸‹å°è¯•ä¿å­˜è¿›åº¦
        try:
            if 'next_index' in locals() and 'total_count' in locals():
                new_index = start_idx + i + 1 if 'i' in locals() else next_index
                new_index = new_index % total_count
                logger.error("å°è¯•ä¿å­˜è¿›åº¦ä»¥æ¢å¤çŠ¶æ€...")
                save_crawl_progress(new_index)
                commit_crawl_progress()
                logger.info(f"è¿›åº¦å·²ä¿å­˜ä¸º {new_index}/{total_count}")
        except Exception as save_error:
            logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹ä¿å­˜è¿›åº¦å¤±è´¥: {str(save_error)}", exc_info=True)
        raise

def main():
    """ä¸»å‡½æ•°"""
    logger.info("===== å¼€å§‹æ›´æ–°è‚¡ç¥¨æ•°æ® =====")
    
    # ç¡®ä¿åŸºç¡€ä¿¡æ¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(BASIC_INFO_FILE) or os.path.getsize(BASIC_INFO_FILE) == 0:
        logger.info("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ­£åœ¨åˆ›å»º...")
        if not update_stock_list():
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
    
    # æ‰§è¡Œçˆ¬å–
    try:
        crawl_all_stocks_daily_data()
        logger.info("âœ… è‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å–å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ è‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}", exc_info=True)
        # å‘é€é”™è¯¯é€šçŸ¥
        try:
            from wechat_push.push import send_wechat_message
            send_wechat_message(
                message=f"è‚¡ç¥¨æ—¥çº¿æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}",
                message_type="error"
            )
        except:
            pass
    
    logger.info("===== è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ =====")

if __name__ == "__main__":
    main()
