#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®çˆ¬å–æ¨¡å— - ä¸¥æ ¼ç¡®ä¿è‚¡ç¥¨ä»£ç ä¸º6ä½æ ¼å¼ï¼Œæ—¥æœŸå¤„ç†é€»è¾‘å®Œå–„
ã€2025-10-14-0836ï¼šå¾ªç¯ç´¢å¼•ï¼Œä¿è¯æ¯æ¬¡éƒ½æ˜¯çˆ¬å–150åªè‚¡ç¥¨ã€‘
ã€2025-11-14ï¼šçˆ¬å–æ•°æ®æºé¢å¤–æ–°ç¼–å†™ä»£ç ã€‘
- å½»åº•è§£å†³Gitæäº¤é—®é¢˜
- å¾ªç¯æ‰¹å¤„ç†æœºåˆ¶ï¼ˆå¯é…ç½®æ‰¹æ¬¡å¤§å°ï¼‰
- ä¸“ä¸šé‡‘èç³»ç»Ÿå¯é æ€§ä¿éšœ
- 100%å¯ç›´æ¥å¤åˆ¶ä½¿ç”¨
"""

import os
import logging
import pandas as pd
import akshare as ak
import time
import random
import json
from datetime import datetime, timedelta, date
import subprocess  # æ–°å¢ï¼šç”¨äºç›´æ¥æ‰§è¡Œgitå‘½ä»¤
from config import Config
from utils.date_utils import is_trading_day, get_last_trading_day, get_beijing_time
from utils.git_utils import commit_files_in_batches, force_commit_remaining_files
# å¯¼å…¥è‚¡ç¥¨åˆ—è¡¨æ›´æ–°æ¨¡å—
from stock.all_stocks import update_stock_list
# æ–°å¢ï¼šå¯¼å…¥æ•°æ®æºæ¨¡å—ï¼ˆæ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼‰
from stock.stock_source import get_stock_daily_data_from_sources

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR = Config.DATA_DIR
DAILY_DIR = os.path.join(DATA_DIR, "daily")
BASIC_INFO_FILE = os.path.join(DATA_DIR, "all_stocks.csv")
LOG_DIR = os.path.join(DATA_DIR, "logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# ã€å…³é”®å‚æ•°ã€‘å¯åœ¨æ­¤å¤„ä¿®æ”¹æ¯æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
# ä¸“ä¸šä¿®å¤ï¼šæ‰¹æ¬¡å¤§å°ä½œä¸ºå¯é…ç½®å‚æ•°
BATCH_SIZE = 8  # å¯æ ¹æ®éœ€è¦è°ƒæ•´ä¸º100ã€150ã€200ç­‰
MINOR_BATCH_SIZE = 10  # æ¯10åªè‚¡ç¥¨æäº¤ä¸€æ¬¡
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# ===== æ–°å¢ï¼šGitå†²çªå¤„ç†å‡½æ•° =====
def handle_git_conflicts(repo_root):
    """å¤„ç†Gitå†²çªï¼Œä¸ä¾èµ–git_utilså†…éƒ¨å®ç°"""
    try:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†²çª
        status_cmd = ['git', 'status', '--porcelain']
        status_result = subprocess.run(
            status_cmd,
            capture_output=True,
            text=True,
            cwd=repo_root,
            check=True
        )
        
        # å¦‚æœæ£€æµ‹åˆ°unmerged pathsï¼ˆUUå¼€å¤´ï¼‰æˆ–ä¸èƒ½pull with rebaseçš„é—®é¢˜
        if "UU" in status_result.stdout or "cannot pull with rebase" in status_result.stdout:
            logger.warning("æ£€æµ‹åˆ°Gitå†²çªï¼Œæ­£åœ¨å¤„ç†...")
            
            # æ–¹æ¡ˆ1ï¼šå°è¯•é‡ç½®å¹¶æ¸…ç†å·¥ä½œåŒº
            try:
                logger.info("å°è¯•é‡ç½®å·¥ä½œåŒº...")
                subprocess.run(['git', 'reset', '--hard'], cwd=repo_root, check=True)
                subprocess.run(['git', 'clean', '-fd'], cwd=repo_root, check=True)
                logger.info("âœ… å·¥ä½œåŒºå·²é‡ç½®")
                return True
            except subprocess.CalledProcessError as e1:
                logger.error(f"é‡ç½®å·¥ä½œåŒºå¤±è´¥: {str(e1)}")
            
            # æ–¹æ¡ˆ2ï¼šå°è¯•stashå’Œpop
            try:
                logger.info("å°è¯•stashä¿å­˜æ›´æ”¹...")
                subprocess.run(['git', 'stash', 'push', '-m', 'auto-stash-before-conflict-resolution'], 
                              cwd=repo_root, check=True)
                logger.info("âœ… æ›´æ”¹å·²æš‚å­˜ï¼Œå°è¯•æ‹‰å–æœ€æ–°ä»£ç ")
                
                # æ‹‰å–æœ€æ–°ä»£ç 
                subprocess.run(['git', 'pull', 'origin', 'main', '--no-rebase'], 
                              cwd=repo_root, check=True)
                
                # å°è¯•æ¢å¤æš‚å­˜
                try:
                    logger.info("å°è¯•æ¢å¤æš‚å­˜çš„æ›´æ”¹...")
                    subprocess.run(['git', 'stash', 'pop'], cwd=repo_root, check=True)
                    logger.info("âœ… æš‚å­˜çš„æ›´æ”¹å·²æˆåŠŸæ¢å¤")
                    return True
                except subprocess.CalledProcessError as e2:
                    logger.warning(f"æ¢å¤æš‚å­˜æ›´æ”¹å¤±è´¥: {str(e2)}")
                    # å¼ºåˆ¶åˆ é™¤æš‚å­˜
                    subprocess.run(['git', 'stash', 'drop'], cwd=repo_root, check=False)
                    logger.info("âš ï¸ å·²ä¸¢å¼ƒæš‚å­˜çš„æ›´æ”¹")
            except subprocess.CalledProcessError as e3:
                logger.error(f"stashå¤„ç†å¤±è´¥: {str(e3)}")
            
            # æ–¹æ¡ˆ3ï¼šå¼ºåˆ¶æ¥å—è¿œç¨‹æ›´æ”¹
            try:
                logger.warning("å¼ºåˆ¶æ¥å—è¿œç¨‹æ›´æ”¹...")
                subprocess.run(['git', 'fetch', 'origin'], cwd=repo_root, check=True)
                subprocess.run(['git', 'reset', '--hard', 'origin/main'], cwd=repo_root, check=True)
                logger.info("âœ… å·²å¼ºåˆ¶åŒæ­¥åˆ°è¿œç¨‹ä»“åº“")
                return True
            except subprocess.CalledProcessError as e4:
                logger.error(f"å¼ºåˆ¶åŒæ­¥å¤±è´¥: {str(e4)}")
        
        return True
    except Exception as e:
        logger.error(f"å¤„ç†Gitå†²çªæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        return False

# ===== æ–°å¢ï¼šéªŒè¯æ–‡ä»¶æ˜¯å¦çœŸæ­£æäº¤ =====
def verify_file_commit(file_path):
    """éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸæäº¤åˆ°è¿œç¨‹ä»“åº“"""
    try:
        repo_root = os.environ.get('GITHUB_WORKSPACE', os.getcwd())
        relative_path = os.path.relpath(file_path, repo_root)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æš‚å­˜åŒº
        diff_cached = subprocess.run(
            ['git', 'diff', '--cached', '--name-only', relative_path],
            capture_output=True,
            text=True,
            cwd=repo_root
        )
        
        if diff_cached.stdout.strip():
            logger.warning(f"æ–‡ä»¶ {relative_path} ä»åœ¨æš‚å­˜åŒºï¼Œæœªæäº¤æˆåŠŸ")
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘ä¸€æ¬¡æäº¤ä¸­
        log_check = subprocess.run(
            ['git', 'log', '-1', '--name-only', '--pretty=format:', '--', relative_path],
            capture_output=True,
            text=True,
            cwd=repo_root
        )
        
        if not log_check.stdout.strip():
            logger.warning(f"æ–‡ä»¶ {relative_path} ä¸åœ¨æœ€è¿‘æäº¤ä¸­")
            return False
        
        logger.info(f"âœ… æ–‡ä»¶ {relative_path} å·²æˆåŠŸæäº¤")
        return True
    except Exception as e:
        logger.error(f"éªŒè¯æ–‡ä»¶æäº¤çŠ¶æ€å¤±è´¥: {str(e)}")
        return False

# ===== æ–°å¢ï¼šå¢å¼ºç‰ˆæäº¤å‡½æ•° =====
def commit_batch_files(file_paths, commit_message):
    """å¢å¼ºç‰ˆæ‰¹é‡æäº¤å‡½æ•°ï¼ŒåŒ…å«è¯¦ç»†ç»“æœæ—¥å¿—"""
    try:
        # 1. å¤„ç†Gitå†²çª
        repo_root = os.environ.get('GITHUB_WORKSPACE', os.getcwd())
        if not handle_git_conflicts(repo_root):
            logger.error("âŒ Gitå†²çªå¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æäº¤")
            return False
        
        # 2. æ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº
        for file_path in file_paths:
            if os.path.exists(file_path):
                relative_path = os.path.relpath(file_path, repo_root)
                try:
                    subprocess.run(['git', 'add', relative_path], cwd=repo_root, check=True)
                    logger.debug(f"âœ… æ–‡ä»¶å·²æ·»åŠ åˆ°æš‚å­˜åŒº: {relative_path}")
                except subprocess.CalledProcessError as e:
                    logger.error(f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥ {relative_path}: {str(e)}")
        
        # 3. æ‰§è¡Œæäº¤
        logger.info(f"ğŸ“¤ å¼€å§‹æäº¤: {commit_message}")
        commit_success = commit_files_in_batches(file_paths, commit_message)
        
        # 4. è¯¦ç»†è®°å½•æäº¤ç»“æœ
        if commit_success:
            logger.info(f"âœ… æ‰¹é‡æäº¤æˆåŠŸ: {len(file_paths)} ä¸ªæ–‡ä»¶")
            # éªŒè¯æ¯ä¸ªæ–‡ä»¶æ˜¯å¦çœŸæ­£æäº¤
            all_verified = True
            for file_path in file_paths:
                if not verify_file_commit(file_path):
                    all_verified = False
                    logger.warning(f"âš ï¸ æ–‡ä»¶æäº¤éªŒè¯å¤±è´¥: {os.path.basename(file_path)}")
            
            if all_verified:
                logger.info("âœ… æ‰€æœ‰æ–‡ä»¶æäº¤éªŒè¯é€šè¿‡")
            else:
                logger.warning("âš ï¸ éƒ¨åˆ†æ–‡ä»¶æäº¤éªŒè¯å¤±è´¥")
            
            return True
        else:
            logger.error(f"âŒ æ‰¹é‡æäº¤å¤±è´¥: {len(file_paths)} ä¸ªæ–‡ä»¶")
            return False
            
    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        return False

def ensure_directory_exists():
    """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
    if not os.path.exists(DAILY_DIR):
        os.makedirs(DAILY_DIR)
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

def format_stock_code(code):
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½å­—ç¬¦ä¸²æ ¼å¼
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¯èƒ½åŒ…å«å‰ç¼€æˆ–é6ä½ï¼‰
    Returns:
        str: è§„èŒƒåŒ–çš„6ä½è‚¡ç¥¨ä»£ç 
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    code_str = str(code).strip().lower()
    
    # ç§»é™¤å¯èƒ½çš„å¸‚åœºå‰ç¼€
    if code_str.startswith(('sh', 'sz', 'hk', 'bj')):
        code_str = code_str[2:]
    
    # ç§»é™¤å¯èƒ½çš„ç‚¹å·ï¼ˆå¦‚"0.600022"ï¼‰
    if '.' in code_str:
        code_str = code_str.split('.')[1] if code_str.startswith('0.') else code_str
    
    # ç¡®ä¿æ˜¯6ä½æ•°å­—
    code_str = code_str.zfill(6)
    
    # éªŒè¯æ ¼å¼
    if not code_str.isdigit() or len(code_str) != 6:
        logger.warning(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {code_str}")
        return None
    
    return code_str

def get_stock_section(stock_code: str) -> str:
    """
    è·å–è‚¡ç¥¨æ‰€å±æ¿å—
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå·²æ ¼å¼åŒ–ä¸º6ä½ï¼‰
    
    Returns:
        str: æ¿å—åç§°
    """
    # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½
    stock_code = format_stock_code(stock_code)
    if not stock_code:
        return "æ ¼å¼é”™è¯¯"
    
    # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€åˆ¤æ–­æ¿å—
    if stock_code.startswith('60'):
        return "æ²ªå¸‚ä¸»æ¿"
    elif stock_code.startswith('00'):
        return "æ·±å¸‚ä¸»æ¿"
    elif stock_code.startswith('30'):
        return "åˆ›ä¸šæ¿"
    elif stock_code.startswith('688'):
        return "ç§‘åˆ›æ¿"
    elif stock_code.startswith('8'):
        return "åŒ—äº¤æ‰€"
    elif stock_code.startswith('4') or stock_code.startswith('8'):
        return "ä¸‰æ¿å¸‚åœº"
    else:
        return "å…¶ä»–æ¿å—"

def to_naive_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºnaive datetimeï¼ˆæ— æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: naive datetime
    """
    if dt is None:
        return None
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

def to_aware_datetime(dt):
    """
    å°†æ—¥æœŸè½¬æ¢ä¸ºaware datetimeï¼ˆæœ‰æ—¶åŒºï¼‰
    Args:
        dt: å¯èƒ½æ˜¯naiveæˆ–aware datetime
    Returns:
        datetime: aware datetimeï¼ˆåŒ—äº¬æ—¶åŒºï¼‰
    """
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=Config.BEIJING_TIMEZONE)
    return dt

def get_valid_trading_date_range(start_date, end_date):
    """
    è·å–æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´ï¼Œç¡®ä¿åªåŒ…å«å†å²äº¤æ˜“æ—¥
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯èƒ½åŒ…å«éäº¤æ˜“æ—¥ï¼‰
    
    Returns:
        tuple: (valid_start_date, valid_end_date) - æœ‰æ•ˆçš„äº¤æ˜“æ—¥èŒƒå›´
    """
    # ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    start_date = to_datetime(start_date)
    end_date = to_datetime(end_date)
    
    if start_date is None or end_date is None:
        logger.error("æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥")
        return None, None
    
    # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
    now = get_beijing_time()
    # ç¡®ä¿ä¸¤ä¸ªæ—¥æœŸå¯¹è±¡ç±»å‹ä¸€è‡´
    end_date = to_aware_datetime(end_date)
    now = to_aware_datetime(now)
    
    if end_date > now:
        end_date = now
        logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥
    valid_end_date = end_date
    days_back = 0
    while days_back < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_end_date.date()):
            break
        valid_end_date -= timedelta(days=1)
        days_back += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼Œè¿”å›ç©ºèŒƒå›´
    if days_back >= 30:
        logger.warning(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç»“æŸäº¤æ˜“æ—¥ï¼ˆä» {end_date.strftime('%Y-%m-%d')} å¼€å§‹ï¼‰")
        return None, None
    
    # æŸ¥æ‰¾æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥
    valid_start_date = start_date
    days_forward = 0
    while days_forward < 30:  # æœ€å¤šæŸ¥æ‰¾30å¤©
        if is_trading_day(valid_start_date.date()):
            break
        valid_start_date += timedelta(days=1)
        days_forward += 1
    
    # å¦‚æœæ‰¾ä¸åˆ°æœ‰æ•ˆçš„èµ·å§‹äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ç»“æŸäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹æ—¥
    if days_forward >= 30:
        valid_start_date = valid_end_date
    
    # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰ç±»å‹ä¸€è‡´
    start_naive = to_naive_datetime(valid_start_date)
    end_naive = to_naive_datetime(valid_end_date)
    
    if start_naive > end_naive:
        valid_start_date = valid_end_date
    
    return valid_start_date, valid_end_date

def to_datetime(date_input):
    """
    ç»Ÿä¸€è½¬æ¢ä¸ºdatetime.datetimeç±»å‹
    Args:
        date_input: æ—¥æœŸè¾“å…¥ï¼Œå¯ä»¥æ˜¯strã€dateã€datetimeç­‰ç±»å‹
    Returns:
        datetime.datetime: ç»Ÿä¸€çš„datetimeç±»å‹
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ["%Y-%m-%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S"]:
            try:
                return datetime.strptime(date_input, fmt)
            except:
                continue
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_input}")
        return None
    return None

def fetch_stock_daily_data(stock_code: str) -> pd.DataFrame:
    """è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å"""
    try:
        # ===== 1. åŸºç¡€æ£€æŸ¥ä¸æ—¥æœŸèŒƒå›´ =====
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            logger.error(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {stock_code}")
            return pd.DataFrame()
        
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½ï¼ˆå‰é¢è¡¥é›¶ï¼‰
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            logger.error(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {stock_code}")
            return pd.DataFrame()
        
        # ===== 2. è·å–æ—¥æœŸèŒƒå›´ =====
        local_file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
        existing_data = None
        last_date = None
        
        if os.path.exists(local_file_path):
            try:
                # è¯»å–å·²æœ‰çš„æ•°æ®
                existing_data = pd.read_csv(local_file_path)
                if not existing_data.empty and 'æ—¥æœŸ' in existing_data.columns:
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
                    existing_data['æ—¥æœŸ'] = pd.to_datetime(existing_data['æ—¥æœŸ'], errors='coerce')
                    # è·å–æœ€åä¸€æ¡æ•°æ®çš„æ—¥æœŸ
                    last_date = existing_data['æ—¥æœŸ'].max()
                    if pd.notna(last_date):
                        logger.info(f"è‚¡ç¥¨ {stock_code} æœ¬åœ°å·²æœ‰æ•°æ®ï¼Œæœ€åæ—¥æœŸ: {last_date.strftime('%Y-%m-%d')}")
                    else:
                        last_date = None
            except Exception as e:
                logger.warning(f"è¯»å–è‚¡ç¥¨ {stock_code} æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")
                existing_data = None
                last_date = None
        
        # ===== 3. ç¡®å®šçˆ¬å–çš„æ—¥æœŸèŒƒå›´ =====
        if last_date is not None:
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºèµ·å§‹ç‚¹
            current_date = last_date + timedelta(days=1)
            start_date = None
            
            # æœ€å¤šæŸ¥æ‰¾30å¤©ï¼Œé¿å…æ— é™å¾ªç¯
            for i in range(30):
                if is_trading_day(current_date.date()):
                    start_date = current_date
                    break
                current_date += timedelta(days=1)
            
            if not start_date:
                # å¦‚æœæ‰¾ä¸åˆ°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
                last_trading_date = get_last_trading_day()
                if last_trading_date:
                    # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿last_trading_dateæ˜¯datetimeç±»å‹
                    if not isinstance(last_trading_date, datetime):
                        last_trading_date = datetime.combine(last_trading_date, datetime.min.time())
                    start_date = last_trading_date
                    logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥: {start_date.strftime('%Y%m%d')}")
                else:
                    logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æœ‰æ•ˆäº¤æ˜“æ—¥ï¼Œè·³è¿‡çˆ¬å–")
                    return pd.DataFrame()
            
            # è·å–å½“å‰æ—¥æœŸå‰çš„æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
            end_date = get_last_trading_day()
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿end_dateæ˜¯datetimeç±»å‹
            if not isinstance(end_date, datetime):
                end_date = datetime.combine(end_date, datetime.min.time())
            
            # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºå½“å‰æ—¶é—´
            now = get_beijing_time()
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            now_naive = to_naive_datetime(now)
            end_date_naive = to_naive_datetime(end_date)
            
            if end_date_naive > now_naive:
                end_date = now
                logger.warning(f"ç»“æŸæ—¥æœŸæ™šäºå½“å‰æ—¶é—´ï¼Œå·²è°ƒæ•´ä¸ºå½“å‰æ—¶é—´: {end_date.strftime('%Y%m%d %H:%M:%S')}")
            
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            # è½¬æ¢ä¸ºnaive datetimeè¿›è¡Œæ¯”è¾ƒ
            start_date_naive = to_naive_datetime(start_date)
            end_date_naive = to_naive_datetime(end_date)
            
            # ä¸¥æ ¼æ£€æŸ¥æ—¥æœŸ
            if start_date_naive > end_date_naive:
                logger.info(f"è‚¡ç¥¨ {stock_code} æ²¡æœ‰æ–°æ•°æ®éœ€è¦çˆ¬å–ï¼ˆå¼€å§‹æ—¥æœŸ: {start_date.strftime('%Y%m%d')} > ç»“æŸæ—¥æœŸ: {end_date.strftime('%Y%m%d')}ï¼‰")
                return pd.DataFrame()
            
            # å¤„ç†å¼€å§‹æ—¥æœŸç­‰äºç»“æŸæ—¥æœŸçš„æƒ…å†µ
            if start_date_naive == end_date_naive:
                beijing_time = get_beijing_time()
                # Aè‚¡æ”¶å¸‚æ—¶é—´ä¸º15:00ï¼Œä¸ºä¿é™©èµ·è§ï¼Œ15:30åè®¤ä¸ºå½“å¤©æ•°æ®å·²æ›´æ–°
                market_close_time = start_date_naive.replace(hour=15, minute=30, second=0, microsecond=0)
                
                # ç¡®ä¿æ¯”è¾ƒçš„ä¸¤ä¸ªæ—¶é—´éƒ½æ˜¯naiveç±»å‹
                beijing_time_naive = to_naive_datetime(beijing_time)
                
                if beijing_time_naive < market_close_time:
                    logger.info(f"è‚¡ç¥¨ {stock_code} å½“å‰æ—¶é—´({beijing_time_naive.strftime('%H:%M')})æœªè¿‡Aè‚¡æ”¶å¸‚æ—¶é—´(15:30)ï¼Œè·³è¿‡å½“å¤©æ•°æ®çˆ¬å–")
                    return pd.DataFrame()
                else:
                    logger.info(f"è‚¡ç¥¨ {stock_code} å½“å‰æ—¶é—´({beijing_time_naive.strftime('%H:%M')})å·²è¿‡Aè‚¡æ”¶å¸‚æ—¶é—´(15:30)ï¼Œéœ€è¦çˆ¬å–å½“å¤©({start_date_naive.strftime('%Y-%m-%d')})æ•°æ®")
            
            logger.info(f"è‚¡ç¥¨ {stock_code} å¢é‡çˆ¬å–ï¼Œä» {start_date.strftime('%Y%m%d')} åˆ° {end_date.strftime('%Y%m%d')}")
        else:
            # æ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œçˆ¬å–æœ€è¿‘ä¸€å¹´çš„æ•°æ®
            now = get_beijing_time()
            start_date = now - timedelta(days=365)
            end_date = get_last_trading_day()
            
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿end_dateæ˜¯datetimeç±»å‹
            if not isinstance(end_date, datetime):
                end_date = datetime.combine(end_date, datetime.min.time())
            
            # ç¡®ä¿èµ·å§‹æ—¥æœŸæ˜¯äº¤æ˜“æ—¥
            current_date = start_date
            start_date = None
            for i in range(30):
                if is_trading_day(current_date.date()):
                    start_date = current_date
                    break
                current_date += timedelta(days=1)
            
            if not start_date:
                start_date = end_date
            
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯”è¾ƒå‰æ—¥æœŸç±»å‹ä¸€è‡´
            # è½¬æ¢ä¸ºnaive datetimeè¿›è¡Œæ¯”è¾ƒ
            start_date_naive = to_naive_datetime(start_date)
            end_date_naive = to_naive_datetime(end_date)
            
            # ç¡®ä¿èµ·å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
            if start_date_naive > end_date_naive:
                start_date = end_date
            
            logger.info(f"è‚¡ç¥¨ {stock_code} é¦–æ¬¡çˆ¬å–ï¼Œè·å–ä» {start_date.strftime('%Y%m%d')} åˆ° {end_date.strftime('%Y%m%d')} çš„æ•°æ®")
        
        # ===== 4. ä½¿ç”¨æ–°çš„æ•°æ®æºæ¨¡å—è·å–æ•°æ® =====
        df = get_stock_daily_data_from_sources(
            stock_code=stock_code,
            start_date=start_date,
            end_date=end_date,
            existing_data=existing_data
        )
        
        # ===== 5. æ•°æ®åå¤„ç† =====
        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # æ·»åŠ åˆ—åæ£€æŸ¥æ—¥å¿—
        logger.debug(f"è‚¡ç¥¨ {stock_code} è·å–åˆ°çš„åˆ—å: {df.columns.tolist()}")
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        required_columns = ["æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æ¢æ‰‹ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"è‚¡ç¥¨ {stock_code} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return pd.DataFrame()
        
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        # ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å€¼ç±»å‹
        numeric_columns = ["å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç§»é™¤NaNå€¼
        df = df.dropna(subset=['æ”¶ç›˜', 'æˆäº¤é‡'])
        
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿"è‚¡ç¥¨ä»£ç "åˆ—å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
        if 'è‚¡ç¥¨ä»£ç ' not in df.columns:
            df['è‚¡ç¥¨ä»£ç '] = stock_code
        else:
            # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½æ ¼å¼
            df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].apply(lambda x: format_stock_code(str(x)))
            # ç§»é™¤æ ¼å¼åŒ–å¤±è´¥çš„è¡Œ
            df = df[df['è‚¡ç¥¨ä»£ç '].notna()]
        
        return df
    
    except Exception as e:
        # æ·»åŠ è¯¦ç»†çš„å¼‚å¸¸æ—¥å¿—
        logger.error(f"è·å–è‚¡ç¥¨ {stock_code} æ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸:", exc_info=True)
        logger.error(f"akshare ç‰ˆæœ¬: {ak.__version__}")
        logger.error(f"akshare æ¨¡å—è·¯å¾„: {ak.__file__}")
        return pd.DataFrame()
        
def save_stock_daily_data(stock_code: str, df: pd.DataFrame):
    """ä¿å­˜è‚¡ç¥¨æ—¥çº¿æ•°æ®åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å"""
    if df.empty:
        return
    
    try:
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½ï¼ˆå‰é¢è¡¥é›¶ï¼‰
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            logger.error(f"æ— æ³•ä¿å­˜ï¼šè‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥")
            return
        
        file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
        
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ•°æ®ä¸­çš„"è‚¡ç¥¨ä»£ç "åˆ—æ˜¯6ä½æ ¼å¼
        if 'è‚¡ç¥¨ä»£ç ' in df.columns:
            # ç¡®ä¿æ•°æ®ä¸­çš„è‚¡ç¥¨ä»£ç åˆ—æ˜¯6ä½æ ¼å¼
            df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].apply(lambda x: format_stock_code(str(x)))
            # ç§»é™¤æ ¼å¼åŒ–å¤±è´¥çš„è¡Œ
            df = df[df['è‚¡ç¥¨ä»£ç '].notna()]
        else:
            # å¦‚æœæ²¡æœ‰è‚¡ç¥¨ä»£ç åˆ—ï¼Œæ·»åŠ å®ƒ
            df['è‚¡ç¥¨ä»£ç '] = stock_code
        
        # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ä¿å­˜å‰å°†æ—¥æœŸåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if 'æ—¥æœŸ' in df.columns:
            df_save = df.copy()
            df_save['æ—¥æœŸ'] = df_save['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
        else:
            df_save = df
        
        # ä¿å­˜æ•°æ®
        df_save.to_csv(file_path, index=False)
        
        logger.debug(f"å·²ä¿å­˜è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®åˆ° {file_path}")
        
        # ã€æ–°å¢ã€‘ç«‹å³æ‰§è¡Œ git addï¼Œå°†æ–‡ä»¶åŠ å…¥æš‚å­˜åŒº
        repo_root = os.environ.get('GITHUB_WORKSPACE', os.getcwd())
        try:
            relative_path = os.path.relpath(file_path, repo_root)
            subprocess.run(['git', 'add', relative_path], cwd=repo_root, check=True)
            logger.debug(f"âœ… æ–‡ä»¶å·²æ·»åŠ åˆ°Gitæš‚å­˜åŒº: {relative_path}")
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡ä»¶åˆ°Gitæš‚å­˜åŒºå¤±è´¥: {str(e)}")
        
        return file_path
        
    except Exception as e:
        logger.error(f"ä¿å­˜è‚¡ç¥¨ {stock_code} æ—¥çº¿æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return None

def complete_missing_stock_data():
    """
    è¡¥å…¨ç¼ºå¤±çš„è‚¡ç¥¨æ—¥çº¿æ•°æ®
    1. æ¯”å¯¹è‚¡ç¥¨åˆ—è¡¨ä¸æ—¥çº¿æ•°æ®ç›®å½•
    2. ä¸ºç¼ºå¤±çš„è‚¡ç¥¨è°ƒç”¨æ­£å¸¸çˆ¬å–æµç¨‹
    """
    logger.info("å¼€å§‹æ£€æŸ¥å¹¶è¡¥å…¨ç¼ºå¤±çš„è‚¡ç¥¨æ—¥çº¿æ•°æ®...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_directory_exists()
    
    # æ£€æŸ¥åŸºç¡€ä¿¡æ¯æ–‡ä»¶
    if not os.path.exists(BASIC_INFO_FILE):
        logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œç¼ºå¤±æ•°æ®è¡¥å…¨")
        return False
    
    try:
        # åŠ è½½åŸºç¡€ä¿¡æ¯
        basic_info_df = pd.read_csv(BASIC_INFO_FILE)
        if basic_info_df.empty:
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œç¼ºå¤±æ•°æ®è¡¥å…¨")
            return False
        
        # ç¡®ä¿"ä»£ç "åˆ—æ˜¯6ä½æ ¼å¼
        basic_info_df["ä»£ç "] = basic_info_df["ä»£ç "].apply(format_stock_code)
        # ç§»é™¤æ— æ•ˆè‚¡ç¥¨
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].notna()]
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].str.len() == 6]
        basic_info_df = basic_info_df.reset_index(drop=True)
        
        # ç»Ÿè®¡æœ‰æ•ˆè‚¡ç¥¨æ•°é‡
        total_stocks = len(basic_info_df)
        logger.info(f"åŸºç¡€ä¿¡æ¯ä¸­åŒ…å« {total_stocks} åªè‚¡ç¥¨")
        
        # æ£€æŸ¥å“ªäº›è‚¡ç¥¨ç¼ºå¤±æ—¥çº¿æ•°æ®
        missing_stocks = []
        for _, row in basic_info_df.iterrows():
            stock_code = format_stock_code(row["ä»£ç "])
            if not stock_code:
                continue
                
            file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
            if not os.path.exists(file_path):
                missing_stocks.append(stock_code)
        
        # æ²¡æœ‰ç¼ºå¤±æ•°æ®ï¼Œç›´æ¥è¿”å›
        if not missing_stocks:
            logger.info("æ‰€æœ‰è‚¡ç¥¨æ—¥çº¿æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return True
        
        logger.info(f"å‘ç° {len(missing_stocks)} åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ç¼ºå¤±ï¼Œå¼€å§‹è¡¥å…¨...")
        
        # æŒ‰é¡ºåºå¤„ç†ç¼ºå¤±è‚¡ç¥¨
        for i, stock_code in enumerate(missing_stocks):
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1.5, 2.5))
            
            logger.info(f"è¡¥å…¨ç¬¬ {i+1}/{len(missing_stocks)} åªç¼ºå¤±è‚¡ç¥¨: {stock_code}")
            df = fetch_stock_daily_data(stock_code)
            
            if not df.empty:
                file_path = save_stock_daily_data(stock_code, df)
                if file_path:
                    logger.info(f"æˆåŠŸè¡¥å…¨è‚¡ç¥¨ {stock_code} çš„æ—¥çº¿æ•°æ®")
            else:
                logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®è¡¥å…¨å¤±è´¥")
        
        # æ£€æŸ¥è¡¥å…¨ç»“æœ
        still_missing = []
        for stock_code in missing_stocks:
            file_path = os.path.join(DAILY_DIR, f"{stock_code}.csv")
            if not os.path.exists(file_path):
                still_missing.append(stock_code)
        
        if still_missing:
            logger.warning(f"è¡¥å…¨åä»æœ‰ {len(still_missing)} åªè‚¡ç¥¨ç¼ºå¤±æ—¥çº¿æ•°æ®: {still_missing}")
        else:
            logger.info(f"æ‰€æœ‰ç¼ºå¤±è‚¡ç¥¨æ•°æ®å·²æˆåŠŸè¡¥å…¨")
        
        return len(still_missing) == 0
    
    except Exception as e:
        logger.error(f"è¡¥å…¨ç¼ºå¤±è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return False

def update_all_stocks_daily_data():
    """æ›´æ–°æ‰€æœ‰è‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ï¼Œä½¿ç”¨ä¸­æ–‡åˆ—å"""
    ensure_directory_exists()
    
    # ç¡®ä¿åŸºç¡€ä¿¡æ¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(BASIC_INFO_FILE):
        logger.info("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        if not update_stock_list():  # è°ƒç”¨è‚¡ç¥¨åˆ—è¡¨æ›´æ–°æ¨¡å—
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•æ›´æ–°æ—¥çº¿æ•°æ®")
            return False
    
    # è·å–åŸºç¡€ä¿¡æ¯æ–‡ä»¶
    try:
        basic_info_df = pd.read_csv(BASIC_INFO_FILE)
        if basic_info_df.empty:
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°æ—¥çº¿æ•°æ®")
            return False
        
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿"ä»£ç "åˆ—æ˜¯6ä½æ ¼å¼
        basic_info_df["ä»£ç "] = basic_info_df["ä»£ç "].apply(format_stock_code)
        # ç§»é™¤æ— æ•ˆè‚¡ç¥¨
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].notna()]
        basic_info_df = basic_info_df[basic_info_df["ä»£ç "].str.len() == 6]
        basic_info_df = basic_info_df.reset_index(drop=True)
        
        # ä¿å­˜æ›´æ–°åçš„åŸºç¡€ä¿¡æ¯æ–‡ä»¶
        basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
        commit_files_in_batches(BASIC_INFO_FILE, "æ›´æ–°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
        logger.info(f"å·²æ›´æ–°åŸºç¡€ä¿¡æ¯æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰è‚¡ç¥¨ä»£ç ä¸º6ä½æ ¼å¼ï¼Œå…± {len(basic_info_df)} æ¡è®°å½•")
        
    except Exception as e:
        logger.error(f"è¯»å–åŸºç¡€ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return False
    
    # ã€å…³é”®ä¿®å¤ã€‘è·å– next_crawl_index å€¼
    # ç”±äºæ‰€æœ‰è¡Œçš„ next_crawl_index å€¼ç›¸åŒï¼Œå–ç¬¬ä¸€è¡Œå³å¯
    next_index = int(basic_info_df["next_crawl_index"].iloc[0])
    total_stocks = len(basic_info_df)
    
    logger.info(f"å½“å‰çˆ¬å–çŠ¶æ€: next_crawl_index = {next_index} (å…± {total_stocks} åªè‚¡ç¥¨)")
    
    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # ä¸“ä¸šä¿®å¤ï¼šå¾ªç¯æ‰¹å¤„ç†æœºåˆ¶
    # 1. ç¡®å®šå¤„ç†èŒƒå›´ï¼ˆä½¿ç”¨å¾ªç¯å¤„ç†ï¼‰
    start_idx = next_index % total_stocks
    end_idx = start_idx + BATCH_SIZE
    
    # 2. è®¡ç®—å®é™…çš„end_idxï¼ˆç”¨äºè¿›åº¦æ›´æ–°ï¼‰
    actual_end_idx = end_idx % total_stocks
    
    # 3. è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹è‚¡ç¥¨ï¼ˆä½¿ç”¨å®é™…ç´¢å¼•ï¼‰
    first_stock_idx = start_idx % total_stocks
    last_stock_idx = (end_idx - 1) % total_stocks
    
    # 4. å¤„ç†å¾ªç¯æƒ…å†µ
    if end_idx <= total_stocks:
        batch_df = basic_info_df.iloc[start_idx:end_idx]
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ è‚¡ç¥¨ ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹")
    else:
        # å¾ªç¯å¤„ç†ï¼šç¬¬ä¸€éƒ¨åˆ†ï¼ˆstart_idxåˆ°total_stocksï¼‰+ ç¬¬äºŒéƒ¨åˆ†ï¼ˆ0åˆ°end_idx-total_stocksï¼‰
        batch_df = pd.concat([basic_info_df.iloc[start_idx:total_stocks], basic_info_df.iloc[0:end_idx-total_stocks]])
        logger.info(f"å¤„ç†æœ¬æ‰¹æ¬¡ è‚¡ç¥¨ ({BATCH_SIZE}åª)ï¼Œä»ç´¢å¼• {start_idx} å¼€å§‹ï¼ˆå¾ªç¯å¤„ç†ï¼‰")
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

    # è®°å½•ç¬¬ä¸€æ‰¹å’Œæœ€åä¸€æ‰¹è‚¡ç¥¨
    first_stock = basic_info_df.iloc[first_stock_idx]
    last_stock = basic_info_df.iloc[last_stock_idx]
    logger.info(f"å½“å‰æ‰¹æ¬¡ç¬¬ä¸€åªè‚¡ç¥¨: {first_stock['ä»£ç ']} - {first_stock['åç§°']} (ç´¢å¼• {first_stock_idx})")
    logger.info(f"å½“å‰æ‰¹æ¬¡æœ€åä¸€åªè‚¡ç¥¨: {last_stock['ä»£ç ']} - {last_stock['åç§°']} (ç´¢å¼• {last_stock_idx})")
    
    # å¤„ç†è¿™æ‰¹è‚¡ç¥¨
    batch_codes = batch_df["ä»£ç "].tolist()
    
    if not batch_codes:
        logger.warning("æ²¡æœ‰å¯çˆ¬å–çš„è‚¡ç¥¨")
        return False
    
    # æ–‡ä»¶è·¯å¾„ç¼“å­˜
    file_paths = []
    
    # å¤„ç†è¿™æ‰¹è‚¡ç¥¨
    for i, stock_code in enumerate(batch_codes):
        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½
        stock_code = format_stock_code(stock_code)
        if not stock_code:
            continue
            
        # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(random.uniform(1.5, 2.5))  # å¢åŠ å»¶æ—¶ï¼Œé¿å…è¢«é™æµ
        df = fetch_stock_daily_data(stock_code)
        if not df.empty:
            file_path = save_stock_daily_data(stock_code, df)
            if file_path:
                file_paths.append(file_path)
        
        # ã€å…³é”®ä¿®å¤ã€‘æ¯å¤„ç†10ä¸ªè‚¡ç¥¨å°±æ£€æŸ¥ä¸€æ¬¡æäº¤çŠ¶æ€
        if (i + 1) % MINOR_BATCH_SIZE == 0 and file_paths:
            logger.info(f"æ‰¹é‡æäº¤ {len(file_paths)} åªè‚¡ç¥¨æ—¥çº¿æ•°æ®...")
            
            # æ„å»ºè¦æäº¤çš„æ–‡ä»¶åˆ—è¡¨
            commit_msg = f"feat: æ‰¹é‡æäº¤{len(file_paths)}åªè‚¡ç¥¨æ—¥çº¿æ•°æ® [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
            commit_success = commit_batch_files(file_paths, commit_msg)  # ä½¿ç”¨å¢å¼ºç‰ˆæäº¤å‡½æ•°
            
            if commit_success:
                logger.info(f"âœ… å°æ‰¹æ¬¡æ•°æ®æ–‡ä»¶æäº¤æˆåŠŸï¼š{len(file_paths)}åª")
            else:
                logger.error(f"âŒ å°æ‰¹æ¬¡æ•°æ®æ–‡ä»¶æäº¤å¤±è´¥ï¼š{len(file_paths)}åª")
            
            # æ¸…ç©ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            file_paths = []
    
    # ã€å…³é”®ä¿®å¤ã€‘å¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œç¡®ä¿æäº¤ä»»ä½•å‰©ä½™æ–‡ä»¶
    logger.info(f"å¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œæ£€æŸ¥å¹¶æäº¤ä»»ä½•å‰©ä½™æ–‡ä»¶...")
    
    # æäº¤å‰©ä½™æ–‡ä»¶
    if file_paths:
        logger.info(f"æ‰¹é‡æäº¤å‰©ä½™ {len(file_paths)} åªè‚¡ç¥¨æ—¥çº¿æ•°æ®...")
        
        # æ„å»ºè¦æäº¤çš„æ–‡ä»¶åˆ—è¡¨
        commit_msg = f"feat: æ‰¹é‡æäº¤{len(file_paths)}åªè‚¡ç¥¨æ—¥çº¿æ•°æ® [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
        commit_success = commit_batch_files(file_paths, commit_msg)  # ä½¿ç”¨å¢å¼ºç‰ˆæäº¤å‡½æ•°
        
        if commit_success:
            logger.info(f"âœ… å‰©ä½™è‚¡ç¥¨æ•°æ®æ–‡ä»¶æäº¤æˆåŠŸï¼š{len(file_paths)}åª")
        else:
            logger.error(f"âŒ å‰©ä½™è‚¡ç¥¨æ•°æ®æ–‡ä»¶æäº¤å¤±è´¥ï¼š{len(file_paths)}åª")
    
    # ã€å…³é”®ä¿®å¤ã€‘å¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œç¡®ä¿æäº¤ä»»ä½•å‰©ä½™æ–‡ä»¶
    logger.info(f"å¤„ç†å®Œæœ¬æ‰¹æ¬¡åï¼Œå¼ºåˆ¶æäº¤æ‰€æœ‰å‰©ä½™æ–‡ä»¶...")
    if force_commit_remaining_files():
        logger.info("âœ… å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶æˆåŠŸ")
    else:
        logger.error("âŒ å¼ºåˆ¶æäº¤å‰©ä½™æ–‡ä»¶å¤±è´¥")
    
    # ã€å…³é”®ä¿®å¤ã€‘æ›´æ–° next_crawl_index
    new_index = actual_end_idx
    logger.info(f"æ›´æ–° next_crawl_index = {new_index}")
    basic_info_df["next_crawl_index"] = new_index
    basic_info_df.to_csv(BASIC_INFO_FILE, index=False)
    
    # æäº¤æ›´æ–°åçš„åŸºç¡€ä¿¡æ¯æ–‡ä»¶
    commit_msg = f"æ›´æ–°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ [skip ci] - {datetime.now().strftime('%Y%m%d%H%M%S')}"
    commit_success = commit_batch_files([BASIC_INFO_FILE], commit_msg)  # ä½¿ç”¨å¢å¼ºç‰ˆæäº¤å‡½æ•°
    
    if commit_success:
        logger.info(f"âœ… å·²æäº¤æ›´æ–°åçš„åŸºç¡€ä¿¡æ¯æ–‡ä»¶åˆ°ä»“åº“: {BASIC_INFO_FILE}")
    else:
        logger.error(f"âŒ æäº¤åŸºç¡€ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {BASIC_INFO_FILE}")
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„è‚¡ç¥¨
    remaining_stocks = (total_stocks - new_index) % total_stocks
    logger.info(f"å·²å®Œæˆ {BATCH_SIZE} åªè‚¡ç¥¨çˆ¬å–ï¼Œè¿˜æœ‰ {remaining_stocks} åªè‚¡ç¥¨å¾…çˆ¬å–")
    
    return True

def main():
    """ä¸»å‡½æ•°ï¼šæ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
    logger.info("===== å¼€å§‹æ›´æ–°è‚¡ç¥¨æ•°æ® =====")
    
    # æ·»åŠ åˆå§‹å»¶æ—¶ï¼Œé¿å…ç«‹å³è¯·æ±‚
    time.sleep(random.uniform(1.0, 2.0))
    
    # 1. ç¡®ä¿åŸºç¡€ä¿¡æ¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(BASIC_INFO_FILE) or os.path.getsize(BASIC_INFO_FILE) == 0:
        logger.info("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ­£åœ¨åˆ›å»º...")
        if not update_stock_list():  # è°ƒç”¨è‚¡ç¥¨åˆ—è¡¨æ›´æ–°æ¨¡å—
            logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
    
    # 2. åªæ›´æ–°ä¸€æ‰¹è‚¡ç¥¨ï¼ˆæœ€å¤šBATCH_SIZEåªï¼‰
    if update_all_stocks_daily_data():
        logger.info("å·²æˆåŠŸå¤„ç†ä¸€æ‰¹è‚¡ç¥¨æ•°æ®")
    else:
        logger.error("å¤„ç†è‚¡ç¥¨æ•°æ®å¤±è´¥")
    
    logger.info("===== è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ =====")

if __name__ == "__main__":
    main()
