#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨åˆ—è¡¨è´¢åŠ¡è¿‡æ»¤å™¨
åŠŸèƒ½ï¼š
1. è¯»å–all_stocks.csvæ–‡ä»¶
2. é€ä¸ªè‚¡ç¥¨è·å–å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆä»…æ›´æ–°ã€æµé€šå¸‚å€¼ã€æ€»å¸‚å€¼ã€åŠ¨æ€å¸‚ç›ˆç‡ã€‘ä¸‰ä¸ªå­—æ®µï¼‰
3. åº”ç”¨è´¢åŠ¡æ¡ä»¶è¿‡æ»¤
4. å°†è¿‡æ»¤åçš„è‚¡ç¥¨åˆ—è¡¨ä¿å­˜å›all_stocks.csv

è´¢åŠ¡è¿‡æ»¤æ¡ä»¶ï¼š
- åŠ¨æ€å¸‚ç›ˆç‡ >= å‚æ•°å€¼
- æµé€šå¸‚å€¼ / æ€»å¸‚å€¼ > å‚æ•°å€¼

ä½¿ç”¨è¯´æ˜ï¼š
1. è¯¥è„šæœ¬åº”åœ¨æ¯å‘¨å›ºå®šæ—¶é—´è¿è¡Œï¼ˆä¾‹å¦‚å‘¨æœ«ï¼‰
2. è¿è¡Œå‰ç¡®ä¿å·²å®‰è£…å¿…è¦ä¾èµ–ï¼špip install baostock pandas akshare
3. è„šæœ¬ä¼šæ›´æ–°all_stocks.csvæ–‡ä»¶
"""

import os
import pandas as pd
import baostock as bs
import time
import logging
import sys
from datetime import datetime
from config import Config
from utils.date_utils import get_beijing_time
from utils.git_utils import commit_files_in_batches
import akshare as ak  # æ–°å¢ï¼šç”¨äºè·å–å®æ—¶è¡Œæƒ…æ•°æ®

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ·»åŠ BATCH_SIZEå‚æ•°ï¼Œæ–¹ä¾¿çµæ´»è°ƒæ•´æ¯æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
BATCH_SIZE = 100  # æ¯æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡

# ğŸš« åˆ é™¤æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡é…ç½®ï¼Œåªä¿ç•™ä¸¤ä¸ªå‚æ•°
FINANCIAL_FILTER_PARAMS = {
    "dynamic_pe_ratio": {
        "enabled": True,
        "threshold": 0.0,  # åŠ¨æ€å¸‚ç›ˆç‡é˜ˆå€¼
        "column": "åŠ¨æ€å¸‚ç›ˆç‡",
        "condition": ">= {threshold}ï¼ˆæ’é™¤åŠ¨æ€å¸‚ç›ˆç‡ä½äºé˜ˆå€¼çš„è‚¡ç¥¨ï¼‰"
    },
    "circulation_market_cap_ratio": {
        "enabled": True,
        "threshold": 0.9,  # æµé€šå¸‚å€¼/æ€»å¸‚å€¼æ¯”å€¼é˜ˆå€¼
        "column": "æµé€šå¸‚å€¼/æ€»å¸‚å€¼",
        "condition": "> {threshold}ï¼ˆæ’é™¤æµé€šå¸‚å€¼/æ€»å¸‚å€¼æ¯”å€¼ä½äºé˜ˆå€¼çš„è‚¡ç¥¨ï¼‰"
    }
}

def get_stock_quote(code):
    """
    ä½¿ç”¨ ak.stock_zh_a_daily æ¥å£è·å–å•åªè‚¡ç¥¨çš„æœ€æ–°è¡Œæƒ…æ•°æ®
    å‚æ•°ï¼š
    - code: è‚¡ç¥¨ä»£ç ï¼ˆ6ä½å­—ç¬¦ä¸²ï¼‰
    è¿”å›ï¼š
    - dict: åŒ…å«æµé€šå¸‚å€¼ã€æ€»å¸‚å€¼ã€åŠ¨æ€å¸‚ç›ˆç‡çš„å­—å…¸
    - None: è·å–å¤±è´¥
    """
    try:
        # æ„é€  akstock çš„å‚æ•°
        df = ak.stock_zh_a_daily(symbol=code, adjust="qfq")
        
        if df.empty:
            logger.warning(f"è‚¡ç¥¨ {code} è¡Œæƒ…æ•°æ®ä¸ºç©º")
            return None
        
        # å–æœ€æ–°ä¸€æ¡æ•°æ®
        latest_row = df.iloc[-1]
        
        # æå–éœ€è¦çš„å­—æ®µ
        quote_data = {
            'æ€»å¸‚å€¼': latest_row.get('æ€»å¸‚å€¼', 0.0),
            'æµé€šå¸‚å€¼': latest_row.get('æµé€šå¸‚å€¼', 0.0),
            'åŠ¨æ€å¸‚ç›ˆç‡': latest_row.get('å¸‚ç›ˆç‡-åŠ¨æ€', 0.0)
        }
        
        # è½¬æ¢ä¸ºæ•°å€¼å‹
        for key in quote_data:
            try:
                quote_data[key] = float(quote_data[key])
            except (ValueError, TypeError):
                quote_data[key] = 0.0
        
        return quote_data
    
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {code} è¡Œæƒ…æ•°æ®å¤±è´¥: {str(e)}")
        return None

def filter_and_update_stocks():
    """
    ä¸»å‡½æ•°ï¼šè¿‡æ»¤è‚¡ç¥¨å¹¶æ›´æ–°all_stocks.csv
    """
    # è·å–all_stocks.csvæ–‡ä»¶è·¯å¾„
    basic_info_file = os.path.join(Config.DATA_DIR, "all_stocks.csv")
    
    if not os.path.exists(basic_info_file):
        logger.error("åŸºç¡€ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        # è¯»å–æ‰€æœ‰è‚¡ç¥¨
        basic_info_df = pd.read_csv(basic_info_file)
        logger.info(f"æˆåŠŸè¯»å–åŸºç¡€ä¿¡æ¯æ–‡ä»¶ï¼Œå…± {len(basic_info_df)} åªè‚¡ç¥¨")
        
        # ç¡®ä¿æœ‰filteråˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
        if 'filter' not in basic_info_df.columns:
            basic_info_df['filter'] = False
            logger.info("æ·»åŠ filteråˆ—åˆ°all_stocks.csvæ–‡ä»¶")
        
        # æ‰¾å‡ºéœ€è¦å¤„ç†çš„è‚¡ç¥¨ï¼ˆfilterä¸ºFalseï¼‰
        to_process = basic_info_df[basic_info_df['filter'] == False]
        logger.info(f"éœ€è¦å¤„ç†çš„è‚¡ç¥¨æ•°é‡: {len(to_process)}")
        
        # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„è‚¡ç¥¨ï¼Œé‡ç½®æ‰€æœ‰filterä¸ºFalseå¹¶é€€å‡º
        if len(to_process) == 0:
            logger.info("æ‰€æœ‰è‚¡ç¥¨éƒ½å·²å¤„ç†ï¼Œé‡ç½®filteråˆ—")
            basic_info_df['filter'] = False
            basic_info_df.to_csv(basic_info_file, index=False)
            logger.info("filteråˆ—å·²é‡ç½®ï¼Œé€€å‡ºæ‰§è¡Œ")
            return
        
        # åªå¤„ç†å‰BATCH_SIZEåªè‚¡ç¥¨
        process_batch = to_process.head(BATCH_SIZE)
        logger.info(f"æœ¬æ¬¡å¤„ç†è‚¡ç¥¨æ•°é‡: {len(process_batch)}")

        # ğŸš« åˆ é™¤åŸè´¢åŠ¡æ•°æ®è·å–é€»è¾‘ï¼Œæ”¹ä¸ºé€åªè‚¡ç¥¨è·å–å®æ—¶è¡Œæƒ…æ•°æ®
        for _, row in process_batch.iterrows():
            code = row['ä»£ç ']
            logger.info(f"æ­£åœ¨å¤„ç†è‚¡ç¥¨ {code}...")

            try:
                # è·å–å•åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…æ•°æ®
                quote_data = get_stock_quote(code)
                if quote_data is None:
                    logger.warning(f"è‚¡ç¥¨ {code} å®æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©º")
                    continue

                # æ›´æ–° basic_info_df ä¸­å¯¹åº”çš„ä¸‰åˆ—
                basic_info_df.loc[basic_info_df['ä»£ç '] == code, 'æ€»å¸‚å€¼'] = quote_data['æ€»å¸‚å€¼']
                basic_info_df.loc[basic_info_df['ä»£ç '] == code, 'æµé€šå¸‚å€¼'] = quote_data['æµé€šå¸‚å€¼']
                basic_info_df.loc[basic_info_df['ä»£ç '] == code, 'åŠ¨æ€å¸‚ç›ˆç‡'] = quote_data['åŠ¨æ€å¸‚ç›ˆç‡']

                logger.info(f"âœ… è‚¡ç¥¨ {code} å®æ—¶è¡Œæƒ…æ•°æ®æ›´æ–°æˆåŠŸ")

            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {code} å®æ—¶è¡Œæƒ…æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                continue  # è·³è¿‡å½“å‰è‚¡ç¥¨ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

            # æ¯å¤„ç†å®Œä¸€åªè‚¡ç¥¨ï¼Œæš‚åœ 0.5 ç§’ï¼Œé¿å…ç³»ç»Ÿè´Ÿè½½è¿‡é«˜
            time.sleep(0.5)

        # è®°å½•è¡¥å……å‰çŠ¶æ€
        initial_count = len(basic_info_df)
        logger.info(f"è¡¥å……æŒ‡æ ‡å‰è‚¡ç¥¨æ•°é‡: {initial_count}")

        # åº”ç”¨æ–°è¿‡æ»¤æ¡ä»¶
        # æ¡ä»¶1ï¼šåŠ¨æ€å¸‚ç›ˆç‡ >= 0
        before_pe = len(basic_info_df)
        basic_info_df = basic_info_df.dropna(subset=['åŠ¨æ€å¸‚ç›ˆç‡'])  # å…ˆæ’é™¤NaN
        basic_info_df = basic_info_df[basic_info_df['åŠ¨æ€å¸‚ç›ˆç‡'] >= FINANCIAL_FILTER_PARAMS["dynamic_pe_ratio"]["threshold"]]
        removed_pe = before_pe - len(basic_info_df)
        logger.info(f"æ’é™¤ {removed_pe} åªåŠ¨æ€å¸‚ç›ˆç‡ < {FINANCIAL_FILTER_PARAMS['dynamic_pe_ratio']['threshold']} çš„è‚¡ç¥¨ï¼ˆPEè¿‡æ»¤ï¼‰")

        # æ¡ä»¶2ï¼šæµé€šå¸‚å€¼ / æ€»å¸‚å€¼ > 90%
        before_ratio = len(basic_info_df)
        basic_info_df = basic_info_df.dropna(subset=['æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼'])
        basic_info_df = basic_info_df[basic_info_df['æ€»å¸‚å€¼'] > 0]
        basic_info_df['æµé€šå¸‚å€¼å æ¯”'] = basic_info_df['æµé€šå¸‚å€¼'] / basic_info_df['æ€»å¸‚å€¼']
        basic_info_df = basic_info_df[basic_info_df['æµé€šå¸‚å€¼å æ¯”'] > FINANCIAL_FILTER_PARAMS["circulation_market_cap_ratio"]["threshold"]]
        removed_ratio = before_ratio - len(basic_info_df)
        logger.info(f"æ’é™¤ {removed_ratio} åªæµé€šå¸‚å€¼å æ¯” <= {FINANCIAL_FILTER_PARAMS['circulation_market_cap_ratio']['threshold']} çš„è‚¡ç¥¨ï¼ˆå¸‚å€¼ç»“æ„è¿‡æ»¤ï¼‰")

        # æ¸…ç†ä¸´æ—¶åˆ—
        if 'æµé€šå¸‚å€¼å æ¯”' in basic_info_df.columns:
            basic_info_df = basic_info_df.drop(columns=['æµé€šå¸‚å€¼å æ¯”'])

        # æ›´æ–° filter åˆ—ï¼šé€šè¿‡è¿‡æ»¤çš„è®¾ç½®ä¸º True
        basic_info_df['filter'] = True  # æ‰€æœ‰é€šè¿‡è¿‡æ»¤çš„è‚¡ç¥¨æ ‡è®°ä¸º True

        # é‡æ–°æ•´ç†åˆ—é¡ºåºï¼ˆç¡®ä¿ä¸åŸç»“æ„ä¸€è‡´ï¼‰
        target_columns = [
            "ä»£ç ", "åç§°", "æ‰€å±æ¿å—", "æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "æ•°æ®çŠ¶æ€", 
            "åŠ¨æ€å¸‚ç›ˆç‡", "filter", "next_crawl_index", "è´¨æŠ¼è‚¡æ•°"
        ]
        # è¡¥å……ç¼ºå¤±åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for col in target_columns:
            if col not in basic_info_df.columns:
                if col == "filter":
                    basic_info_df[col] = False
                elif col == "next_crawl_index":
                    basic_info_df[col] = 0
                elif col in ["æµé€šå¸‚å€¼", "æ€»å¸‚å€¼", "åŠ¨æ€å¸‚ç›ˆç‡"]:
                    basic_info_df[col] = 0.0
                elif col == "è´¨æŠ¼è‚¡æ•°":
                    basic_info_df[col] = 0
                else:
                    basic_info_df[col] = ""

        # é€‰æ‹©ç›®æ ‡åˆ—å¹¶æ’åº
        basic_info_df = basic_info_df[target_columns]

        # ä¿å­˜æœ€ç»ˆç»“æœ
        basic_info_df.to_csv(basic_info_file, index=False, float_format='%.2f')
        commit_files_in_batches(basic_info_file, "æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆè¡¥å……æµé€šå¸‚å€¼/æ€»å¸‚å€¼/åŠ¨æ€å¸‚ç›ˆç‡å¹¶è¿‡æ»¤ï¼‰")
        logger.info(f"âœ… è‚¡ç¥¨åˆ—è¡¨å·²æˆåŠŸè¡¥å……è´¢åŠ¡æŒ‡æ ‡å¹¶å®Œæˆæœ€ç»ˆè¿‡æ»¤ï¼Œå…± {len(basic_info_df)} æ¡è®°å½•")

    except Exception as e:
        logger.error(f"å¤„ç†è‚¡ç¥¨åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)

if __name__ == "__main__":
    start_time = datetime.now()
    logger.info("å¼€å§‹æ‰§è¡Œè‚¡ç¥¨è´¢åŠ¡è¿‡æ»¤å™¨")
    
    filter_and_update_stocks()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    logger.info(f"è‚¡ç¥¨è´¢åŠ¡è¿‡æ»¤å™¨æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {duration:.2f} ç§’")
