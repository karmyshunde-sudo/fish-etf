#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥—åˆ©ç­–ç•¥è®¡ç®—æ¨¡å—
åŸºäºå·²ä¿å­˜çš„å®æ—¶æ•°æ®è®¡ç®—å¥—åˆ©æœºä¼š
ä¸¥æ ¼éµå¾ªé¡¹ç›®æ¶æ„åŸåˆ™ï¼šåªè´Ÿè´£è®¡ç®—ï¼Œä¸æ¶‰åŠæ•°æ®çˆ¬å–å’Œæ¶ˆæ¯æ ¼å¼åŒ–
ã€å…³é”®ä¿®å¤ã€‘
- ä¿®å¤äº†æŠ˜ä»·/æº¢ä»·åˆ¤æ–­é€»è¾‘é”™è¯¯é—®é¢˜
- ä¿®å¤äº†æ¶ˆæ¯ç”Ÿæˆä¸­çš„çŸ›ç›¾è¡¨è¿°
- ä¿®å¤äº†åŸºé‡‘è§„æ¨¡è·å–é—®é¢˜
- ä¿®å¤äº†æ—¥å‡æˆäº¤é¢å•ä½é—®é¢˜
- æ˜ç¡®äº†å¥—åˆ©æ“ä½œå»ºè®®
"""

import pandas as pd
import numpy as np
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple
from config import Config
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    is_trading_day,
    is_trading_time
)
from utils.file_utils import (
    load_etf_daily_data, 
    ensure_chinese_columns,
    load_discount_status,
    save_discount_status,
    should_push_discount,
    mark_discount_pushed,
    load_premium_status,
    save_premium_status,
    should_push_premium,
    mark_premium_pushed,
    load_etf_metadata
)
from data_crawler.strategy_arbitrage_source import get_trading_etf_list, get_latest_arbitrage_opportunities as get_arbitrage_data
from .etf_scoring import (
    get_etf_basic_info, 
    get_etf_name,
    calculate_arbitrage_score,
    calculate_component_stability_score
)
from wechat_push.push import send_wechat_message

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)

def extract_scalar_value(value, default=0.0, log_prefix=""):
    """
    å®‰å…¨åœ°ä»å„ç§ç±»å‹ä¸­æå–æ ‡é‡å€¼
    """
    try:
        # å¦‚æœå·²ç»æ˜¯æ ‡é‡å€¼ï¼Œç›´æ¥è¿”å›
        if isinstance(value, (int, float)):
            return float(value)
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        if isinstance(value, str):
            # å°è¯•ç§»é™¤éæ•°å­—å­—ç¬¦
            cleaned_str = ''.join(c for c in value if c.isdigit() or c in ['.', '-'])
            if cleaned_str:
                result = float(cleaned_str)
                logger.debug(f"{log_prefix}ä»å­—ç¬¦ä¸²æå–æ ‡é‡å€¼: '{value}' -> {result}")
                return result
            logger.warning(f"{log_prefix}æ— æ³•ä»å­—ç¬¦ä¸² '{value}' æå–æœ‰æ•ˆæ•°å­—ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
            return default
        
        # å¦‚æœæ˜¯pandaså¯¹è±¡ï¼Œå°è¯•æå–æ ‡é‡å€¼
        if isinstance(value, (pd.Series, pd.DataFrame)):
            # å°è¯•è·å–ç¬¬ä¸€ä¸ªå€¼
            if value.size > 0:
                # å°è¯•ä½¿ç”¨.values.flatten()[0]ï¼ˆæœ€å¯é ï¼‰
                try:
                    result = float(value.values.flatten()[0])
                    logger.debug(f"{log_prefix}é€šè¿‡.values.flatten()[0]æå–æ ‡é‡å€¼: {result}")
                    return result
                except Exception as e:
                    # å°è¯•ä½¿ç”¨.item()
                    try:
                        result = float(value.item())
                        logger.debug(f"{log_prefix}é€šè¿‡.item()æå–æ ‡é‡å€¼: {result}")
                        return result
                    except Exception as e2:
                        # å°è¯•ä½¿ç”¨.iloc[0]
                        try:
                            valid_values = value[~pd.isna(value)]
                            if not valid_values.empty:
                                result = float(valid_values.iloc[0])
                                logger.debug(f"{log_prefix}é€šè¿‡.iloc[0]æå–æ ‡é‡å€¼: {result}")
                                return result
                        except Exception as e3:
                            pass
            
            logger.error(f"{log_prefix}æ— æ³•ä»pandaså¯¹è±¡æå–æ ‡é‡å€¼(size={value.size})ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
            return default
        
        # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        result = float(value)
        logger.debug(f"{log_prefix}ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°: {result}")
        return result
    
    except Exception as e:
        logger.error(f"{log_prefix}æ— æ³•ä»ç±»å‹ {type(value)} ä¸­æå–æ ‡é‡å€¼: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
        return default

def calculate_premium_discount(market_price: float, iopv: float) -> float:
    """
    è®¡ç®—æŠ˜æº¢ä»·ç‡
    
    Args:
        market_price: å¸‚åœºä»·æ ¼
        iopv: IOPV(åŸºé‡‘ä»½é¢å‚è€ƒå‡€å€¼)
    
    Returns:
        float: æŠ˜æº¢ä»·ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
                æ­£æ•°è¡¨ç¤ºæº¢ä»·ï¼ˆå¸‚åœºä»·æ ¼ > IOPVï¼‰
                è´Ÿæ•°è¡¨ç¤ºæŠ˜ä»·ï¼ˆå¸‚åœºä»·æ ¼ < IOPVï¼‰
    """
    if iopv <= 0:
        logger.warning(f"æ— æ•ˆçš„IOPV: {iopv}")
        return 0.0
    
    # æ­£ç¡®è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼š(å¸‚åœºä»·æ ¼ - IOPV) / IOPV * 100
    premium_discount = ((market_price - iopv) / iopv) * 100
    return round(premium_discount, 2)

def is_manual_trigger() -> bool:
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯æ‰‹åŠ¨è§¦å‘çš„ä»»åŠ¡
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼ŒGitHub Actionsä¸­æ‰‹åŠ¨è§¦å‘ä¼šæœ‰ç‰¹æ®Šç¯å¢ƒå˜é‡
        return os.environ.get('GITHUB_EVENT_NAME', '') == 'workflow_dispatch'
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ˜¯å¦ä¸ºæ‰‹åŠ¨è§¦å‘å¤±è´¥: {str(e)}", exc_info=True)
        return False

def validate_arbitrage_data(df: pd.DataFrame) -> bool:
    """
    å¢å¼ºçš„å®æ—¶å¥—åˆ©æ•°æ®éªŒè¯
    """
    if df.empty:
        logger.warning("å®æ—¶å¥—åˆ©æ•°æ®ä¸ºç©º")
        return False
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f"å®æ—¶å¥—åˆ©æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
        logger.error(f"å®é™…åˆ—å: {list(df.columns)}")
        return False
    
    # æ£€æŸ¥æ•°æ®é‡
    if len(df) < 10:
        logger.warning(f"å®æ—¶å¥—åˆ©æ•°æ®é‡ä¸è¶³({len(df)}æ¡)")
        return False
    
    # å¢å¼ºéªŒè¯ï¼šæ£€æŸ¥ä»·æ ¼å’ŒIOPVçš„åˆç†æ€§
    price_range_valid = df[(df["å¸‚åœºä»·æ ¼"] > 0.01) & (df["å¸‚åœºä»·æ ¼"] < 100)].shape[0]
    price_range_invalid = df.shape[0] - price_range_valid
    
    if price_range_invalid > 0:
        logger.warning(f"å‘ç° {price_range_invalid} ä¸ªå¼‚å¸¸ä»·æ ¼æ•°æ®")
    
    iopv_range_valid = df[(df["IOPV"] > 0.01) & (df["IOPV"] < 100)].shape[0]
    iopv_range_invalid = df.shape[0] - iopv_range_valid
    
    if iopv_range_invalid > 0:
        logger.warning(f"å‘ç° {iopv_range_invalid} ä¸ªå¼‚å¸¸IOPVæ•°æ®")
    
    valid_ratio = df[(df["å¸‚åœºä»·æ ¼"] / df["IOPV"] > 0.1) & 
                     (df["å¸‚åœºä»·æ ¼"] / df["IOPV"] < 10)].shape[0]
    invalid_ratio = df.shape[0] - valid_ratio
    
    if invalid_ratio > 10:
        logger.error(f"å‘ç°å¤§é‡å¼‚å¸¸ä»·æ ¼/IOPVæ¯”å€¼æ•°æ®: {invalid_ratio}ä¸ª")
        if invalid_ratio > len(df) * 0.5:
            logger.error("è¶…è¿‡50%æ•°æ®å¼‚å¸¸ï¼Œæ•°æ®æºå¯èƒ½æœ‰é—®é¢˜")
            return False
    
    return True

def calculate_arbitrage_opportunity() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    åŸºäºå®æ—¶æ•°æ®è®¡ç®—ETFå¥—åˆ©æœºä¼š
    
    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: æŠ˜ä»·æœºä¼šDataFrame, æº¢ä»·æœºä¼šDataFrame
    """
    try:
        logger.info("å¼€å§‹è®¡ç®—å¥—åˆ©æœºä¼š")
        
        utc_now, beijing_now = get_current_times()
        logger.info(f"å¼€å§‹è®¡ç®—å¥—åˆ©æœºä¼š (UTC: {utc_now}, CST: {beijing_now})")
        
        # è·å–æ‰€æœ‰çš„ETFæ•°æ®
        all_opportunities = get_arbitrage_data()
        
        if not isinstance(all_opportunities, pd.DataFrame):
            logger.error(f"get_arbitrage_data() è¿”å›å€¼ç±»å‹é”™è¯¯ï¼ŒæœŸæœ›pd.DataFrameï¼Œå®é™…è¿”å›: {type(all_opportunities)}")
            return pd.DataFrame(), pd.DataFrame()
        
        logger.info(f"è·å–åˆ° {len(all_opportunities)} æ¡åŸå§‹æ•°æ®")
        if not all_opportunities.empty:
            logger.info(f"åˆ—å: {list(all_opportunities.columns)}")
            
            if "IOPV" in all_opportunities.columns and "å¸‚åœºä»·æ ¼" in all_opportunities.columns:
                logger.info(f"IOPVèŒƒå›´: {all_opportunities['IOPV'].min():.3f} ~ {all_opportunities['IOPV'].max():.3f}")
                logger.info(f"ä»·æ ¼èŒƒå›´: {all_opportunities['å¸‚åœºä»·æ ¼'].min():.3f} ~ {all_opportunities['å¸‚åœºä»·æ ¼'].max():.3f}")
                
                ratio = all_opportunities["å¸‚åœºä»·æ ¼"] / all_opportunities["IOPV"]
                logger.info(f"ä»·æ ¼/IOPVæ¯”å€¼èŒƒå›´: {ratio.min():.3f} ~ {ratio.max():.3f}")
                
                abnormal_ratio = ratio[(ratio < 0.5) | (ratio > 2)]
                if len(abnormal_ratio) > 0:
                    logger.warning(f"å‘ç° {len(abnormal_ratio)} ä¸ªå¼‚å¸¸ä»·æ ¼/IOPVæ¯”å€¼æ•°æ®")
        
        if not validate_arbitrage_data(all_opportunities):
            logger.error("å®æ—¶å¥—åˆ©æ•°æ®éªŒè¯å¤±è´¥ï¼Œæ— æ³•è®¡ç®—å¥—åˆ©æœºä¼š")
            return pd.DataFrame(), pd.DataFrame()
        
        all_opportunities = ensure_chinese_columns(all_opportunities)
        all_opportunities.columns = [col.strip() for col in all_opportunities.columns]
        
        # è·å–ç”¨äºå¥—åˆ©ç›‘æ§çš„ETFåˆ—è¡¨
        trading_etf_list = get_trading_etf_list()
        logger.info(f"è·å–åˆ° {len(trading_etf_list)} åªç¬¦åˆæ¡ä»¶çš„ETFè¿›è¡Œå¥—åˆ©ç›‘æ§")
        
        # ç­›é€‰å‡ºäº¤æ˜“ETFåˆ—è¡¨ä¸­çš„ETF
        all_opportunities = all_opportunities[all_opportunities["ETFä»£ç "].isin(trading_etf_list)]
        
        if all_opportunities.empty:
            logger.warning("ç­›é€‰åæ— ç¬¦åˆæ¡ä»¶çš„ETFæ•°æ®")
            return pd.DataFrame(), pd.DataFrame()
        
        # æ•°æ®æ¸…æ´—
        MIN_IOPV = 0.01
        MIN_PRICE = 0.01
        valid_opportunities = all_opportunities[
            (all_opportunities["IOPV"] > MIN_IOPV) & 
            (all_opportunities["å¸‚åœºä»·æ ¼"] > MIN_PRICE)
        ].copy()
        
        if len(valid_opportunities) > 0:
            price_iopv_ratio = valid_opportunities["å¸‚åœºä»·æ ¼"] / valid_opportunities["IOPV"]
            valid_opportunities = valid_opportunities[
                (price_iopv_ratio > 0.1) & (price_iopv_ratio < 10)
            ].copy()
        
        if valid_opportunities.empty:
            logger.warning("æ•°æ®æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame(), pd.DataFrame()
        
        # é‡æ–°è®¡ç®—æŠ˜ä»·ç‡
        valid_opportunities["æŠ˜ä»·ç‡"] = (
            (valid_opportunities["å¸‚åœºä»·æ ¼"] - valid_opportunities["IOPV"]) / 
            valid_opportunities["IOPV"] * 100
        )
        
        original_count = len(valid_opportunities)
        
        abnormal_mask = (valid_opportunities["æŠ˜ä»·ç‡"].abs() > 20)
        if abnormal_mask.any():
            abnormal_data = valid_opportunities[abnormal_mask]
            logger.error(f"âš ï¸ å‘ç° {len(abnormal_data)} ä¸ªå¼‚å¸¸æŠ˜ä»·ç‡æ•°æ®ï¼Œå°†è¢«è¿‡æ»¤:")
            for _, row in abnormal_data.head(5).iterrows():
                logger.error(f"  ETF {row['ETFä»£ç ']}: ä»·æ ¼={row['å¸‚åœºä»·æ ¼']}, IOPV={row['IOPV']}, æŠ˜ä»·ç‡={row['æŠ˜ä»·ç‡']:.2f}%")
            
            valid_opportunities = valid_opportunities[~abnormal_mask].copy()
            logger.info(f"è¿‡æ»¤æ‰ {len(abnormal_data)} ä¸ªå¼‚å¸¸æ•°æ®ï¼Œå‰©ä½™ {len(valid_opportunities)} ä¸ªæ•°æ®")
        
        if valid_opportunities.empty:
            logger.warning("è¿‡æ»¤å¼‚å¸¸æ•°æ®åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame(), pd.DataFrame()
        
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(valid_opportunities)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {valid_opportunities['æŠ˜ä»·ç‡'].min():.2f}% ~ {valid_opportunities['æŠ˜ä»·ç‡'].max():.2f}%")
        
        abs_threshold = Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD
        
        # ã€å…³é”®ä¿®å¤ã€‘æ­£ç¡®åŒºåˆ†æŠ˜ä»·å’Œæº¢ä»·ï¼š
        # æŠ˜ä»·ï¼šå¸‚åœºä»·æ ¼ < IOPV (æŠ˜ä»·ç‡ä¸ºè´Ÿ)
        # æº¢ä»·ï¼šå¸‚åœºä»·æ ¼ > IOPV (æŠ˜ä»·ç‡ä¸ºæ­£)
        discount_opportunities = valid_opportunities[
            (valid_opportunities["æŠ˜ä»·ç‡"] < 0) & 
            (valid_opportunities["æŠ˜ä»·ç‡"].abs() >= abs_threshold)
        ].copy()
        
        premium_opportunities = valid_opportunities[
            (valid_opportunities["æŠ˜ä»·ç‡"] > 0) & 
            (valid_opportunities["æŠ˜ä»·ç‡"].abs() >= abs_threshold)
        ].copy()
        
        # éªŒè¯é€»è¾‘ï¼Œç¡®ä¿æŠ˜ä»·å’Œæº¢ä»·åŒºåˆ†æ­£ç¡®
        invalid_discount = discount_opportunities[discount_opportunities["æŠ˜ä»·ç‡"] >= 0]
        if not invalid_discount.empty:
            logger.error(f"å‘ç° {len(invalid_discount)} ä¸ªé”™è¯¯æ ‡è¯†ä¸ºæŠ˜ä»·çš„æœºä¼šï¼ˆå®é™…ä¸ºæº¢ä»·ï¼‰")
            discount_opportunities = discount_opportunities[discount_opportunities["ETFä»£ç "].isin(invalid_discount["ETFä»£ç "]) == False]
        
        invalid_premium = premium_opportunities[premium_opportunities["æŠ˜ä»·ç‡"] <= 0]
        if not invalid_premium.empty:
            logger.error(f"å‘ç° {len(invalid_premium)} ä¸ªé”™è¯¯æ ‡è¯†ä¸ºæº¢ä»·çš„æœºä¼šï¼ˆå®é™…ä¸ºæŠ˜ä»·ï¼‰")
            premium_opportunities = premium_opportunities[premium_opportunities["ETFä»£ç "].isin(invalid_premium["ETFä»£ç "]) == False]
        
        # æŒ‰æŠ˜ä»·ç‡æ’åº
        if not discount_opportunities.empty:
            discount_opportunities = discount_opportunities.sort_values("æŠ˜ä»·ç‡", ascending=True)
        
        if not premium_opportunities.empty:
            premium_opportunities = premium_opportunities.sort_values("æŠ˜ä»·ç‡", ascending=False)
        
        logger.info(f"å‘ç° {len(discount_opportunities)} ä¸ªæŠ˜ä»·æœºä¼š (æŠ˜ä»·ç‡â‰¤-{abs_threshold}%)")
        logger.info(f"å‘ç° {len(premium_opportunities)} ä¸ªæº¢ä»·æœºä¼š (æº¢ä»·ç‡â‰¥{abs_threshold}%)")
        
        # æ·»åŠ è§„æ¨¡å’Œæ—¥å‡æˆäº¤é¢ä¿¡æ¯
        discount_opportunities = add_etf_basic_info(discount_opportunities)
        premium_opportunities = add_etf_basic_info(premium_opportunities)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        discount_opportunities = calculate_arbitrage_scores(discount_opportunities)
        premium_opportunities = calculate_arbitrage_scores(premium_opportunities)
        
        # ç­›é€‰ä»Šå¤©å°šæœªæ¨é€çš„å¥—åˆ©æœºä¼š
        discount_opportunities = filter_new_discount_opportunities(discount_opportunities)
        premium_opportunities = filter_new_premium_opportunities(premium_opportunities)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if not premium_opportunities.empty:
            for _, row in premium_opportunities.head(3).iterrows():
                logger.info(f"ETF {row['ETFä»£ç ']} æº¢ä»·ç‡: {row['æŠ˜ä»·ç‡']:.2f}%, è¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']:.2f}")
        
        return discount_opportunities, premium_opportunities

    except Exception as e:
        error_msg = f"è®¡ç®—å¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        send_wechat_message(
            message=error_msg,
            message_type="error"
        )
        return pd.DataFrame(), pd.DataFrame()
    
def filter_new_discount_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æ‰ä»Šå¤©å·²ç»æ¨é€è¿‡çš„æŠ˜ä»·æœºä¼š
    """
    if df.empty:
        return df
    
    try:
        etfs_to_push = []
        
        for _, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            if should_push_discount(etf_code):
                etfs_to_push.append(etf_code)
        
        new_opportunities = df[df["ETFä»£ç "].isin(etfs_to_push)].copy()
        
        logger.info(f"ä» {len(df)} ä¸ªæŠ˜ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(new_opportunities)} ä¸ªæ–°æœºä¼šï¼ˆå¢é‡æ¨é€ï¼‰")
        return new_opportunities
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æ–°æŠ˜ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def filter_new_premium_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æ‰ä»Šå¤©å·²ç»æ¨é€è¿‡çš„æº¢ä»·æœºä¼š
    """
    if df.empty:
        return df
    
    try:
        etfs_to_push = []
        
        for _, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            if should_push_premium(etf_code):
                etfs_to_push.append(etf_code)
        
        new_opportunities = df[df["ETFä»£ç "].isin(etfs_to_push)].copy()
        
        logger.info(f"ä» {len(df)} ä¸ªæº¢ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(new_opportunities)} ä¸ªæ–°æœºä¼šï¼ˆå¢é‡æ¨é€ï¼‰")
        return new_opportunities
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æ–°æº¢ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def sort_opportunities_by_abs_premium(df: pd.DataFrame) -> pd.DataFrame:
    """
    æŒ‰æŠ˜ä»·ç‡ç»å¯¹å€¼æ’åº
    """
    if df.empty:
        return df
    
    try:
        df["abs_premium_discount"] = df["æŠ˜ä»·ç‡"].abs()
        df = df.sort_values("abs_premium_discount", ascending=False)
        df = df.drop(columns=["abs_premium_discount"])
        return df
    except Exception as e:
        logger.error(f"æ’åºå¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def add_etf_basic_info(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¸ºå¥—åˆ©æœºä¼šæ•°æ®æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯ï¼ˆè§„æ¨¡ã€æ—¥å‡æˆäº¤é¢ï¼‰
    ã€å…³é”®ä¿®å¤ã€‘æ”¹è¿›åŸºé‡‘è§„æ¨¡è·å–é€»è¾‘ï¼Œä¿®æ­£æˆäº¤é¢å•ä½
    """
    if df.empty:
        return df
    
    try:
        # åŠ è½½ETFå…ƒæ•°æ®
        etf_metadata = load_etf_metadata()
        
        for idx, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            
            # ã€ä¿®å¤ã€‘ä»å…ƒæ•°æ®ä¸­è·å–åŸºé‡‘è§„æ¨¡
            fund_size = 0.0
            if etf_metadata is not None and not etf_metadata.empty:
                metadata_row = etf_metadata[etf_metadata["ETFä»£ç "] == etf_code]
                if not metadata_row.empty:
                    # å°è¯•ä¸åŒçš„åˆ—å
                    for size_col in ["åŸºé‡‘è§„æ¨¡(äº¿å…ƒ)", "è§„æ¨¡(äº¿å…ƒ)", "åŸºé‡‘è§„æ¨¡", "è§„æ¨¡"]:
                        if size_col in metadata_row.columns:
                            try:
                                fund_size_str = str(metadata_row.iloc[0][size_col])
                                # æ¸…ç†æ•°æ®ï¼šç§»é™¤å•ä½å­—ç¬¦
                                fund_size_str = fund_size_str.replace('äº¿å…ƒ', '').replace('äº¿', '').strip()
                                if fund_size_str:
                                    fund_size = float(fund_size_str)
                                    break
                            except:
                                continue
            
            # ã€ä¿®å¤ã€‘è®¡ç®—æ—¥å‡æˆäº¤é¢ï¼ˆå•ä½ï¼šå…ƒï¼‰
            avg_volume = 0.0
            etf_df = load_etf_daily_data(etf_code)
            if not etf_df.empty and "æˆäº¤é¢" in etf_df.columns:
                recent_data = etf_df.tail(30)
                if len(recent_data) > 0:
                    # æ—¥çº¿æ•°æ®ä¸­çš„æˆäº¤é¢å•ä½æ˜¯"å…ƒ"
                    avg_volume = recent_data["æˆäº¤é¢"].mean()
                    # å¦‚æœæˆäº¤é¢è¿‡å¤§ï¼ˆå¯èƒ½æ˜¯å•ä½é—®é¢˜ï¼‰ï¼Œè¿›è¡Œè°ƒæ•´
                    if avg_volume > 100000000000:  # è¶…è¿‡1000äº¿
                        avg_volume = avg_volume / 10000  # å‡è®¾åŸå§‹å•ä½æ˜¯"ä¸‡å…ƒ"ï¼Œè½¬æ¢ä¸º"å…ƒ"
            
            df.loc[idx, "åŸºé‡‘è§„æ¨¡"] = fund_size
            df.loc[idx, "æ—¥å‡æˆäº¤é¢"] = avg_volume
        
        logger.info(f"æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯å®Œæˆï¼Œå…±å¤„ç† {len(df)} ä¸ªæœºä¼š")
        return df
    
    except Exception as e:
        logger.error(f"æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return df

def calculate_arbitrage_scores(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†
    """
    if df.empty:
        return df
    
    try:
        scores = []
        for idx, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            etf_df = load_etf_daily_data(etf_code)
            if etf_df.empty:
                logger.warning(f"ETF {etf_code} æ— æ—¥çº¿æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç»¼åˆè¯„åˆ†")
                scores.append(0.0)
                continue
            
            required_columns = ["æŠ˜ä»·ç‡", "å¸‚åœºä»·æ ¼", "IOPV"]
            missing_columns = [col for col in required_columns if col not in row.index]
            if missing_columns:
                logger.error(f"ETF {etf_code} ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
                scores.append(0.0)
                continue
            
            premium_discount = extract_scalar_value(
                row["æŠ˜ä»·ç‡"],
                log_prefix=f"ETF {etf_code} æŠ˜ä»·ç‡: "
            )
            
            etf_name = row["ETFåç§°"]
            market_price = extract_scalar_value(row["å¸‚åœºä»·æ ¼"], log_prefix=f"ETF {etf_code} å¸‚åœºä»·æ ¼: ")
            iopv = extract_scalar_value(row["IOPV"], log_prefix=f"ETF {etf_code} IOPV: ")
            fund_size = extract_scalar_value(row["åŸºé‡‘è§„æ¨¡"], log_prefix=f"ETF {etf_code} åŸºé‡‘è§„æ¨¡: ")
            avg_volume = extract_scalar_value(row["æ—¥å‡æˆäº¤é¢"], log_prefix=f"ETF {etf_code} æ—¥å‡æˆäº¤é¢: ")
            
            if premium_discount < -15.0:
                logger.warning(f"ETF {etf_code} æŠ˜ä»·ç‡å¼‚å¸¸ä½: {premium_discount:.2f}%")
            elif premium_discount > 15.0:
                logger.warning(f"ETF {etf_code} æº¢ä»·ç‡å¼‚å¸¸é«˜: {premium_discount:.2f}%")
            
            logger.debug(f"ETF {etf_code} å®é™…ä½¿ç”¨çš„æŠ˜ä»·ç‡: {premium_discount:.2f}%")
            
            score = calculate_arbitrage_score(
                etf_code,
                etf_name,
                premium_discount,
                market_price,
                iopv,
                fund_size,
                avg_volume,
                etf_df
            )
            scores.append(score)
        
        df["ç»¼åˆè¯„åˆ†"] = scores
        logger.info(f"è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†å®Œæˆï¼Œå…± {len(df)} ä¸ªæœºä¼š")
        return df
    except Exception as e:
        logger.error(f"è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†å¤±è´¥: {str(e)}", exc_info=True)
        df["ç»¼åˆè¯„åˆ†"] = 0.0
        return df

def filter_valid_discount_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æœ‰æ•ˆçš„æŠ˜ä»·æœºä¼šï¼ˆåŸºäºç»¼åˆè¯„åˆ†å’Œé˜ˆå€¼ï¼‰
    """
    if df.empty:
        return df
    
    try:
        required_columns = ["ETFä»£ç ", "ETFåç§°", "æŠ˜ä»·ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        filtered_df = df[df["æŠ˜ä»·ç‡"] <= -Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD]
        
        if not filtered_df.empty:
            filtered_df = filtered_df.sort_values("æŠ˜ä»·ç‡", ascending=True)
        
        logger.info(f"ä» {len(df)} ä¸ªæŠ˜ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(filtered_df)} ä¸ªæœºä¼šï¼ˆé˜ˆå€¼ï¼šæŠ˜ä»·ç‡â‰¤-{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%ï¼‰")
        return filtered_df
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æœ‰æ•ˆæŠ˜ä»·æœºä¼šå¤±è´¥: {str(e)}")
        return df

def filter_valid_premium_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æœ‰æ•ˆçš„æº¢ä»·æœºä¼šï¼ˆåŸºäºç»¼åˆè¯„åˆ†å’Œé˜ˆå€¼ï¼‰
    """
    if df.empty:
        return df
    
    try:
        required_columns = ["ETFä»£ç ", "ETFåç§°", "æŠ˜ä»·ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        filtered_df = df[df["æŠ˜ä»·ç‡"] >= Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD]
        
        if not filtered_df.empty:
            filtered_df = filtered_df.sort_values("æŠ˜ä»·ç‡", ascending=False)
        
        logger.info(f"ä» {len(df)} ä¸ªæº¢ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(filtered_df)} ä¸ªæœºä¼šï¼ˆé˜ˆå€¼ï¼šæº¢ä»·ç‡â‰¥{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%ï¼‰")
        return filtered_df
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æœ‰æ•ˆæº¢ä»·æœºä¼šå¤±è´¥: {str(e)}")
        return df

def calculate_daily_volume(etf_code: str) -> float:
    """
    è®¡ç®—ETFçš„æ—¥å‡æˆäº¤é¢ï¼ˆåŸºäºæœ€è¿‘30ä¸ªäº¤æ˜“æ—¥ï¼‰
    """
    try:
        etf_df = load_etf_daily_data(etf_code)
        
        if etf_df.empty:
            logger.debug(f"ETF {etf_code} æ— æ—¥çº¿æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        etf_df = ensure_chinese_columns(etf_df)
        
        if "æ—¥æœŸ" not in etf_df.columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘'æ—¥æœŸ'åˆ—ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        etf_df = etf_df.sort_values("æ—¥æœŸ", ascending=False)
        
        recent_data = etf_df.head(30)
        
        if len(recent_data) < 10:
            logger.debug(f"ETF {etf_code} æ•°æ®ä¸è¶³ï¼ˆ{len(recent_data)}å¤©ï¼‰ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        if "æˆäº¤é¢" in recent_data.columns:
            # å•ä½ï¼šå…ƒ
            avg_volume = recent_data["æˆäº¤é¢"].mean()
            logger.debug(f"ETF {etf_code} æ—¥å‡æˆäº¤é¢: {avg_volume:.2f}å…ƒï¼ˆ{len(recent_data)}å¤©æ•°æ®ï¼‰")
            return avg_volume
        else:
            logger.warning(f"ETF {etf_code} ç¼ºå°‘æˆäº¤é¢æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
    
    except Exception as e:
        logger.error(f"è®¡ç®—ETF {etf_code} æ—¥å‡æˆäº¤é¢å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_arbitrage_history(days: int = 7) -> pd.DataFrame:
    """
    è·å–å¥—åˆ©å†å²æ•°æ®
    """
    try:
        history = []
        beijing_now = get_beijing_time()
        
        for i in range(days):
            date = (beijing_now - timedelta(days=i)).strftime("%Y-%m-%d")
            flag_file = os.path.join(Config.FLAG_DIR, f"arbitrage_pushed_{date}.txt")
            
            if os.path.exists(flag_file):
                history.append({
                    "æ—¥æœŸ": date,
                    "æœºä¼šæ•°é‡": 3,
                    "æœ€å¤§æŠ˜ä»·ç‡": 2.5,
                    "æœ€å°æŠ˜ä»·ç‡": -1.8
                })
        
        if not history:
            logger.info("æœªæ‰¾åˆ°å¥—åˆ©å†å²æ•°æ®")
            return pd.DataFrame()
        
        return pd.DataFrame(history)
    
    except Exception as e:
        error_msg = f"è·å–å¥—åˆ©å†å²æ•°æ®å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return pd.DataFrame()

def analyze_arbitrage_performance() -> Dict[str, Any]:
    """
    åˆ†æå¥—åˆ©è¡¨ç°
    """
    try:
        history_df = get_arbitrage_history()
        if history_df.empty:
            logger.info("æ— å†å²æ•°æ®å¯ä¾›åˆ†æ")
            return {
                "avg_opportunities": 0,
                "max_premium": 0,
                "min_discount": 0,
                "trend": "æ— æ•°æ®",
                "has_high_premium": False,
                "has_high_discount": False
            }
        
        avg_opportunities = history_df["æœºä¼šæ•°é‡"].mean()
        max_premium = history_df["æœ€å¤§æŠ˜ä»·ç‡"].max()
        min_discount = history_df["æœ€å°æŠ˜ä»·ç‡"].min()
        
        trend = "å¹³ç¨³"
        if len(history_df) >= 3:
            trend = "ä¸Šå‡" if history_df["æœºä¼šæ•°é‡"].iloc[-3:].mean() > history_df["æœºä¼šæ•°é‡"].iloc[:3].mean() else "ä¸‹é™"
        
        return {
            "avg_opportunities": avg_opportunities,
            "max_premium": max_premium,
            "min_discount": min_discount,
            "trend": trend,
            "has_high_premium": max_premium > 2.0,
            "has_high_discount": min_discount < -2.0
        }
    
    except Exception as e:
        error_msg = f"å¥—åˆ©è¡¨ç°åˆ†æå¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {
            "avg_opportunities": 0,
            "max_premium": 0,
            "min_discount": 0,
            "trend": "åˆ†æå¤±è´¥",
            "has_high_premium": False,
            "has_high_discount": False
        }

def check_arbitrage_exit_signals() -> List[Dict[str, Any]]:
    """
    æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·ï¼ˆæŒæœ‰1å¤©åï¼‰
    """
    try:
        logger.info("å¼€å§‹æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·")
        
        utc_now, beijing_now = get_current_times()
        
        if not os.path.exists(Config.TRADE_RECORD_FILE):
            logger.warning("äº¤æ˜“è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·")
            return []
        
        trade_df = pd.read_csv(Config.TRADE_RECORD_FILE, encoding="utf-8")
        
        yesterday = (beijing_now - timedelta(days=1)).strftime("%Y-%m-%d")
        logger.debug(f"æ£€æŸ¥æ˜¨å¤©({yesterday})æ‰§è¡Œçš„å¥—åˆ©äº¤æ˜“")
        
        yesterday_arbitrage = trade_df[
            (trade_df["æ“ä½œ"] == "å¥—åˆ©ä¹°å…¥") & 
            (trade_df["åˆ›å»ºæ—¥æœŸ"] == yesterday)
        ]
        
        if not yesterday_arbitrage.empty:
            logger.info(f"å‘ç°{len(yesterday_arbitrage)}æ¡éœ€è¦é€€å‡ºçš„å¥—åˆ©äº¤æ˜“")
            
            exit_signals = []
            for _, row in yesterday_arbitrage.iterrows():
                exit_signals.append({
                    "ETFä»£ç ": row["ETFä»£ç "],
                    "ETFåç§°": row["ETFåç§°"],
                    "ä¹°å…¥ä»·æ ¼": row["ä»·æ ¼"],
                    "ä¹°å…¥æ—¥æœŸ": row["åˆ›å»ºæ—¥æœŸ"]
                })
            
            return exit_signals
        
        logger.info("æœªå‘ç°éœ€è¦é€€å‡ºçš„å¥—åˆ©äº¤æ˜“")
        return []
    
    except Exception as e:
        error_msg = f"æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return []

def load_arbitrage_data(date_str: str) -> pd.DataFrame:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸçš„å¥—åˆ©æ•°æ®
    """
    try:
        arbitrage_dir = os.path.join(Config.DATA_DIR, "arbitrage")
        os.makedirs(arbitrage_dir, exist_ok=True)
        
        file_path = os.path.join(arbitrage_dir, f"{date_str}.csv")
        
        if not os.path.exists(file_path):
            logger.info(f"å¥—åˆ©æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path, encoding="utf-8-sig")
        
        logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®: {file_path}")
        logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
        if not df.empty:
            logger.info(f"å‰å‡ è¡Œæ•°æ®ç¤ºä¾‹: {df.head().to_dict()}")
        
        df = ensure_chinese_columns(df)
        
        return df
    
    except Exception as e:
        logger.error(f"åŠ è½½å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_latest_arbitrage_opportunities(max_retry: int = 3) -> pd.DataFrame:
    """
    è·å–æœ€æ–°çš„å¥—åˆ©æœºä¼š
    """
    try:
        today = get_beijing_time().strftime("%Y%m%d")
        
        df = load_arbitrage_data(today)
        
        if not df.empty:
            logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®ï¼Œå®é™…åˆ—å: {list(df.columns)}")
        
        if df.empty:
            logger.warning("åŠ è½½çš„å¥—åˆ©æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®ï¼Œå®é™…åˆ—å: {list(df.columns)}")
        
        df.columns = [col.strip() for col in df.columns]
        
        required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        df["æŠ˜ä»·ç‡"] = ((df["å¸‚åœºä»·æ ¼"] - df["IOPV"]) / df["IOPV"]) * 100
        
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        return df
    
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°å¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}")
        return pd.DataFrame()

def load_latest_valid_arbitrage_data(days_back: int = 7) -> pd.DataFrame:
    """
    åŠ è½½æœ€è¿‘æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®
    """
    try:
        beijing_now = get_beijing_time()
        
        for i in range(days_back):
            date = (beijing_now - timedelta(days=i)).strftime("%Y%m%d")
            logger.debug(f"å°è¯•åŠ è½½å†å²å¥—åˆ©æ•°æ®: {date}")
            
            df = load_arbitrage_data(date)
            
            if not df.empty:
                required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
                if all(col in df.columns for col in required_columns):
                    df["æŠ˜ä»·ç‡"] = ((df["å¸‚åœºä»·æ ¼"] - df["IOPV"]) / df["IOPV"]) * 100
                    
                    logger.info(f"æ‰¾åˆ°æœ‰æ•ˆå†å²å¥—åˆ©æ•°æ®: {date}, å…± {len(df)} ä¸ªæœºä¼š")
                    logger.debug(f"å†å²æ•°æ®æŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
                    return df
        
        logger.warning(f"åœ¨æœ€è¿‘ {days_back} å¤©å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®")
        return pd.DataFrame()
    
    except Exception as e:
        logger.error(f"åŠ è½½æœ€è¿‘æœ‰æ•ˆå¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def mark_arbitrage_opportunities_pushed(discount_df: pd.DataFrame, premium_df: pd.DataFrame) -> bool:
    """
    æ ‡è®°å¥—åˆ©æœºä¼šä¸ºå·²æ¨é€
    """
    try:
        current_date = get_beijing_time().strftime("%Y-%m-%d")
        
        discount_status = load_discount_status()
        premium_status = load_premium_status()
        
        for _, row in discount_df.iterrows():
            etf_code = row["ETFä»£ç "]
            discount_status[etf_code] = {
                "last_pushed": current_date,
                "score": row["ç»¼åˆè¯„åˆ†"]
            }
        
        for _, row in premium_df.iterrows():
            etf_code = row["ETFä»£ç "]
            premium_status[etf_code] = {
                "last_pushed": current_date,
                "score": row["ç»¼åˆè¯„åˆ†"]
            }
        
        save_discount_status(discount_status)
        save_premium_status(premium_status)
        
        logger.info(f"æˆåŠŸæ ‡è®° {len(discount_df) + len(premium_df)} ä¸ªETFå¥—åˆ©æœºä¼šä¸ºå·²æ¨é€")
        return True
    
    except Exception as e:
        logger.error(f"æ ‡è®°å¥—åˆ©æœºä¼šä¸ºå·²æ¨é€å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_arbitrage_push_statistics() -> Dict[str, Any]:
    """
    è·å–å¥—åˆ©æ¨é€ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from utils.file_utils import (
            get_arbitrage_push_count, 
            get_discount_push_count,
            get_premium_push_count,
            get_arbitrage_push_history,
            get_discount_push_history,
            get_premium_push_history
        )
        
        arbitrage_count = get_arbitrage_push_count()
        discount_count = get_discount_push_count()
        premium_count = get_premium_push_count()
        
        arbitrage_history = get_arbitrage_push_history(days=7)
        discount_history = get_discount_push_history(days=7)
        premium_history = get_premium_push_history(days=7)
        
        total_arbitrage = sum(arbitrage_history.values())
        total_discount = sum(discount_history.values())
        total_premium = sum(premium_history.values())
        
        daily_avg_arbitrage = total_arbitrage / len(arbitrage_history) if arbitrage_history else 0
        daily_avg_discount = total_discount / len(discount_history) if discount_history else 0
        daily_avg_premium = total_premium / len(premium_history) if premium_history else 0
        
        latest_arbitrage_date = max(arbitrage_history.keys()) if arbitrage_history else "N/A"
        latest_discount_date = max(discount_history.keys()) if discount_history else "N/A"
        latest_premium_date = max(premium_history.keys()) if premium_history else "N/A"
        
        return {
            "arbitrage": {
                "total_pushed": arbitrage_count["total"],
                "today_pushed": arbitrage_count["today"],
                "total_history": total_arbitrage,
                "daily_avg": round(daily_avg_arbitrage, 2),
                "latest_date": latest_arbitrage_date,
                "history": arbitrage_history
            },
            "discount": {
                "total_pushed": discount_count["total"],
                "today_pushed": discount_count["today"],
                "total_history": total_discount,
                "daily_avg": round(daily_avg_discount, 2),
                "latest_date": latest_discount_date,
                "history": discount_history
            },
            "premium": {
                "total_pushed": premium_count["total"],
                "today_pushed": premium_count["today"],
                "total_history": total_premium,
                "daily_avg": round(daily_avg_premium, 2),
                "latest_date": latest_premium_date,
                "history": premium_history
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–å¥—åˆ©æ¨é€ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {
            "arbitrage": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            },
            "discount": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            },
            "premium": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            }
        }

def generate_arbitrage_message(discount_opportunities: pd.DataFrame, premium_opportunities: pd.DataFrame) -> List[str]:
    """
    ç”Ÿæˆå¥—åˆ©æœºä¼šæ¶ˆæ¯
    ã€å…³é”®ä¿®å¤ã€‘æ­£ç¡®åŒºåˆ†æŠ˜ä»·å’Œæº¢ä»·æœºä¼šï¼Œç»™å‡ºæ˜ç¡®æ“ä½œå»ºè®®
    ã€å…³é”®ä¿®å¤ã€‘ä¿®æ­£æ—¥å‡æˆäº¤é¢å•ä½
    ã€å…³é”®ä¿®å¤ã€‘æ­£ç¡®è·å–åŸºé‡‘è§„æ¨¡
    """
    try:
        messages = []
        
        # ===== ç”ŸæˆæŠ˜ä»·æœºä¼šæ¶ˆæ¯ =====
        if not discount_opportunities.empty:
            discount_msg = generate_discount_message(discount_opportunities)
            if discount_msg:
                messages.append(discount_msg)
        
        # ===== ç”Ÿæˆæº¢ä»·æœºä¼šæ¶ˆæ¯ =====
        if not premium_opportunities.empty:
            premium_msg = generate_premium_message(premium_opportunities)
            if premium_msg:
                messages.append(premium_msg)
        
        if not messages:
            logger.info("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å¥—åˆ©æœºä¼š")
            return []
        
        return messages
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¥—åˆ©æ¶ˆæ¯å¤±è´¥: {str(e)}", exc_info=True)
        return ["ã€ETFå¥—åˆ©æœºä¼šã€‘ç”Ÿæˆæ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"]

def generate_discount_message(df: pd.DataFrame) -> str:
    """ç”ŸæˆæŠ˜ä»·æœºä¼šæ¶ˆæ¯ï¼ˆå¸‚åœºä»·æ ¼ < IOPVï¼‰"""
    if df.empty:
        return ""
    
    # æŒ‰æŠ˜ä»·ç‡æ’åºï¼ˆæŠ˜ä»·è¶Šå¤šè¶Šé å‰ï¼‰
    df = df.sort_values("æŠ˜ä»·ç‡", ascending=True)
    
    # è·å–å½“å‰æ—¶é—´
    beijing_time = get_beijing_time()
    date_str = beijing_time.strftime("%Y-%m-%d %H:%M")
    env_name = os.getenv("ENVIRONMENT", "Git-fish-etf")
    
    # è®¡ç®—å®é™…æŠ˜ä»·ç‡ï¼ˆè´Ÿæ•°ï¼Œå–ç»å¯¹å€¼æ˜¾ç¤ºï¼‰
    df["æ˜¾ç¤ºæŠ˜ä»·ç‡"] = df["æŠ˜ä»·ç‡"].abs()
    
    # ç”Ÿæˆæ¶ˆæ¯
    message = "ã€äºŒçº§å¸‚åœºä»·æ ¼ä½äºå‡€å€¼ï¼Œä¹°å…¥å¥—åˆ©æœºä¼šã€‘\n"
    message += f"ğŸ’° æ“ä½œå»ºè®®ï¼šäºŒçº§å¸‚åœºä¹°å…¥ETFï¼Œä¸€çº§å¸‚åœºèµå›å¥—åˆ©\n"
    message += f"ğŸ“Š ç­›é€‰æ¡ä»¶ï¼šåŸºé‡‘è§„æ¨¡â‰¥{Config.MIN_FUND_SIZE}äº¿å…ƒï¼Œæ—¥å‡æˆäº¤é¢â‰¥{Config.MIN_DAILY_VOLUME/10000:.1f}ä¸‡å…ƒ\n"
    message += f"ğŸ¯ æŠ˜ä»·é˜ˆå€¼ï¼šæŠ˜ä»·ç‡è¶…è¿‡{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%\n"
    message += f"â­ ç»¼åˆè¯„åˆ†è¦æ±‚ï¼šâ‰¥{Config.MIN_ARBITRAGE_SCORE:.1f}\n"
    message += "==================\n"
    
    for i, (_, row) in enumerate(df.head(10).iterrows(), 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        # æŠ˜ä»·ç‡æ˜¾ç¤ºç»å¯¹å€¼
        discount_rate = row["æ˜¾ç¤ºæŠ˜ä»·ç‡"]
        
        # åŸºé‡‘è§„æ¨¡
        fund_size = row["åŸºé‡‘è§„æ¨¡"]
        
        # ã€ä¿®å¤ã€‘æ—¥å‡æˆäº¤é¢å•ä½è½¬æ¢ï¼ˆå…ƒ -> ä¸‡å…ƒï¼‰
        daily_volume_yuan = row["æ—¥å‡æˆäº¤é¢"]  # å•ä½ï¼šå…ƒ
        daily_volume_wan = daily_volume_yuan / 10000  # è½¬æ¢ä¸ºä¸‡å…ƒ
        
        # ä»·å·®è®¡ç®—
        price_diff = row["IOPV"] - row["å¸‚åœºä»·æ ¼"]
        
        message += f"{i}. {row['ETFåç§°']} ({row['ETFä»£ç ']})\n"
        message += f"   â­ ç»¼åˆè¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']:.2f}åˆ†\n"
        message += f"   ğŸ“‰ æŠ˜ä»·ç‡: {discount_rate:.2f}%\n"
        message += f"   ğŸ’° å¸‚åœºä»·æ ¼: {row['å¸‚åœºä»·æ ¼']:.3f}å…ƒ\n"
        message += f"   ğŸ“Š åŸºé‡‘å‡€å€¼(IOPV): {row['IOPV']:.3f}å…ƒ\n"
        message += f"   ğŸ¦ åŸºé‡‘è§„æ¨¡: {fund_size:.2f}äº¿å…ƒ\n"
        message += f"   ğŸ“ˆ æ—¥å‡æˆäº¤é¢: {daily_volume_wan:.2f}ä¸‡å…ƒ\n"
        message += f"   ğŸ’µ å¥—åˆ©ç©ºé—´: {price_diff:.3f}å…ƒ ({discount_rate:.2f}%)\n"
        message += f"   ğŸ“Œ æ“ä½œï¼šä¹°å…¥ä»· {row['å¸‚åœºä»·æ ¼']:.3f}å…ƒ < å‡€å€¼ {row['IOPV']:.3f}å…ƒï¼Œå¯èµå›å¥—åˆ©\n\n"
    
    message += f"ğŸ“… åŒ—äº¬æ—¶é—´: {date_str}\n"
    message += f"ğŸ“Š ç¯å¢ƒï¼š{env_name}"
    
    logger.info(f"ç”ŸæˆæŠ˜ä»·æœºä¼šæ¶ˆæ¯ï¼ŒåŒ…å« {min(len(df), 10)} ä¸ªæœºä¼š")
    return message

def generate_premium_message(df: pd.DataFrame) -> str:
    """ç”Ÿæˆæº¢ä»·æœºä¼šæ¶ˆæ¯ï¼ˆå¸‚åœºä»·æ ¼ > IOPVï¼‰"""
    if df.empty:
        return ""
    
    # æŒ‰æº¢ä»·ç‡æ’åºï¼ˆæº¢ä»·è¶Šå¤šè¶Šé å‰ï¼‰
    df = df.sort_values("æŠ˜ä»·ç‡", ascending=False)
    
    # è·å–å½“å‰æ—¶é—´
    beijing_time = get_beijing_time()
    date_str = beijing_time.strftime("%Y-%m-%d %H:%M")
    env_name = os.getenv("ENVIRONMENT", "Git-fish-etf")
    
    # ç”Ÿæˆæ¶ˆæ¯
    message = "ã€äºŒçº§å¸‚åœºä»·æ ¼é«˜äºå‡€å€¼ï¼Œç”³è´­å¥—åˆ©æœºä¼šã€‘\n"
    message += f"ğŸ’° æ“ä½œå»ºè®®ï¼šä¸€çº§å¸‚åœºç”³è´­ETFï¼ŒäºŒçº§å¸‚åœºå–å‡ºå¥—åˆ©\n"
    message += f"ğŸ“Š ç­›é€‰æ¡ä»¶ï¼šåŸºé‡‘è§„æ¨¡â‰¥{Config.MIN_FUND_SIZE}äº¿å…ƒï¼Œæ—¥å‡æˆäº¤é¢â‰¥{Config.MIN_DAILY_VOLUME/10000:.1f}ä¸‡å…ƒ\n"
    message += f"ğŸ¯ æº¢ä»·é˜ˆå€¼ï¼šæº¢ä»·ç‡è¶…è¿‡{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%\n"
    message += f"â­ ç»¼åˆè¯„åˆ†è¦æ±‚ï¼šâ‰¥{Config.MIN_ARBITRAGE_SCORE:.1f}\n"
    message += "==================\n"
    
    for i, (_, row) in enumerate(df.head(10).iterrows(), 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        # æº¢ä»·ç‡ï¼ˆæ­£æ•°ï¼‰
        premium_rate = row["æŠ˜ä»·ç‡"]
        
        # åŸºé‡‘è§„æ¨¡
        fund_size = row["åŸºé‡‘è§„æ¨¡"]
        
        # ã€ä¿®å¤ã€‘æ—¥å‡æˆäº¤é¢å•ä½è½¬æ¢ï¼ˆå…ƒ -> ä¸‡å…ƒï¼‰
        daily_volume_yuan = row["æ—¥å‡æˆäº¤é¢"]  # å•ä½ï¼šå…ƒ
        daily_volume_wan = daily_volume_yuan / 10000  # è½¬æ¢ä¸ºä¸‡å…ƒ
        
        # ä»·å·®è®¡ç®—
        price_diff = row["å¸‚åœºä»·æ ¼"] - row["IOPV"]
        
        message += f"{i}. {row['ETFåç§°']} ({row['ETFä»£ç ']})\n"
        message += f"   â­ ç»¼åˆè¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']:.2f}åˆ†\n"
        message += f"   ğŸ“ˆ æº¢ä»·ç‡: {premium_rate:.2f}%\n"
        message += f"   ğŸ’° å¸‚åœºä»·æ ¼: {row['å¸‚åœºä»·æ ¼']:.3f}å…ƒ\n"
        message += f"   ğŸ“Š åŸºé‡‘å‡€å€¼(IOPV): {row['IOPV']:.3f}å…ƒ\n"
        message += f"   ğŸ¦ åŸºé‡‘è§„æ¨¡: {fund_size:.2f}äº¿å…ƒ\n"
        message += f"   ğŸ“ˆ æ—¥å‡æˆäº¤é¢: {daily_volume_wan:.2f}ä¸‡å…ƒ\n"
        message += f"   ğŸ’µ å¥—åˆ©ç©ºé—´: {price_diff:.3f}å…ƒ ({premium_rate:.2f}%)\n"
        message += f"   ğŸ“Œ æ“ä½œï¼šå–å‡ºä»· {row['å¸‚åœºä»·æ ¼']:.3f}å…ƒ > å‡€å€¼ {row['IOPV']:.3f}å…ƒï¼Œå¯ç”³è´­å¥—åˆ©\n\n"
    
    message += f"ğŸ“… åŒ—äº¬æ—¶é—´: {date_str}\n"
    message += f"ğŸ“Š ç¯å¢ƒï¼š{env_name}"
    
    logger.info(f"ç”Ÿæˆæº¢ä»·æœºä¼šæ¶ˆæ¯ï¼ŒåŒ…å« {min(len(df), 10)} ä¸ªæœºä¼š")
    return message

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Config.init_dirs()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger.info("å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    # æ¸…ç†è¿‡æœŸçš„å¥—åˆ©çŠ¶æ€è®°å½•
    try:
        from utils.file_utils import (
            clear_expired_arbitrage_status,
            clear_expired_discount_status,
            clear_expired_premium_status
        )
        clear_expired_arbitrage_status()
        clear_expired_discount_status()
        clear_expired_premium_status()
        logger.info("å·²æ¸…ç†è¿‡æœŸçš„å¥—åˆ©çŠ¶æ€è®°å½•")
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸå¥—åˆ©çŠ¶æ€è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
    
except Exception as e:
    error_msg = f"å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    logger.error(error_msg, exc_info=True)
    
    try:
        # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
        import logging
        logging.basicConfig(
            level="INFO",
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler()]
        )
        logging.error(error_msg)
    except Exception as basic_log_error:
        print(f"åŸºç¡€æ—¥å¿—é…ç½®å¤±è´¥: {str(basic_log_error)}")
        print(error_msg)
    
    # å‘é€é”™è¯¯é€šçŸ¥
    try:
        from wechat_push.push import send_wechat_message
        send_wechat_message(
            message=f"å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}",
            message_type="error"
        )
    except Exception as send_error:
        logger.error(f"å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {str(send_error)}", exc_info=True)
