#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥—åˆ©ç­–ç•¥è®¡ç®—æ¨¡å—
åŸºäºå·²ä¿å­˜çš„å®æ—¶æ•°æ®è®¡ç®—å¥—åˆ©æœºä¼š
ä¸¥æ ¼éµå¾ªé¡¹ç›®æ¶æ„åŸåˆ™ï¼šåªè´Ÿè´£è®¡ç®—ï¼Œä¸æ¶‰åŠæ•°æ®çˆ¬å–å’Œæ¶ˆæ¯æ ¼å¼åŒ–
ã€å·²ä¿®å¤ã€‘
- ä¿®å¤äº†éäº¤æ˜“æ—¥ä»å°è¯•è®¡ç®—çš„é—®é¢˜
- ä¿®å¤äº†ETFæ•°é‡ä¸ä¸€è‡´é—®é¢˜
- ä¿®å¤äº†æ— æ—¥çº¿æ•°æ®ä½†æœ‰æº¢ä»·ç‡çš„é€»è¾‘çŸ›ç›¾
- ç¡®ä¿æ•°æ®æºä¸€è‡´æ€§
- ä¿®å¤åˆ—åä¸€è‡´æ€§é—®é¢˜ï¼šæ•°æ®æ–‡ä»¶ä¸­å®é™…ä¸º"æŠ˜ä»·ç‡"è€Œé"æŠ˜æº¢ä»·ç‡"
- ç¡®ä¿åŸºé‡‘è§„æ¨¡æ•°æ®æ­£ç¡®è·å–
- ã€å…³é”®ä¿®å¤ã€‘å½»åº•ä¿®å¤æŠ˜æº¢ä»·æ ‡è¯†é”™è¯¯é—®é¢˜
"""

import pandas as pd
import numpy as np
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple
from config import Config
from utils.date_utils import (
    get_current_times,
    get_beijing_time,
    get_utc_time,
    is_trading_day,
    is_trading_time
)
from utils.file_utils import (
    load_etf_daily_data, 
    ensure_chinese_columns,
    load_discount_status,
    save_discount_status,
    should_push_discount,
    mark_discount_pushed,
    load_premium_status,
    save_premium_status,
    should_push_premium,
    mark_premium_pushed,
    load_etf_metadata
)
from data_crawler.strategy_arbitrage_source import get_trading_etf_list, get_latest_arbitrage_opportunities as get_arbitrage_data
from .etf_scoring import (
    get_etf_basic_info, 
    get_etf_name,
    calculate_arbitrage_score,
    calculate_component_stability_score
)
from wechat_push.push import send_wechat_message

# åˆå§‹åŒ–æ—¥å¿—
logger = logging.getLogger(__name__)
# logger.setLevel(logging.INFO)
# handler = logging.StreamHandler()
# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# handler.setFormatter(formatter)
# logger.addHandler(handler)

def extract_scalar_value(value, default=0.0, log_prefix=""):
    """
    å®‰å…¨åœ°ä»å„ç§ç±»å‹ä¸­æå–æ ‡é‡å€¼
    
    Args:
        value: å¯èƒ½æ˜¯æ ‡é‡ã€Seriesã€DataFrameã€å­—ç¬¦ä¸²ç­‰
        default: é»˜è®¤å€¼ï¼Œå¦‚æœæ— æ³•æå–æ ‡é‡å€¼
        log_prefix: æ—¥å¿—å‰ç¼€ï¼Œç”¨äºæ ‡è¯†è°ƒç”¨ä½ç½®
    
    Returns:
        float: æ ‡é‡å€¼
    """
    try:
        # å¦‚æœå·²ç»æ˜¯æ ‡é‡å€¼ï¼Œç›´æ¥è¿”å›
        if isinstance(value, (int, float)):
            return float(value)
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        if isinstance(value, str):
            # å°è¯•ç§»é™¤éæ•°å­—å­—ç¬¦
            cleaned_str = ''.join(c for c in value if c.isdigit() or c in ['.', '-'])
            if cleaned_str:
                result = float(cleaned_str)
                logger.debug(f"{log_prefix}ä»å­—ç¬¦ä¸²æå–æ ‡é‡å€¼: '{value}' -> {result}")
                return result
            logger.warning(f"{log_prefix}æ— æ³•ä»å­—ç¬¦ä¸² '{value}' æå–æœ‰æ•ˆæ•°å­—ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
            return default
        
        # å¦‚æœæ˜¯pandaså¯¹è±¡ï¼Œå°è¯•æå–æ ‡é‡å€¼
        if isinstance(value, (pd.Series, pd.DataFrame)):
            # å°è¯•è·å–ç¬¬ä¸€ä¸ªå€¼
            if value.size > 0:
                # å°è¯•ä½¿ç”¨.values.flatten()[0]ï¼ˆæœ€å¯é ï¼‰
                try:
                    result = float(value.values.flatten()[0])
                    logger.debug(f"{log_prefix}é€šè¿‡.values.flatten()[0]æå–æ ‡é‡å€¼: {result}")
                    return result
                except Exception as e:
                    # å°è¯•ä½¿ç”¨.item()
                    try:
                        result = float(value.item())
                        logger.debug(f"{log_prefix}é€šè¿‡.item()æå–æ ‡é‡å€¼: {result}")
                        return result
                    except Exception as e2:
                        # å°è¯•ä½¿ç”¨.iloc[0]
                        try:
                            valid_values = value[~pd.isna(value)]
                            if not valid_values.empty:
                                result = float(valid_values.iloc[0])
                                logger.debug(f"{log_prefix}é€šè¿‡.iloc[0]æå–æ ‡é‡å€¼: {result}")
                                return result
                        except Exception as e3:
                            pass
            
            logger.error(f"{log_prefix}æ— æ³•ä»pandaså¯¹è±¡æå–æ ‡é‡å€¼(size={value.size})ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
            return default
        
        # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        result = float(value)
        logger.debug(f"{log_prefix}ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°: {result}")
        return result
    
    except Exception as e:
        logger.error(f"{log_prefix}æ— æ³•ä»ç±»å‹ {type(value)} ä¸­æå–æ ‡é‡å€¼: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default}")
        return default

def calculate_premium_discount(market_price: float, iopv: float) -> float:
    """
    è®¡ç®—æŠ˜æº¢ä»·ç‡
    Args:
        market_price: å¸‚åœºä»·æ ¼
        iopv: IOPV(åŸºé‡‘ä»½é¢å‚è€ƒå‡€å€¼)
    
    Returns:
        float: æŠ˜æº¢ä»·ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œæ­£æ•°è¡¨ç¤ºæº¢ä»·ï¼Œè´Ÿæ•°è¡¨ç¤ºæŠ˜ä»·
    """
    if iopv <= 0:
        logger.warning(f"æ— æ•ˆçš„IOPV: {iopv}")
        return 0.0
    
    # æ­£ç¡®è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼š(å¸‚åœºä»·æ ¼ - IOPV) / IOPV * 100
    # ç»“æœä¸ºæ­£ï¼šæº¢ä»·ï¼ˆå¸‚åœºä»·æ ¼ > IOPVï¼‰
    # ç»“æœä¸ºè´Ÿï¼šæŠ˜ä»·ï¼ˆå¸‚åœºä»·æ ¼ < IOPVï¼‰
    premium_discount = ((market_price - iopv) / iopv) * 100
    return round(premium_discount, 2)

# ä¿ç•™åŸæœ‰çš„ is_manual_trigger å‡½æ•°å®šä¹‰
def is_manual_trigger() -> bool:
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯æ‰‹åŠ¨è§¦å‘çš„ä»»åŠ¡
    
    Returns:
        bool: å¦‚æœæ˜¯æ‰‹åŠ¨è§¦å‘è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼ŒGitHub Actionsä¸­æ‰‹åŠ¨è§¦å‘ä¼šæœ‰ç‰¹æ®Šç¯å¢ƒå˜é‡
        return os.environ.get('GITHUB_EVENT_NAME', '') == 'workflow_dispatch'
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ˜¯å¦ä¸ºæ‰‹åŠ¨è§¦å‘å¤±è´¥: {str(e)}", exc_info=True)
        return False

def validate_arbitrage_data(df: pd.DataFrame) -> bool:
    """
    éªŒè¯å®æ—¶å¥—åˆ©æ•°æ®
    Args:
        df: å®æ—¶å¥—åˆ©æ•°æ®DataFrame
    Returns:
        bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    if df.empty:
        logger.warning("å®æ—¶å¥—åˆ©æ•°æ®ä¸ºç©º")
        return False
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.warning(f"å®æ—¶å¥—åˆ©æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
        return False
    
    # æ£€æŸ¥æ•°æ®é‡
    if len(df) < 10:  # è‡³å°‘éœ€è¦10ä¸ªETFæ‰æœ‰åˆ†æä»·å€¼
        logger.warning(f"å®æ—¶å¥—åˆ©æ•°æ®é‡ä¸è¶³({len(df)}æ¡)ï¼Œéœ€è¦è‡³å°‘10æ¡æ•°æ®")
        return False
    
    return True

def calculate_arbitrage_opportunity() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    åŸºäºå®æ—¶æ•°æ®è®¡ç®—ETFå¥—åˆ©æœºä¼š
    
    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: æŠ˜ä»·æœºä¼šDataFrame, æº¢ä»·æœºä¼šDataFrame
    """
    try:
        # ===== å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ =====
        if not is_trading_day():
            logger.warning("å½“å‰ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè·³è¿‡å¥—åˆ©æœºä¼šè®¡ç®—")
            return pd.DataFrame(), pd.DataFrame()
        
        # è·å–å½“å‰åŒæ—¶åŒºæ—¶é—´
        utc_now, beijing_now = get_current_times()
        logger.info(f"å¼€å§‹è®¡ç®—å¥—åˆ©æœºä¼š (UTC: {utc_now}, CST: {beijing_now})")
        
        # è·å–æ‰€æœ‰çš„ETFæ•°æ®ï¼ˆä¸€ä¸ªDataFrameï¼‰
        all_opportunities = get_arbitrage_data()
        
        # æ£€æŸ¥è¿”å›å€¼ç±»å‹
        if not isinstance(all_opportunities, pd.DataFrame):
            logger.error(f"get_arbitrage_data() è¿”å›å€¼ç±»å‹é”™è¯¯ï¼ŒæœŸæœ›pd.DataFrameï¼Œå®é™…è¿”å›: {type(all_opportunities)}")
            return pd.DataFrame(), pd.DataFrame()
        
        # ===== ä½¿ç”¨æ–°çš„éªŒè¯å‡½æ•° =====
        # éªŒè¯å®æ—¶å¥—åˆ©æ•°æ®
        if not validate_arbitrage_data(all_opportunities):
            logger.error("å®æ—¶å¥—åˆ©æ•°æ®éªŒè¯å¤±è´¥ï¼Œæ— æ³•è®¡ç®—å¥—åˆ©æœºä¼š")
            return pd.DataFrame(), pd.DataFrame()
        
        # ç¡®ä¿DataFrameä½¿ç”¨ä¸­æ–‡åˆ—å
        all_opportunities = ensure_chinese_columns(all_opportunities)
        
        # æ ‡å‡†åŒ–åˆ—å - å¤„ç†å¯èƒ½çš„ç©ºæ ¼é—®é¢˜
        all_opportunities.columns = [col.strip() for col in all_opportunities.columns]
        
        # ===== å…³é”®ä¿®å¤ï¼šç¡®ä¿ETFåˆ—è¡¨ä¸€è‡´æ€§ =====
        # è·å–ç”¨äºå¥—åˆ©ç›‘æ§çš„ETFåˆ—è¡¨
        trading_etf_list = get_trading_etf_list()
        logger.info(f"è·å–åˆ° {len(trading_etf_list)} åªç¬¦åˆæ¡ä»¶çš„ETFè¿›è¡Œå¥—åˆ©ç›‘æ§")
        
        # ç­›é€‰å‡ºäº¤æ˜“ETFåˆ—è¡¨ä¸­çš„ETF
        all_opportunities = all_opportunities[all_opportunities["ETFä»£ç "].isin(trading_etf_list)]
        
        # æ£€æŸ¥ç­›é€‰åçš„æ•°æ®é‡
        if all_opportunities.empty:
            logger.warning("ç­›é€‰åæ— ç¬¦åˆæ¡ä»¶çš„ETFæ•°æ®")
            return pd.DataFrame(), pd.DataFrame()
        
        # ===== å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æœ‰æ•ˆæ€§ =====
        # 1. ç¡®ä¿IOPVæœ‰æ•ˆï¼ˆå¤§äºæœ€å°é˜ˆå€¼ï¼‰
        MIN_IOPV = 0.01  # æœ€å°IOPVé˜ˆå€¼
        valid_opportunities = all_opportunities[all_opportunities["IOPV"] > MIN_IOPV].copy()
        
        # 2. ç¡®ä¿å¸‚åœºä»·æ ¼æœ‰æ•ˆ
        valid_opportunities = valid_opportunities[valid_opportunities["å¸‚åœºä»·æ ¼"] > 0].copy()
        
        # 3. ä»åŸå§‹æ•°æ®é‡æ–°è®¡ç®—æŠ˜æº¢ä»·ç‡ï¼ˆä¸ä¾èµ–å¯èƒ½ä¸å¯é çš„å¤–éƒ¨è®¡ç®—å€¼ï¼‰
        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨"æŠ˜ä»·ç‡"ä½œä¸ºåˆ—åï¼ˆä¸æ•°æ®æ–‡ä»¶ä¸€è‡´ï¼‰
        valid_opportunities["æŠ˜ä»·ç‡"] = (
            (valid_opportunities["å¸‚åœºä»·æ ¼"] - valid_opportunities["IOPV"]) / valid_opportunities["IOPV"]
        ) * 100
        
        # æ£€æŸ¥å¹¶è®°å½•å¼‚å¸¸æŠ˜ä»·ç‡ï¼ˆä¸ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
        abnormal_discount = valid_opportunities[valid_opportunities["æŠ˜ä»·ç‡"] < -15.0]
        abnormal_premium = valid_opportunities[valid_opportunities["æŠ˜ä»·ç‡"] > 15.0]
        
        if not abnormal_discount.empty:
            logger.warning(f"å‘ç° {len(abnormal_discount)} ä¸ªå¼‚å¸¸æŠ˜ä»·ç‡ï¼ˆ<-15%ï¼‰: {abnormal_discount[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
        if not abnormal_premium.empty:
            logger.warning(f"å‘ç° {len(abnormal_premium)} ä¸ªå¼‚å¸¸æº¢ä»·ç‡ï¼ˆ>15%ï¼‰: {abnormal_premium[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
        
        # è®°å½•ç­›é€‰å‰çš„ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(valid_opportunities)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {valid_opportunities['æŠ˜ä»·ç‡'].min():.2f}% ~ {valid_opportunities['æŠ˜ä»·ç‡'].max():.2f}%")
        
        # ===== æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨ç»å¯¹å€¼æ¯”è¾ƒé˜ˆå€¼ =====
        abs_threshold = Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD
        
        # ã€å…³é”®ä¿®å¤ã€‘æ˜ç¡®åŒºåˆ†æŠ˜ä»·å’Œæº¢ä»·ï¼š
        # æŠ˜ä»·ï¼šå¸‚åœºä»·æ ¼ < IOPV (æŠ˜ä»·ç‡ä¸ºè´Ÿ)ï¼Œä¸”ç»å¯¹å€¼å¤§äºé˜ˆå€¼
        # æº¢ä»·ï¼šå¸‚åœºä»·æ ¼ > IOPV (æŠ˜ä»·ç‡ä¸ºæ­£)ï¼Œä¸”ç»å¯¹å€¼å¤§äºé˜ˆå€¼
        discount_opportunities = valid_opportunities[
            (valid_opportunities["æŠ˜ä»·ç‡"] < 0) & 
            (valid_opportunities["æŠ˜ä»·ç‡"].abs() >= abs_threshold)
        ].copy()
        
        premium_opportunities = valid_opportunities[
            (valid_opportunities["æŠ˜ä»·ç‡"] > 0) & 
            (valid_opportunities["æŠ˜ä»·ç‡"].abs() >= abs_threshold)
        ].copy()
        
        # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ éªŒè¯é€»è¾‘ï¼Œç¡®ä¿æŠ˜ä»·å’Œæº¢ä»·åŒºåˆ†æ­£ç¡®
        # æ£€æŸ¥æŠ˜ä»·æœºä¼šæ˜¯å¦çœŸçš„ä¸ºæŠ˜ä»·
        invalid_discount = discount_opportunities[discount_opportunities["æŠ˜ä»·ç‡"] >= 0]
        if not invalid_discount.empty:
            logger.error(f"å‘ç° {len(invalid_discount)} ä¸ªé”™è¯¯æ ‡è¯†ä¸ºæŠ˜ä»·çš„æœºä¼šï¼ˆå®é™…ä¸ºæº¢ä»·ï¼‰: {invalid_discount[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
            # ä»æŠ˜ä»·æœºä¼šä¸­ç§»é™¤
            discount_opportunities = discount_opportunities[discount_opportunities["ETFä»£ç "].isin(invalid_discount["ETFä»£ç "]) == False]
        
        # æ£€æŸ¥æº¢ä»·æœºä¼šæ˜¯å¦çœŸçš„ä¸ºæº¢ä»·
        invalid_premium = premium_opportunities[premium_opportunities["æŠ˜ä»·ç‡"] <= 0]
        if not invalid_premium.empty:
            logger.error(f"å‘ç° {len(invalid_premium)} ä¸ªé”™è¯¯æ ‡è¯†ä¸ºæº¢ä»·çš„æœºä¼šï¼ˆå®é™…ä¸ºæŠ˜ä»·ï¼‰: {invalid_premium[['ETFä»£ç ', 'æŠ˜ä»·ç‡']].to_dict()}")
            # ä»æº¢ä»·æœºä¼šä¸­ç§»é™¤
            premium_opportunities = premium_opportunities[premium_opportunities["ETFä»£ç "].isin(invalid_premium["ETFä»£ç "]) == False]
        
        # æŒ‰æŠ˜ä»·ç‡ç»å¯¹å€¼æ’åº
        if not discount_opportunities.empty:
            discount_opportunities = discount_opportunities.sort_values("æŠ˜ä»·ç‡", ascending=True)
        
        if not premium_opportunities.empty:
            premium_opportunities = premium_opportunities.sort_values("æŠ˜ä»·ç‡", ascending=False)
        
        # ä¿®å¤ï¼šæ›´æ–°æ—¥å¿—ä¿¡æ¯ï¼Œå‡†ç¡®åæ˜ ç­›é€‰æ¡ä»¶
        logger.info(f"å‘ç° {len(discount_opportunities)} ä¸ªæŠ˜ä»·æœºä¼š (é˜ˆå€¼â‰¤-{abs_threshold}%)")
        logger.info(f"å‘ç° {len(premium_opportunities)} ä¸ªæº¢ä»·æœºä¼š (é˜ˆå€¼â‰¥{abs_threshold}%)")
        
        # æ·»åŠ è§„æ¨¡å’Œæ—¥å‡æˆäº¤é¢ä¿¡æ¯
        discount_opportunities = add_etf_basic_info(discount_opportunities)
        premium_opportunities = add_etf_basic_info(premium_opportunities)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        discount_opportunities = calculate_arbitrage_scores(discount_opportunities)
        premium_opportunities = calculate_arbitrage_scores(premium_opportunities)
        
        # ç­›é€‰ä»Šå¤©å°šæœªæ¨é€çš„å¥—åˆ©æœºä¼šï¼ˆå¢é‡æ¨é€åŠŸèƒ½ï¼‰
        discount_opportunities = filter_new_discount_opportunities(discount_opportunities)
        premium_opportunities = filter_new_premium_opportunities(premium_opportunities)
        
        # ä¿®å¤ï¼šæ·»åŠ æ—¥å¿—ï¼Œæ˜¾ç¤ºè¯„åˆ†è¯¦æƒ…
        for _, row in premium_opportunities.iterrows():
            logger.info(f"ETF {row['ETFä»£ç ']} æº¢ä»·ç‡: {row['æŠ˜ä»·ç‡']:.2f}%, è¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']:.2f}")
        
        return discount_opportunities, premium_opportunities

    except Exception as e:
        error_msg = f"è®¡ç®—å¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        send_wechat_message(
            message=error_msg,
            message_type="error"
        )
        return pd.DataFrame(), pd.DataFrame()
    
def filter_new_discount_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æ‰ä»Šå¤©å·²ç»æ¨é€è¿‡çš„æŠ˜ä»·æœºä¼š
    
    Args:
        df: åŸå§‹æŠ˜ä»·æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: ä»…åŒ…å«æ–°å‘ç°çš„æŠ˜ä»·æœºä¼šçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«åº”è¯¥æ¨é€çš„ETFä»£ç 
        etfs_to_push = []
        
        for _, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            if should_push_discount(etf_code):
                etfs_to_push.append(etf_code)
        
        # è¿‡æ»¤DataFrame
        new_opportunities = df[df["ETFä»£ç "].isin(etfs_to_push)].copy()
        
        logger.info(f"ä» {len(df)} ä¸ªæŠ˜ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(new_opportunities)} ä¸ªæ–°æœºä¼šï¼ˆå¢é‡æ¨é€ï¼‰")
        return new_opportunities
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æ–°æŠ˜ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶è¿”å›åŸå§‹DataFrameï¼Œç¡®ä¿è‡³å°‘èƒ½æ¨é€æ–°å‘ç°çš„æœºä¼š
        return df

def filter_new_premium_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æ‰ä»Šå¤©å·²ç»æ¨é€è¿‡çš„æº¢ä»·æœºä¼š
    
    Args:
        df: åŸå§‹æº¢ä»·æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: ä»…åŒ…å«æ–°å‘ç°çš„æº¢ä»·æœºä¼šçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«åº”è¯¥æ¨é€çš„ETFä»£ç 
        etfs_to_push = []
        
        for _, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            if should_push_premium(etf_code):
                etfs_to_push.append(etf_code)
        
        # è¿‡æ»¤DataFrame
        new_opportunities = df[df["ETFä»£ç "].isin(etfs_to_push)].copy()
        
        logger.info(f"ä» {len(df)} ä¸ªæº¢ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(new_opportunities)} ä¸ªæ–°æœºä¼šï¼ˆå¢é‡æ¨é€ï¼‰")
        return new_opportunities
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æ–°æº¢ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        # å‡ºé”™æ—¶è¿”å›åŸå§‹DataFrameï¼Œç¡®ä¿è‡³å°‘èƒ½æ¨é€æ–°å‘ç°çš„æœºä¼š
        return df

def sort_opportunities_by_abs_premium(df: pd.DataFrame) -> pd.DataFrame:
    """
    æŒ‰æŠ˜ä»·ç‡ç»å¯¹å€¼æ’åº
    
    Args:
        df: åŸå§‹å¥—åˆ©æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: æ’åºåçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        df["abs_premium_discount"] = df["æŠ˜ä»·ç‡"].abs()
        df = df.sort_values("abs_premium_discount", ascending=False)
        df = df.drop(columns=["abs_premium_discount"])
        return df
    except Exception as e:
        logger.error(f"æ’åºå¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def add_etf_basic_info(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¸ºå¥—åˆ©æœºä¼šæ•°æ®æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯ï¼ˆè§„æ¨¡ã€æ—¥å‡æˆäº¤é¢ï¼‰
    
    Args:
        df: åŸå§‹å¥—åˆ©æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: æ·»åŠ åŸºæœ¬ä¿¡æ¯åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # ä¸ºæ¯åªETFæ·»åŠ åŸºæœ¬ä¿¡æ¯
        for idx, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            # ã€å…³é”®ä¿®å¤ã€‘æ˜ç¡®ä»all_etfs.csvè·å–åŸºé‡‘è§„æ¨¡
            size = get_etf_basic_info(etf_code)
            
            # è®¡ç®—æ—¥å‡æˆäº¤é¢
            avg_volume = 0.0
            etf_df = load_etf_daily_data(etf_code)
            if not etf_df.empty and "æˆäº¤é¢" in etf_df.columns:
                # å–æœ€è¿‘30å¤©æ•°æ®
                recent_data = etf_df.tail(30)
                if len(recent_data) > 0:
                    # ä¿®å¤ï¼šä¸å†è¿›è¡Œå•ä½è½¬æ¢ï¼Œå› ä¸ºdata_crawlerä¸­å·²ç»Ÿä¸€è½¬æ¢ä¸º"ä¸‡å…ƒ"
                    avg_volume = recent_data["æˆäº¤é¢"].mean()
            
            # ä½¿ç”¨.locé¿å…SettingWithCopyWarning
            df.loc[idx, "åŸºé‡‘è§„æ¨¡"] = size
            df.loc[idx, "æ—¥å‡æˆäº¤é¢"] = avg_volume
        
        logger.info(f"æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯å®Œæˆï¼Œå…±å¤„ç† {len(df)} ä¸ªæœºä¼š")
        return df
    
    except Exception as e:
        logger.error(f"æ·»åŠ ETFåŸºæœ¬ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return df

def calculate_arbitrage_scores(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†
    
    Args:
        df: åŸå§‹å¥—åˆ©æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: åŒ…å«ç»¼åˆè¯„åˆ†çš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # ä¸ºæ¯åªETFè®¡ç®—ç»¼åˆè¯„åˆ†
        scores = []
        for idx, row in df.iterrows():
            etf_code = row["ETFä»£ç "]
            # è·å–ETFæ—¥çº¿æ•°æ®
            etf_df = load_etf_daily_data(etf_code)
            if etf_df.empty:
                logger.warning(f"ETF {etf_code} æ— æ—¥çº¿æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç»¼åˆè¯„åˆ†")
                scores.append(0.0)
                continue
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ["æŠ˜ä»·ç‡", "å¸‚åœºä»·æ ¼", "IOPV"]
            missing_columns = [col for col in required_columns if col not in row.index]
            if missing_columns:
                logger.error(f"ETF {etf_code} ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
                scores.append(0.0)
                continue
            
            # ä½¿ç”¨è¾…åŠ©å‡½æ•°å®‰å…¨æå–æ ‡é‡å€¼
            premium_discount = extract_scalar_value(
                row["æŠ˜ä»·ç‡"],
                log_prefix=f"ETF {etf_code} æŠ˜ä»·ç‡: "
            )
            
            # ä»DataFrameè¡Œä¸­æå–æ‰€æœ‰å¿…éœ€å‚æ•°
            # ä¿®å¤ï¼šETFåç§°æ˜¯å­—ç¬¦ä¸²ï¼Œä¸åº”è¯¥ä½¿ç”¨extract_scalar_value
            etf_name = row["ETFåç§°"]
            market_price = extract_scalar_value(row["å¸‚åœºä»·æ ¼"], log_prefix=f"ETF {etf_code} å¸‚åœºä»·æ ¼: ")
            iopv = extract_scalar_value(row["IOPV"], log_prefix=f"ETF {etf_code} IOPV: ")
            fund_size = extract_scalar_value(row["åŸºé‡‘è§„æ¨¡"], log_prefix=f"ETF {etf_code} åŸºé‡‘è§„æ¨¡: ")
            avg_volume = extract_scalar_value(row["æ—¥å‡æˆäº¤é¢"], log_prefix=f"ETF {etf_code} æ—¥å‡æˆäº¤é¢: ")
            
            # æ£€æŸ¥å¼‚å¸¸æŠ˜ä»·ç‡ï¼ˆä¸ä¿®æ”¹åŸå§‹å€¼ï¼‰
            if premium_discount < -15.0:
                logger.warning(f"ETF {etf_code} æŠ˜ä»·ç‡å¼‚å¸¸ä½: {premium_discount:.2f}%")
            elif premium_discount > 15.0:
                logger.warning(f"ETF {etf_code} æº¢ä»·ç‡å¼‚å¸¸é«˜: {premium_discount:.2f}%")
            
            # è®°å½•å®é™…ä½¿ç”¨çš„å€¼ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.debug(f"ETF {etf_code} å®é™…ä½¿ç”¨çš„æŠ˜ä»·ç‡: {premium_discount:.2f}%")
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            score = calculate_arbitrage_score(
                etf_code,
                etf_name,
                premium_discount,
                market_price,
                iopv,
                fund_size,
                avg_volume,
                etf_df
            )
            scores.append(score)
        
        # æ·»åŠ è¯„åˆ†åˆ—
        df["ç»¼åˆè¯„åˆ†"] = scores
        logger.info(f"è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†å®Œæˆï¼Œå…± {len(df)} ä¸ªæœºä¼š")
        return df
    except Exception as e:
        logger.error(f"è®¡ç®—ETFå¥—åˆ©ç»¼åˆè¯„åˆ†å¤±è´¥: {str(e)}", exc_info=True)
        # æ·»åŠ é»˜è®¤è¯„åˆ†åˆ—
        df["ç»¼åˆè¯„åˆ†"] = 0.0
        return df

def filter_valid_discount_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æœ‰æ•ˆçš„æŠ˜ä»·æœºä¼šï¼ˆåŸºäºç»¼åˆè¯„åˆ†å’Œé˜ˆå€¼ï¼‰
    
    Args:
        df: åŸå§‹æŠ˜ä»·æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: è¿‡æ»¤åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["ETFä»£ç ", "ETFåç§°", "æŠ˜ä»·ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # è®°å½•å®é™…å­˜åœ¨çš„åˆ—
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        # è®°å½•ç­›é€‰å‰çš„ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„æŠ˜ä»·ç‡åˆ—ï¼Œä¸å†é‡æ–°è®¡ç®—
        # æŠ˜ä»·æœºä¼šï¼šæŠ˜ä»·ç‡ä¸ºè´Ÿ
        # å…³é”®ä¿®å¤ï¼šåªæŒ‰æŠ˜ä»·ç‡é˜ˆå€¼ç­›é€‰ï¼Œä¸æŒ‰è¯„åˆ†ç­›é€‰
        filtered_df = df[df["æŠ˜ä»·ç‡"] <= -Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD]
        
        # æŒ‰æŠ˜ä»·ç‡ç»å¯¹å€¼æ’åºï¼ˆé™åºï¼ŒæŠ˜ä»·ç‡è¶Šå¤§è¶Šé å‰ï¼‰
        if not filtered_df.empty:
            filtered_df = filtered_df.sort_values("æŠ˜ä»·ç‡", ascending=True)
        
        # ä¿®å¤ï¼šæ›´æ–°æ—¥å¿—ä¿¡æ¯
        logger.info(f"ä» {len(df)} ä¸ªæŠ˜ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(filtered_df)} ä¸ªæœºä¼šï¼ˆé˜ˆå€¼ï¼šæŠ˜ä»·ç‡â‰¤-{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%ï¼‰")
        return filtered_df
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æœ‰æ•ˆæŠ˜ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def filter_valid_premium_opportunities(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¿‡æ»¤æœ‰æ•ˆçš„æº¢ä»·æœºä¼šï¼ˆåŸºäºç»¼åˆè¯„åˆ†å’Œé˜ˆå€¼ï¼‰
    
    Args:
        df: åŸå§‹æº¢ä»·æœºä¼šDataFrame
    
    Returns:
        pd.DataFrame: è¿‡æ»¤åçš„DataFrame
    """
    if df.empty:
        return df
    
    try:
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["ETFä»£ç ", "ETFåç§°", "æŠ˜ä»·ç‡"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # è®°å½•å®é™…å­˜åœ¨çš„åˆ—
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        # è®°å½•ç­›é€‰å‰çš„ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„æŠ˜ä»·ç‡åˆ—ï¼Œä¸å†é‡æ–°è®¡ç®—
        # æº¢ä»·æœºä¼šï¼šæŠ˜ä»·ç‡ä¸ºæ­£
        # å…³é”®ä¿®å¤ï¼šåªæŒ‰æº¢ä»·ç‡é˜ˆå€¼ç­›é€‰ï¼Œä¸æŒ‰è¯„åˆ†ç­›é€‰
        filtered_df = df[df["æŠ˜ä»·ç‡"] >= Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD]
        
        # æŒ‰æŠ˜ä»·ç‡é™åºæ’åºï¼ˆæº¢ä»·ç‡è¶Šå¤§è¶Šé å‰ï¼‰
        if not filtered_df.empty:
            filtered_df = filtered_df.sort_values("æŠ˜ä»·ç‡", ascending=False)
        
        # ä¿®å¤ï¼šæ›´æ–°æ—¥å¿—ä¿¡æ¯
        logger.info(f"ä» {len(df)} ä¸ªæº¢ä»·æœºä¼šä¸­ç­›é€‰å‡º {len(filtered_df)} ä¸ªæœºä¼šï¼ˆé˜ˆå€¼ï¼šæº¢ä»·ç‡â‰¥{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%ï¼‰")
        return filtered_df
    
    except Exception as e:
        logger.error(f"è¿‡æ»¤æœ‰æ•ˆæº¢ä»·æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return df

def calculate_daily_volume(etf_code: str) -> float:
    """
    è®¡ç®—ETFçš„æ—¥å‡æˆäº¤é¢ï¼ˆåŸºäºæœ€è¿‘30ä¸ªäº¤æ˜“æ—¥ï¼‰
    
    Args:
        etf_code: ETFä»£ç 
        
    Returns:
        float: æ—¥å‡æˆäº¤é¢ï¼ˆä¸‡å…ƒï¼‰
    """
    try:
        # åŠ è½½ETFæ—¥çº¿æ•°æ®
        etf_df = load_etf_daily_data(etf_code)
        
        if etf_df.empty:
            logger.debug(f"ETF {etf_code} æ— æ—¥çº¿æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        # ç¡®ä¿ä½¿ç”¨ä¸­æ–‡åˆ—å
        etf_df = ensure_chinese_columns(etf_df)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ—¥æœŸ"åˆ—
        if "æ—¥æœŸ" not in etf_df.columns:
            logger.warning(f"ETF {etf_code} æ•°æ®ç¼ºå°‘'æ—¥æœŸ'åˆ—ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
        etf_df = etf_df.sort_values("æ—¥æœŸ", ascending=False)
        
        # å–æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
        recent_data = etf_df.head(30)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        if len(recent_data) < 10:  # è‡³å°‘éœ€è¦10å¤©æ•°æ®
            logger.debug(f"ETF {etf_code} æ•°æ®ä¸è¶³ï¼ˆ{len(recent_data)}å¤©ï¼‰ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
        
        # è®¡ç®—æ—¥å‡æˆäº¤é¢
        if "æˆäº¤é¢" in recent_data.columns:
            # ä¿®å¤ï¼šä¸å†è¿›è¡Œå•ä½è½¬æ¢ï¼Œå› ä¸ºdata_crawlerä¸­å·²ç»Ÿä¸€è½¬æ¢ä¸º"ä¸‡å…ƒ"
            avg_volume = recent_data["æˆäº¤é¢"].mean()
            logger.debug(f"ETF {etf_code} æ—¥å‡æˆäº¤é¢: {avg_volume:.2f}ä¸‡å…ƒï¼ˆ{len(recent_data)}å¤©æ•°æ®ï¼‰")
            return avg_volume
        else:
            logger.warning(f"ETF {etf_code} ç¼ºå°‘æˆäº¤é¢æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ—¥å‡æˆäº¤é¢")
            return 0.0
    
    except Exception as e:
        logger.error(f"è®¡ç®—ETF {etf_code} æ—¥å‡æˆäº¤é¢å¤±è´¥: {str(e)}", exc_info=True)
        return 0.0

def get_arbitrage_history(days: int = 7) -> pd.DataFrame:
    """
    è·å–å¥—åˆ©å†å²æ•°æ®
    
    Args:
        days: æŸ¥è¯¢å¤©æ•°
    
    Returns:
        pd.DataFrame: å¥—åˆ©å†å²æ•°æ®
    """
    try:
        history = []
        beijing_now = get_beijing_time()
        
        for i in range(days):
            # ã€æ—¥æœŸdatetimeç±»å‹è§„åˆ™ã€‘ç¡®ä¿æ—¥æœŸæ˜¯datetimeç±»å‹
            date = (beijing_now - timedelta(days=i)).strftime("%Y-%m-%d")
            flag_file = os.path.join(Config.FLAG_DIR, f"arbitrage_pushed_{date}.txt")
            
            if os.path.exists(flag_file):
                # è¯»å–å½“æ—¥å¥—åˆ©æ•°æ®
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ä»æ•°æ®åº“æˆ–æ–‡ä»¶ä¸­è¯»å–å†å²å¥—åˆ©æ•°æ®
                history.append({
                    "æ—¥æœŸ": date,
                    "æœºä¼šæ•°é‡": 3,  # ç¤ºä¾‹æ•°æ®
                    "æœ€å¤§æŠ˜ä»·ç‡": 2.5,  # ç¤ºä¾‹æ•°æ®
                    "æœ€å°æŠ˜ä»·ç‡": -1.8  # ç¤ºä¾‹æ•°æ®
                })
        
        if not history:
            logger.info("æœªæ‰¾åˆ°å¥—åˆ©å†å²æ•°æ®")
            return pd.DataFrame()
        
        return pd.DataFrame(history)
    
    except Exception as e:
        error_msg = f"è·å–å¥—åˆ©å†å²æ•°æ®å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return pd.DataFrame()

def analyze_arbitrage_performance() -> Dict[str, Any]:
    """
    åˆ†æå¥—åˆ©è¡¨ç°
    
    Returns:
        Dict[str, Any]: åˆ†æç»“æœ
    """
    try:
        # è·å–å†å²æ•°æ®
        history_df = get_arbitrage_history()
        if history_df.empty:
            logger.info("æ— å†å²æ•°æ®å¯ä¾›åˆ†æ")
            return {
                "avg_opportunities": 0,
                "max_premium": 0,
                "min_discount": 0,
                "trend": "æ— æ•°æ®",
                "has_high_premium": False,
                "has_high_discount": False
            }
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_opportunities = history_df["æœºä¼šæ•°é‡"].mean()
        max_premium = history_df["æœ€å¤§æŠ˜ä»·ç‡"].max()
        min_discount = history_df["æœ€å°æŠ˜ä»·ç‡"].min()
        
        # æ·»åŠ è¶‹åŠ¿åˆ†æ
        trend = "å¹³ç¨³"
        if len(history_df) >= 3:
            trend = "ä¸Šå‡" if history_df["æœºä¼šæ•°é‡"].iloc[-3:].mean() > history_df["æœºä¼šæ•°é‡"].iloc[:3].mean() else "ä¸‹é™"
        
        # è¿”å›ç»“æ„åŒ–åˆ†æç»“æœ
        return {
            "avg_opportunities": avg_opportunities,
            "max_premium": max_premium,
            "min_discount": min_discount,
            "trend": trend,
            "has_high_premium": max_premium > 2.0,
            "has_high_discount": min_discount < -2.0
        }
    
    except Exception as e:
        error_msg = f"å¥—åˆ©è¡¨ç°åˆ†æå¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {
            "avg_opportunities": 0,
            "max_premium": 0,
            "min_discount": 0,
            "trend": "åˆ†æå¤±è´¥",
            "has_high_premium": False,
            "has_high_discount": False
        }

def check_arbitrage_exit_signals() -> List[Dict[str, Any]]:
    """
    æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·ï¼ˆæŒæœ‰1å¤©åï¼‰
    
    Returns:
        List[Dict[str, Any]]: éœ€è¦é€€å‡ºçš„å¥—åˆ©äº¤æ˜“åˆ—è¡¨
    """
    try:
        logger.info("å¼€å§‹æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·")
        
        # è·å–å½“å‰åŒæ—¶åŒºæ—¶é—´
        utc_now, beijing_now = get_current_times()
        
        # æ£€æŸ¥äº¤æ˜“è®°å½•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(Config.TRADE_RECORD_FILE):
            logger.warning("äº¤æ˜“è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·")
            return []
        
        # è¯»å–äº¤æ˜“è®°å½•
        trade_df = pd.read_csv(Config.TRADE_RECORD_FILE, encoding="utf-8")
        
        # è·å–æ˜¨å¤©çš„æ—¥æœŸï¼ˆåŸºäºåŒ—äº¬æ—¶é—´ï¼‰
        yesterday = (beijing_now - timedelta(days=1)).strftime("%Y-%m-%d")
        logger.debug(f"æ£€æŸ¥æ˜¨å¤©({yesterday})æ‰§è¡Œçš„å¥—åˆ©äº¤æ˜“")
        
        # æŸ¥æ‰¾æ˜¨å¤©æ‰§è¡Œçš„å¥—åˆ©äº¤æ˜“
        yesterday_arbitrage = trade_df[
            (trade_df["æ“ä½œ"] == "å¥—åˆ©ä¹°å…¥") & 
            (trade_df["åˆ›å»ºæ—¥æœŸ"] == yesterday)
        ]
        
        if not yesterday_arbitrage.empty:
            logger.info(f"å‘ç°{len(yesterday_arbitrage)}æ¡éœ€è¦é€€å‡ºçš„å¥—åˆ©äº¤æ˜“")
            
            # æ„å»ºé€€å‡ºä¿¡å·åˆ—è¡¨
            exit_signals = []
            for _, row in yesterday_arbitrage.iterrows():
                exit_signals.append({
                    "ETFä»£ç ": row["ETFä»£ç "],
                    "ETFåç§°": row["ETFåç§°"],
                    "ä¹°å…¥ä»·æ ¼": row["ä»·æ ¼"],
                    "ä¹°å…¥æ—¥æœŸ": row["åˆ›å»ºæ—¥æœŸ"]
                })
            
            return exit_signals
        
        logger.info("æœªå‘ç°éœ€è¦é€€å‡ºçš„å¥—åˆ©äº¤æ˜“")
        return []
    
    except Exception as e:
        error_msg = f"æ£€æŸ¥å¥—åˆ©é€€å‡ºä¿¡å·å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return []

def load_arbitrage_data(date_str: str) -> pd.DataFrame:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸçš„å¥—åˆ©æ•°æ®
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYYMMDD
    
    Returns:
        pd.DataFrame: å¥—åˆ©æ•°æ®DataFrame
    """
    try:
        # æ„å»ºå¥—åˆ©æ•°æ®ç›®å½•
        arbitrage_dir = os.path.join(Config.DATA_DIR, "arbitrage")
        os.makedirs(arbitrage_dir, exist_ok=True)
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(arbitrage_dir, f"{date_str}.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.info(f"å¥—åˆ©æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        # è¯»å–CSVæ–‡ä»¶ï¼ˆæ˜ç¡®æŒ‡å®šç¼–ç ï¼‰
        df = pd.read_csv(file_path, encoding="utf-8-sig")
        
        # æ·»åŠ å…³é”®è¯Šæ–­æ—¥å¿—ï¼ˆINFOçº§åˆ«ï¼Œç¡®ä¿å¯è§ï¼‰
        logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®: {file_path}")
        logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
        if not df.empty:
            logger.info(f"å‰å‡ è¡Œæ•°æ®ç¤ºä¾‹: {df.head().to_dict()}")
        
        # ç¡®ä¿DataFrameä½¿ç”¨ä¸­æ–‡åˆ—å
        df = ensure_chinese_columns(df)
        
        return df
    
    except Exception as e:
        logger.error(f"åŠ è½½å¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def get_latest_arbitrage_opportunities(max_retry: int = 3) -> pd.DataFrame:
    """
    è·å–æœ€æ–°çš„å¥—åˆ©æœºä¼š
    
    Args:
        max_retry: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        pd.DataFrame: å¥—åˆ©æœºä¼šDataFrame
    """
    try:
        # è·å–å½“å‰æ—¥æœŸ
        today = get_beijing_time().strftime("%Y%m%d")
        
        # å°è¯•åŠ è½½ä»Šå¤©çš„å¥—åˆ©æ•°æ®
        df = load_arbitrage_data(today)
        
        # è®°å½•å®é™…åŠ è½½çš„åˆ—åç”¨äºè¯Šæ–­ (INFOçº§åˆ«)
        if not df.empty:
            logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®ï¼Œå®é™…åˆ—å: {list(df.columns)}")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if df.empty:
            logger.warning("åŠ è½½çš„å¥—åˆ©æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # è®°å½•å®é™…åŠ è½½çš„åˆ—åç”¨äºè¯Šæ–­ (INFOçº§åˆ«)
        logger.info(f"æˆåŠŸåŠ è½½å¥—åˆ©æ•°æ®ï¼Œå®é™…åˆ—å: {list(df.columns)}")
        
        # æ ‡å‡†åŒ–åˆ—å - å¤„ç†å¯èƒ½çš„ç©ºæ ¼é—®é¢˜
        df.columns = [col.strip() for col in df.columns]
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
            # è®°å½•å®é™…å­˜åœ¨çš„åˆ—
            logger.info(f"å®é™…åˆ—å: {list(df.columns)}")
            return pd.DataFrame()
        
        # ä¿®å¤ï¼šåœ¨ç­–ç•¥è®¡ç®—æ¨¡å—ä¸­è®¡ç®—æ­£ç¡®çš„æŠ˜ä»·ç‡
        # æ­£ç¡®çš„è®¡ç®—å…¬å¼ï¼š(å¸‚åœºä»·æ ¼ - IOPV) / IOPV * 100
        # ç»“æœä¸ºæ­£ï¼šæº¢ä»·ï¼ˆå¸‚åœºä»·æ ¼ > IOPVï¼‰
        # ç»“æœä¸ºè´Ÿï¼šæŠ˜ä»·ï¼ˆå¸‚åœºä»·æ ¼ < IOPVï¼‰
        df["æŠ˜ä»·ç‡"] = ((df["å¸‚åœºä»·æ ¼"] - df["IOPV"]) / df["IOPV"]) * 100
        
        # è®°å½•ç­›é€‰å‰çš„ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ç­›é€‰å‰æ•°æ®é‡: {len(df)}ï¼ŒæŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
        
        return df
    
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°å¥—åˆ©æœºä¼šå¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def load_latest_valid_arbitrage_data(days_back: int = 7) -> pd.DataFrame:
    """
    åŠ è½½æœ€è¿‘æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®
    
    Args:
        days_back: å‘å‰æŸ¥æ‰¾çš„å¤©æ•°
    
    Returns:
        pd.DataFrame: æœ€è¿‘æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®
    """
    try:
        beijing_now = get_beijing_time()
        
        # ä»ä»Šå¤©å¼€å§‹å‘å‰æŸ¥æ‰¾
        for i in range(days_back):
            date = (beijing_now - timedelta(days=i)).strftime("%Y%m%d")
            logger.debug(f"å°è¯•åŠ è½½å†å²å¥—åˆ©æ•°æ®: {date}")
            
            df = load_arbitrage_data(date)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if not df.empty:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦åˆ—
                required_columns = ["ETFä»£ç ", "ETFåç§°", "å¸‚åœºä»·æ ¼", "IOPV"]
                if all(col in df.columns for col in required_columns):
                    # ä¿®å¤ï¼šåœ¨åŠ è½½å†å²æ•°æ®æ—¶ä¹Ÿè®¡ç®—æ­£ç¡®çš„æŠ˜ä»·ç‡
                    df["æŠ˜ä»·ç‡"] = ((df["å¸‚åœºä»·æ ¼"] - df["IOPV"]) / df["IOPV"]) * 100
                    
                    logger.info(f"æ‰¾åˆ°æœ‰æ•ˆå†å²å¥—åˆ©æ•°æ®: {date}, å…± {len(df)} ä¸ªæœºä¼š")
                    # è®°å½•å†å²æ•°æ®çš„æŠ˜ä»·ç‡èŒƒå›´
                    logger.debug(f"å†å²æ•°æ®æŠ˜ä»·ç‡èŒƒå›´: {df['æŠ˜ä»·ç‡'].min():.2f}% ~ {df['æŠ˜ä»·ç‡'].max():.2f}%")
                    return df
        
        logger.warning(f"åœ¨æœ€è¿‘ {days_back} å¤©å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¥—åˆ©æ•°æ®")
        return pd.DataFrame()
    
    except Exception as e:
        logger.error(f"åŠ è½½æœ€è¿‘æœ‰æ•ˆå¥—åˆ©æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return pd.DataFrame()

def mark_arbitrage_opportunities_pushed(discount_df: pd.DataFrame, premium_df: pd.DataFrame) -> bool:
    """
    æ ‡è®°å¥—åˆ©æœºä¼šä¸ºå·²æ¨é€
    
    Args:
        discount_df: æŠ˜ä»·æœºä¼šDataFrame
        premium_df: æº¢ä»·æœºä¼šDataFrame
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ ‡è®°
    """
    try:
        # è·å–å½“å‰æ—¥æœŸ
        current_date = get_beijing_time().strftime("%Y-%m-%d")
        
        # åŠ è½½ç°æœ‰çŠ¶æ€ - ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å
        discount_status = load_discount_status()
        premium_status = load_premium_status()
        
        # æ›´æ–°æŠ˜ä»·çŠ¶æ€
        for _, row in discount_df.iterrows():
            etf_code = row["ETFä»£ç "]
            discount_status[etf_code] = {
                "last_pushed": current_date,
                "score": row["ç»¼åˆè¯„åˆ†"]
            }
        
        # æ›´æ–°æº¢ä»·çŠ¶æ€
        for _, row in premium_df.iterrows():
            etf_code = row["ETFä»£ç "]
            premium_status[etf_code] = {
                "last_pushed": current_date,
                "score": row["ç»¼åˆè¯„åˆ†"]
            }
        
        # ä¿å­˜çŠ¶æ€ - ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å
        save_discount_status(discount_status)
        save_premium_status(premium_status)
        
        logger.info(f"æˆåŠŸæ ‡è®° {len(discount_df) + len(premium_df)} ä¸ªETFå¥—åˆ©æœºä¼šä¸ºå·²æ¨é€")
        return True
    
    except Exception as e:
        logger.error(f"æ ‡è®°å¥—åˆ©æœºä¼šä¸ºå·²æ¨é€å¤±è´¥: {str(e)}", exc_info=True)
        return False

def get_arbitrage_push_statistics() -> Dict[str, Any]:
    """
    è·å–å¥—åˆ©æ¨é€ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        Dict[str, Any]: å¥—åˆ©æ¨é€ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from utils.file_utils import (
            get_arbitrage_push_count, 
            get_discount_push_count,
            get_premium_push_count,
            get_arbitrage_push_history,
            get_discount_push_history,
            get_premium_push_history
        )
        
        # è·å–æ€»æ¨é€é‡å’Œä»Šæ—¥æ¨é€é‡
        arbitrage_count = get_arbitrage_push_count()
        discount_count = get_discount_push_count()
        premium_count = get_premium_push_count()
        
        # è·å–å†å²æ¨é€è®°å½•
        arbitrage_history = get_arbitrage_push_history(days=7)
        discount_history = get_discount_push_history(days=7)
        premium_history = get_premium_push_history(days=7)
        
        # è®¡ç®—æ€»æ¨é€é‡
        total_arbitrage = sum(arbitrage_history.values())
        total_discount = sum(discount_history.values())
        total_premium = sum(premium_history.values())
        
        # è®¡ç®—æ—¥å‡æ¨é€é‡
        daily_avg_arbitrage = total_arbitrage / len(arbitrage_history) if arbitrage_history else 0
        daily_avg_discount = total_discount / len(discount_history) if discount_history else 0
        daily_avg_premium = total_premium / len(premium_history) if premium_history else 0
        
        # è·å–æœ€æ–°æ¨é€æ—¥æœŸ
        latest_arbitrage_date = max(arbitrage_history.keys()) if arbitrage_history else "N/A"
        latest_discount_date = max(discount_history.keys()) if discount_history else "N/A"
        latest_premium_date = max(premium_history.keys()) if premium_history else "N/A"
        
        return {
            "arbitrage": {
                "total_pushed": arbitrage_count["total"],
                "today_pushed": arbitrage_count["today"],
                "total_history": total_arbitrage,
                "daily_avg": round(daily_avg_arbitrage, 2),
                "latest_date": latest_arbitrage_date,
                "history": arbitrage_history
            },
            "discount": {
                "total_pushed": discount_count["total"],
                "today_pushed": discount_count["today"],
                "total_history": total_discount,
                "daily_avg": round(daily_avg_discount, 2),
                "latest_date": latest_discount_date,
                "history": discount_history
            },
            "premium": {
                "total_pushed": premium_count["total"],
                "today_pushed": premium_count["today"],
                "total_history": total_premium,
                "daily_avg": round(daily_avg_premium, 2),
                "latest_date": latest_premium_date,
                "history": premium_history
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–å¥—åˆ©æ¨é€ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "arbitrage": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            },
            "discount": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            },
            "premium": {
                "total_pushed": 0,
                "today_pushed": 0,
                "total_history": 0,
                "daily_avg": 0,
                "latest_date": "N/A",
                "history": {}
            }
        }

def generate_arbitrage_message(discount_opportunities: pd.DataFrame, premium_opportunities: pd.DataFrame) -> List[str]:
    """
    ç”Ÿæˆå¥—åˆ©æœºä¼šæ¶ˆæ¯ï¼ŒæŒ‰ç…§ç”¨æˆ·æŒ‡å®šçš„æ ¼å¼
    ã€å…³é”®ä¿®å¤ã€‘ä¸åŒºåˆ†æŠ˜æº¢ä»·ï¼ŒåªæŒ‰æŠ˜æº¢ä»·ç‡ç»å¯¹å€¼æ’åº
    ã€å…³é”®ä¿®å¤ã€‘ä¿®æ­£æ—¥å‡æˆäº¤é¢å•ä½ï¼ˆé™¤ä»¥10000ï¼‰
    ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼éµå¾ªç”¨æˆ·æŒ‡å®šçš„æ¶ˆæ¯æ¨¡æ¿
    """
    try:
        # åˆå¹¶æŠ˜ä»·å’Œæº¢ä»·æœºä¼š
        all_opportunities = pd.concat([discount_opportunities, premium_opportunities], ignore_index=True)
        
        # æŒ‰æŠ˜ä»·ç‡ç»å¯¹å€¼æ’åºï¼ˆé™åºï¼‰
        if not all_opportunities.empty:
            all_opportunities["abs_premium_discount"] = all_opportunities["æŠ˜ä»·ç‡"].abs()
            all_opportunities = all_opportunities.sort_values("abs_premium_discount", ascending=False)
            all_opportunities = all_opportunities.drop(columns=["abs_premium_discount"])
        
        # å¦‚æœæ²¡æœ‰æœºä¼šï¼Œè¿”å›ç©ºåˆ—è¡¨
        if all_opportunities.empty:
            logger.info("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å¥—åˆ©æœºä¼š")
            return []
        
        # è·å–å½“å‰æ—¶é—´
        beijing_time = get_beijing_time()
        date_str = beijing_time.strftime("%Y-%m-%d %H:%M")
        env_name = os.getenv("ENVIRONMENT", "Git-fish-etf")
        
        # æ¶ˆæ¯åˆ†é¡µï¼ˆæ¯é¡µæœ€å¤š4ä¸ªETFï¼‰
        messages = []
        etfs_per_page = 4
        total_pages = (len(all_opportunities) + etfs_per_page - 1) // etfs_per_page
        
        # ç”Ÿæˆç¬¬ä¸€é¡µï¼šç­›é€‰æ¡ä»¶ä¿¡æ¯
        if all_opportunities.empty:
            return []
        
        # ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆç¬¬ä¸€é¡µæ¶ˆæ¯
        header_msg = "ã€ä»¥ä¸‹ETFå¸‚åœºä»·æ ¼ä¸å‡€å€¼æœ‰å¤§å·®é¢ã€‘\n"
        header_msg += f"ğŸ’“å…±{len(all_opportunities)}åªETFï¼Œåˆ†{total_pages}æ¡æ¶ˆæ¯æ¨é€ï¼Œè¿™æ˜¯ç¬¬1/{total_pages}æ¡æ¶ˆæ¯\n\n"
        header_msg += "ğŸ“Š ç­›é€‰æ¡ä»¶ï¼šåŸºé‡‘è§„æ¨¡â‰¥10.0äº¿å…ƒï¼Œæ—¥å‡æˆäº¤é¢â‰¥5000.0ä¸‡å…ƒ\n"
        header_msg += "ğŸ’° äº¤æ˜“æˆæœ¬ï¼š0.12%ï¼ˆå«å°èŠ±ç¨å’Œä½£é‡‘ï¼‰\n"
        header_msg += f"ğŸ¯ æŠ˜æº¢ä»·é˜ˆå€¼ï¼šæŠ˜ä»·ç‡è¶…è¿‡{Config.MIN_ARBITRAGE_DISPLAY_THRESHOLD:.2f}%\n"
        header_msg += "â­ ç»¼åˆè¯„åˆ†ï¼šâ‰¥70.0\n"
        header_msg += "==================\n"
        header_msg += f"ğŸ“… åŒ—äº¬æ—¶é—´: {date_str}\n"
        header_msg += f"ğŸ“Š ç¯å¢ƒï¼š{env_name}"
        messages.append(header_msg)
        
        # ç”Ÿæˆåç»­é¡µé¢ï¼šETFåˆ—è¡¨
        for page in range(total_pages):
            start_idx = page * etfs_per_page
            end_idx = min(start_idx + etfs_per_page, len(all_opportunities))
            
            # ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆé¡µç ä¿¡æ¯
            page_msg = f"ã€ç¬¬{page+1}é¡µ å…±{total_pages}é¡µã€‘\n\n"
            
            for i, (_, row) in enumerate(all_opportunities.iloc[start_idx:end_idx].iterrows(), 1):
                # ã€å…³é”®ä¿®å¤ã€‘ä¿®æ­£æ—¥å‡æˆäº¤é¢å•ä½ï¼ˆé™¤ä»¥10000ï¼‰
                daily_volume = row["æ—¥å‡æˆäº¤é¢"] / 10000 if row["æ—¥å‡æˆäº¤é¢"] > 0 else 0
                
                page_msg += f"{i}. {row['ETFåç§°']} ({row['ETFä»£ç ']})\n"
                page_msg += f"   â­ ç»¼åˆè¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']:.2f}åˆ†\n"
                # ã€å…³é”®ä¿®å¤ã€‘åªæ˜¾ç¤º"æŠ˜æº¢ä»·ç‡"ï¼Œä¸åŒºåˆ†æŠ˜ä»·/æº¢ä»·
                page_msg += f"   ğŸ’¹ æŠ˜æº¢ä»·ç‡: {row['æŠ˜ä»·ç‡']:.2f}%\n"
                page_msg += f"   ğŸ“ˆ å¸‚åœºä»·æ ¼: {row['å¸‚åœºä»·æ ¼']:.3f}å…ƒ\n"
                page_msg += f"   ğŸ“Š åŸºé‡‘å‡€å€¼: {row['IOPV']:.3f}å…ƒ\n"
                page_msg += f"   ğŸ¦ åŸºé‡‘è§„æ¨¡: {row['åŸºé‡‘è§„æ¨¡']:.2f}äº¿å…ƒ\n"
                # ã€å…³é”®ä¿®å¤ã€‘ä¿®æ­£æ—¥å‡æˆäº¤é¢æ˜¾ç¤º
                page_msg += f"   ğŸ’° æ—¥å‡æˆäº¤é¢: {daily_volume:.2f}ä¸‡å…ƒ\n\n"
            
            page_msg = page_msg.rstrip()  # ç§»é™¤æœ€åä¸€ä¸ªç©ºè¡Œ
            page_msg += "\n==================\n"
            page_msg += f"ğŸ“… åŒ—äº¬æ—¶é—´: {date_str}\n"
            page_msg += f"ğŸ“Š ç¯å¢ƒï¼š{env_name}"
            
            messages.append(page_msg)
        
        return messages
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¥—åˆ©æ¶ˆæ¯å¤±è´¥: {str(e)}", exc_info=True)
        return ["ã€ETFå¥—åˆ©æœºä¼šã€‘ç”Ÿæˆæ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"]

# æ¨¡å—åˆå§‹åŒ–
try:
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Config.init_dirs()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger.info("å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    # æ¸…ç†è¿‡æœŸçš„å¥—åˆ©çŠ¶æ€è®°å½•
    try:
        from utils.file_utils import (
            clear_expired_arbitrage_status,
            clear_expired_discount_status,
            clear_expired_premium_status
        )
        clear_expired_arbitrage_status()
        clear_expired_discount_status()
        clear_expired_premium_status()
        logger.info("å·²æ¸…ç†è¿‡æœŸçš„å¥—åˆ©çŠ¶æ€è®°å½•")
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸå¥—åˆ©çŠ¶æ€è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
    
except Exception as e:
    error_msg = f"å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}"
    logger.error(error_msg, exc_info=True)
    
    try:
        # é€€å›åˆ°åŸºç¡€æ—¥å¿—é…ç½®
        import logging
        logging.basicConfig(
            level="INFO",
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler()]
        )
        logging.error(error_msg)
    except Exception as basic_log_error:
        print(f"åŸºç¡€æ—¥å¿—é…ç½®å¤±è´¥: {str(basic_log_error)}")
        print(error_msg)
    
    # å‘é€é”™è¯¯é€šçŸ¥
    try:
        from wechat_push.push import send_wechat_message
        send_wechat_message(
            message=f"å¥—åˆ©ç­–ç•¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}",
            message_type="error"
        )
    except Exception as send_error:
        logger.error(f"å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {str(send_error)}", exc_info=True)
